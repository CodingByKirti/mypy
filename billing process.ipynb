{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "\n",
    "def master_input_window():\n",
    "    result = {}\n",
    "\n",
    "    def browse_file(entry_widget):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xlsb *.xls\")])\n",
    "        if file_path:\n",
    "            entry_widget.delete(0, tk.END)\n",
    "            entry_widget.insert(0, file_path)\n",
    "\n",
    "    def submit():\n",
    "        result[\"file1\"] = entry_file1.get()\n",
    "        result[\"file2\"] = entry_file2.get()\n",
    "        result[\"file3\"] = entry_file3.get()\n",
    "        result[\"year\"] = year_var.get()\n",
    "        result[\"month\"] = month_var.get()\n",
    "        root.destroy()\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Master Input Window\")\n",
    "\n",
    "    # File 1\n",
    "    tk.Label(root, text=\"Select First Excel File:\").grid(row=0, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "    entry_file1 = tk.Entry(root, width=50)\n",
    "    entry_file1.grid(row=0, column=1, padx=5, pady=5)\n",
    "    tk.Button(root, text=\"Browse\", command=lambda: browse_file(entry_file1)).grid(row=0, column=2, padx=5, pady=5)\n",
    "\n",
    "    # File 2\n",
    "    tk.Label(root, text=\"Select Second Excel File:\").grid(row=1, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "    entry_file2 = tk.Entry(root, width=50)\n",
    "    entry_file2.grid(row=1, column=1, padx=5, pady=5)\n",
    "    tk.Button(root, text=\"Browse\", command=lambda: browse_file(entry_file2)).grid(row=1, column=2, padx=5, pady=5)\n",
    "\n",
    "    # File 3\n",
    "    tk.Label(root, text=\"Select Third Excel File:\").grid(row=2, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "    entry_file3 = tk.Entry(root, width=50)\n",
    "    entry_file3.grid(row=2, column=1, padx=5, pady=5)\n",
    "    tk.Button(root, text=\"Browse\", command=lambda: browse_file(entry_file3)).grid(row=2, column=2, padx=5, pady=5)\n",
    "\n",
    "    # Year dropdown\n",
    "    tk.Label(root, text=\"Select Year:\").grid(row=3, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "    year_var = tk.StringVar()\n",
    "    year_dropdown = ttk.Combobox(root, textvariable=year_var, values=[str(y) for y in range(2020, 2031)])\n",
    "    year_dropdown.grid(row=3, column=1, padx=5, pady=5)\n",
    "    year_dropdown.current(0)\n",
    "\n",
    "    # Month dropdown\n",
    "    tk.Label(root, text=\"Select Month:\").grid(row=4, column=0, padx=5, pady=5, sticky=\"e\")\n",
    "    month_var = tk.StringVar()\n",
    "    month_dropdown = ttk.Combobox(root, textvariable=month_var, values=[str(m) for m in range(1, 13)])\n",
    "    month_dropdown.grid(row=4, column=1, padx=5, pady=5)\n",
    "    month_dropdown.current(0)\n",
    "\n",
    "    # Submit button\n",
    "    tk.Button(root, text=\"Submit\", command=submit).grid(row=5, column=1, pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "user_inputs = master_input_window()\n",
    "print(\"Returned values:\", user_inputs)\n",
    "\n",
    "file1 = user_inputs[\"file1\"]\n",
    "file2 = user_inputs[\"file2\"]\n",
    "file3 = user_inputs[\"file3\"]\n",
    "year = user_inputs[\"year\"]\n",
    "month = user_inputs[\"month\"]\n",
    "\n",
    "print(f\"\\nFile1: {file1}\\nFile2: {file2}\\nFile3: {file3}\\nYear: {year}\\nMonth: {month}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Apply formatting to ALL sheets ===\n",
    "book = load_workbook(temp_xlsx)\n",
    "header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")\n",
    "header_font = Font(bold=True)\n",
    "\n",
    "for sheet_name in book.sheetnames:   # ðŸ”¹ loop over ALL sheets\n",
    "    ws = book[sheet_name]\n",
    "\n",
    "    # Apply header formatting\n",
    "    for cell in ws[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "\n",
    "    # Set column width = 25\n",
    "    for col in ws.columns:\n",
    "        col_letter = col[0].column_letter\n",
    "        ws.column_dimensions[col_letter].width = 25\n",
    "\n",
    "    # Add autofilter\n",
    "    ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "book.save(temp_xlsx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import win32com.client as win32\n",
    "\n",
    "def update_excel_with_xlsb(dfs_to_update, output_folder=\"Output\"):\n",
    "    \"\"\"\n",
    "    1. Ask user to select an input Excel file (.xlsb).\n",
    "    2. Copy the file to Output folder with '_output' added.\n",
    "    3. Replace/add sheets with given DataFrames.\n",
    "    4. Save final file as .xlsb via Excel COM (to keep size small).\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: Ask user for input file ===\n",
    "    Tk().withdraw()  # hide main Tk window\n",
    "    input_file = filedialog.askopenfilename(\n",
    "        title=\"Select Input XLSB File\",\n",
    "        filetypes=[(\"Excel Binary Workbook\", \"*.xlsb\")]\n",
    "    )\n",
    "    if not input_file:\n",
    "        print(\"âŒ No file selected.\")\n",
    "        return\n",
    "\n",
    "    # === Step 2: Prepare output path ===\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    base_name = os.path.basename(input_file)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    temp_xlsx = os.path.join(output_folder, f\"{name}_temp.xlsx\")   # work in .xlsx\n",
    "    output_xlsb = os.path.join(output_folder, f\"{name}_output.xlsb\")\n",
    "\n",
    "    # === Step 3: Copy input file to temp .xlsx ===\n",
    "    # We must convert to .xlsx because pandas/openpyxl cannot write .xlsb\n",
    "    print(\"ðŸ“‚ Reading input .xlsb ...\")\n",
    "    with pd.ExcelFile(input_file, engine=\"pyxlsb\") as xls:\n",
    "        sheet_names = xls.sheet_names\n",
    "        with pd.ExcelWriter(temp_xlsx, engine=\"openpyxl\") as writer:\n",
    "            for sheet in sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=sheet, engine=\"pyxlsb\")\n",
    "                df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "    # === Step 4: Update/replace sheets ===\n",
    "    with pd.ExcelWriter(temp_xlsx, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        for sheet_name, df in dfs_to_update.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # === Step 5: Apply formatting ===\n",
    "    book = load_workbook(temp_xlsx)\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")\n",
    "    header_font = Font(bold=True)\n",
    "    for sheet_name in dfs_to_update.keys():\n",
    "        if sheet_name in book.sheetnames:\n",
    "            ws = book[sheet_name]\n",
    "            for cell in ws[1]:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            for col in ws.columns:\n",
    "                col_letter = col[0].column_letter\n",
    "                ws.column_dimensions[col_letter].width = 25\n",
    "            ws.auto_filter.ref = ws.dimensions\n",
    "    book.save(temp_xlsx)\n",
    "\n",
    "    # === Step 6: Use Excel COM to save as .xlsb ===\n",
    "    print(\"ðŸ’¾ Converting to .xlsb ...\")\n",
    "    excel = win32.gencache.EnsureDispatch(\"Excel.Application\")\n",
    "    excel.Visible = False\n",
    "    wb = excel.Workbooks.Open(os.path.abspath(temp_xlsx))\n",
    "    wb.SaveAs(os.path.abspath(output_xlsb), FileFormat=50)  # 50 = xlsb\n",
    "    wb.Close(SaveChanges=True)\n",
    "    excel.Quit()\n",
    "\n",
    "    # Cleanup temp file\n",
    "    os.remove(temp_xlsx)\n",
    "\n",
    "    print(f\"âœ… Done! Saved to: {output_xlsb}\")\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    dfs_to_update = {\n",
    "        \"UpdatedSheet1\": pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}),\n",
    "        \"NewReport\": pd.DataFrame({\"X\": [\"hello\", \"world\"]})\n",
    "    }\n",
    "    update_excel_with_xlsb(dfs_to_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "\n",
    "def save_dfs_to_xlsb(input_file, dfs_to_update, output_file=None):\n",
    "    \"\"\"\n",
    "    Replace/add multiple sheets in an .xlsb workbook.\n",
    "    dfs_to_update = { \"SheetName1\": df1, \"SheetName2\": df2 }\n",
    "    \"\"\"\n",
    "    excel = win32.gencache.EnsureDispatch(\"Excel.Application\")\n",
    "    excel.Visible = False\n",
    "\n",
    "    # Open input file\n",
    "    wb = excel.Workbooks.Open(input_file)\n",
    "\n",
    "    for sheet_name, df in dfs_to_update.items():\n",
    "        # Delete old sheet if exists\n",
    "        try:\n",
    "            wb.Sheets(sheet_name).Delete()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Add new sheet\n",
    "        ws = wb.Sheets.Add()\n",
    "        ws.Name = sheet_name\n",
    "\n",
    "        # Write header\n",
    "        for i, col in enumerate(df.columns, start=1):\n",
    "            ws.Cells(1, i).Value = col\n",
    "\n",
    "        # Write data\n",
    "        for r in range(len(df)):\n",
    "            for c in range(len(df.columns)):\n",
    "                ws.Cells(r+2, c+1).Value = df.iat[r, c]\n",
    "\n",
    "    # Save as (new copy or overwrite)\n",
    "    if not output_file:\n",
    "        output_file = input_file\n",
    "    wb.SaveAs(output_file, FileFormat=50)  # 50 = xlsb\n",
    "    wb.Close(SaveChanges=True)\n",
    "    excel.Quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import shutil\n",
    "\n",
    "# Get the selected Excel file path\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "file_path = filedialog.askopenfilename(title=\"Select Excel File\", filetypes=[(\"Excel Files\", \"*.xlsx *.xls\")])\n",
    "\n",
    "# Create a copy of the Excel file\n",
    "file_name = os.path.basename(file_path)\n",
    "file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "output_file_name = f\"{file_name_without_ext}_output.xlsx\"\n",
    "output_file_path = os.path.join(\"Output\", output_file_name)\n",
    "\n",
    "output_folder = \"Output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "shutil.copyfile(file_path, output_file_path)\n",
    "\n",
    "# Process sheets and write to Excel file\n",
    "dfs = {\n",
    "    \"Sheet1\": pd.DataFrame(...),\n",
    "    \"Sheet2\": pd.DataFrame(...),\n",
    "}\n",
    "\n",
    "with pd.ExcelWriter(output_file_path, mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    for sheet_name, df in dfs.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "def update_excel_with_copy(input_file, dfs_to_update, output_folder=\"Output\"):\n",
    "    \"\"\"\n",
    "    Copy input_file -> output_folder/<name>_output.xlsx,\n",
    "    then replace/add sheets from dfs_to_update (dict: sheet_name -> DataFrame),\n",
    "    apply header formatting, set column widths, apply autofilter, freeze header.\n",
    "    \"\"\"\n",
    "    # 1) prepare output path\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    base_name = os.path.basename(input_file)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    output_file = os.path.join(output_folder, f\"{name}_output{ext}\")\n",
    "\n",
    "    # 2) copy input -> output (overwrites existing copy)\n",
    "    shutil.copy2(input_file, output_file)\n",
    "\n",
    "    # 3) load workbook with openpyxl\n",
    "    wb = load_workbook(output_file)\n",
    "\n",
    "    # 4) for each sheet to update: remove if exists, then create and write DF\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # light blue\n",
    "    header_font = Font(bold=True)\n",
    "\n",
    "    for sheet_name, df in dfs_to_update.items():\n",
    "        # remove existing sheet if present\n",
    "        if sheet_name in wb.sheetnames:\n",
    "            ws_old = wb[sheet_name]\n",
    "            wb.remove(ws_old)\n",
    "\n",
    "        # create sheet at the end\n",
    "        ws = wb.create_sheet(title=sheet_name)\n",
    "\n",
    "        # write dataframe rows (includes header)\n",
    "        for r in dataframe_to_rows(df, index=False, header=True):\n",
    "            ws.append(r)\n",
    "\n",
    "        # formatting header row (row 1)\n",
    "        # (we assume header exists since header=True above)\n",
    "        for cell in ws[1]:\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "\n",
    "        # set all column widths to 25\n",
    "        for col_cells in ws.columns:\n",
    "            if not col_cells:\n",
    "                continue\n",
    "            col_letter = col_cells[0].column_letter\n",
    "            ws.column_dimensions[col_letter].width = 25\n",
    "\n",
    "        # add autofilter for used range and freeze header row\n",
    "        max_col = ws.max_column\n",
    "        max_row = ws.max_row\n",
    "        if max_col >= 1 and max_row >= 1:\n",
    "            last_col_letter = ws.cell(row=1, column=max_col).column_letter\n",
    "            ws.auto_filter.ref = f\"A1:{last_col_letter}{max_row}\"\n",
    "            ws.freeze_panes = \"A2\"\n",
    "\n",
    "    # 5) save\n",
    "    wb.save(output_file)\n",
    "    wb.close()\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "def update_excel_with_copy(input_file, dfs_to_update, output_folder=\"Output\"):\n",
    "    \"\"\"\n",
    "    Create a copy of the input Excel file in the Output folder with '_output' added to its name.\n",
    "    Replace existing sheets with DataFrames or add new ones if they don't exist.\n",
    "    Apply formatting (header bold, blue fill, column width, filters).\n",
    "    \"\"\"\n",
    "    # 1. Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 2. Extract filename and create output path\n",
    "    base_name = os.path.basename(input_file)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    output_file = os.path.join(output_folder, f\"{name}_output{ext}\")\n",
    "\n",
    "    # 3. Copy the file\n",
    "    shutil.copy2(input_file, output_file)\n",
    "\n",
    "    # 4. Load workbook\n",
    "    book = load_workbook(output_file)\n",
    "\n",
    "    # 5. Remove sheets that need replacing\n",
    "    for sheet_name in dfs_to_update.keys():\n",
    "        if sheet_name in book.sheetnames:\n",
    "            del book[sheet_name]\n",
    "\n",
    "    # 6. Write updated/new DataFrames\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        writer._book = book   # internal hook, safe in practice\n",
    "        for sheet_name, df in dfs_to_update.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # 7. Re-open to apply formatting\n",
    "    book = load_workbook(output_file)\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # Light Blue\n",
    "    header_font = Font(bold=True)\n",
    "\n",
    "    for sheet_name in dfs_to_update.keys():\n",
    "        ws = book[sheet_name]\n",
    "\n",
    "        # Apply header formatting\n",
    "        for cell in ws[1]:\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "\n",
    "        # Set column width = 25\n",
    "        for col in ws.columns:\n",
    "            col_letter = col[0].column_letter\n",
    "            ws.column_dimensions[col_letter].width = 25\n",
    "\n",
    "        # Add autofilter\n",
    "        ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "    # Save final\n",
    "    book.save(output_file)\n",
    "    print(f\"âœ… File saved with formatting to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd511d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "# === 1. Input/Output Paths ===\n",
    "input_file = r\"Input\\myfile.xlsx\"   # userâ€™s selected file\n",
    "output_folder = \"Output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_folder, os.path.basename(input_file))\n",
    "\n",
    "# === 2. Read / Process Data ===\n",
    "# Example: read two sheets, modify them, create new df\n",
    "df1 = pd.read_excel(input_file, sheet_name=\"Sheet1\")\n",
    "df2 = pd.read_excel(input_file, sheet_name=\"Sheet2\")\n",
    "\n",
    "# Process / create new dfs (example only)\n",
    "df1[\"NewCol\"] = \"Processed\"\n",
    "df2[\"NewCol\"] = \"Updated\"\n",
    "df_new = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
    "\n",
    "# === 3. Write Back to Output File ===\n",
    "if not os.path.exists(output_file):\n",
    "    mode = \"w\"\n",
    "    if_sheet_exists = None\n",
    "else:\n",
    "    mode = \"a\"\n",
    "    if_sheet_exists = \"replace\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\", mode=mode, if_sheet_exists=if_sheet_exists) as writer:\n",
    "    df1.to_excel(writer, sheet_name=\"Sheet1\", index=False)   # replace old\n",
    "    df2.to_excel(writer, sheet_name=\"Sheet2\", index=False)   # replace old\n",
    "    df_new.to_excel(writer, sheet_name=\"New_Sheet\", index=False)  # new sheet\n",
    "\n",
    "# === 4. Formatting Headers, Width, Filters ===\n",
    "wb = load_workbook(output_file)\n",
    "\n",
    "for sheet_name in wb.sheetnames:\n",
    "    ws = wb[sheet_name]\n",
    "\n",
    "    # Header formatting\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")\n",
    "    for cell in ws[1]:\n",
    "        cell.font = Font(bold=True)\n",
    "        cell.fill = header_fill\n",
    "\n",
    "    # Column width = 25\n",
    "    for col in ws.columns:\n",
    "        col_letter = col[0].column_letter\n",
    "        ws.column_dimensions[col_letter].width = 25\n",
    "\n",
    "    # Autofilter\n",
    "    max_col = ws.max_column\n",
    "    max_row = ws.max_row\n",
    "    ws.auto_filter.ref = f\"A1:{ws.cell(row=1, column=max_col).column_letter}{max_row}\"\n",
    "\n",
    "    # Freeze header row\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "# Save back\n",
    "wb.save(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77173d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare billed CCs from base_df_updated ---\n",
    "billed_cc_map = (\n",
    "    base_df_updated.loc[\n",
    "        base_df_updated['Category for Static'].astype(str).str.lower() == 'billed',\n",
    "        ['Cost center']\n",
    "    ]\n",
    "    .assign(Cost_center_str=lambda d: d['Cost center'].astype(str))  # helper string col\n",
    ")\n",
    "\n",
    "# --- Prepare gsc_df with helper string col for matching ---\n",
    "gsc_df['_CostCentre_str'] = gsc_df['Cost Centre'].astype(str)\n",
    "\n",
    "# --- Prepare cognos_df with helper string col ---\n",
    "cognos_df['_CostCode_str'] = cognos_df['Cost Code'].astype(str)\n",
    "\n",
    "# --- Lookup step 1: mark whether Cost Centre is in billed set ---\n",
    "billed_set = set(billed_cc_map['Cost_center_str'])\n",
    "gsc_df['Lookup for Billed CC'] = gsc_df['_CostCentre_str'].apply(\n",
    "    lambda cc: gsc_df.loc[gsc_df['_CostCentre_str'] == cc, 'Cost Centre'].iloc[0]\n",
    "    if cc in billed_set else \"N/A\"\n",
    ")\n",
    "\n",
    "# --- Lookup step 2: fetch Manager Ops from Cognos using string keys ---\n",
    "manager_ops_map = dict(zip(cognos_df['_CostCode_str'], cognos_df['Manager Operations']))\n",
    "gsc_df['Manager Ops'] = gsc_df['_CostCentre_str'].map(manager_ops_map).fillna(\"N/A\")\n",
    "\n",
    "# --- Cleanup helper cols ---\n",
    "gsc_df.drop(columns=['_CostCentre_str'], inplace=True)\n",
    "cognos_df.drop(columns=['_CostCode_str'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfac6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6: Always add a separator column before new block ===\n",
    "sep_base = \"-\"\n",
    "sep_name = sep_base\n",
    "suffix = 1\n",
    "while sep_name in base_df_updated.columns:\n",
    "    sep_name = f\"{sep_base}_set{suffix}\"\n",
    "    suffix += 1\n",
    "\n",
    "base_df_updated.insert(len(base_df_updated.columns), sep_name, pd.NA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc9e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_lookup_block(df):\n",
    "    # The new block of columns (with same headers every time)\n",
    "    new_block = pd.DataFrame({\n",
    "        \"-\": [pd.NA] * len(df),\n",
    "        \"DEP_CODE\": [pd.NA] * len(df),\n",
    "        \"AID (Billed)\": [pd.NA] * len(df),\n",
    "        \"EXCLUDED\": [pd.NA] * len(df),\n",
    "        \"EXCLUDED_REASON\": [pd.NA] * len(df),\n",
    "        \"Check_amd\": [pd.NA] * len(df),\n",
    "        \"Con_amd\": [pd.NA] * len(df),\n",
    "        \"Observations_amd\": [pd.NA] * len(df),\n",
    "    })\n",
    "    a\n",
    "    \n",
    "    # Concat ensures block is appended at the end\n",
    "    return pd.concat([df, new_block], axis=1)\n",
    "\n",
    "# # Example base df\n",
    "# base_df = pd.DataFrame({\n",
    "#     \"PSID\": [101, 102],\n",
    "#     \"Name\": [\"A\", \"B\"]\n",
    "# })\n",
    "\n",
    "# First run\n",
    "# df1 = add_lookup_block(base_df)\n",
    "# Second run\n",
    "df1 = pd.read_excel(\"output.xlsx\")\n",
    "df2 = add_lookup_block(df1)\n",
    "\n",
    "df2.to_excel(\"output.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === STEP 1: Load the Employee Allocation file ===\n",
    "alloc_file = \"Employee Allocation file.xlsx\"\n",
    "df = pd.read_excel(alloc_file, sheet_name=\"Employee Allocation\", dtype={\"EMPLOYEE_ID\": str})\n",
    "\n",
    "# === STEP 2: Select ENTITY values [EDPM, HDPG, HDPI, HSEP] ===\n",
    "gfc_df = df[df[\"ENTITY\"].isin([\"EDPM\", \"HDPG\", \"HDPI\", \"HSEP\"])].copy()\n",
    "\n",
    "# === STEP 3: Save this subset into sheet \"GFC\" later ===\n",
    "# (We will export at the end)\n",
    "\n",
    "# === STEP 4: Separate \"Data Correlation â€“ Request Amendment\" records ===\n",
    "amend_df = gfc_df[gfc_df[\"EXCLUDED_REASON\"] == \"Data Correlation â€“ Request Amendment\"].copy()\n",
    "\n",
    "# === STEP 5: Delete these rows from gfc_df ===\n",
    "gfc_df = gfc_df[gfc_df[\"EXCLUDED_REASON\"] != \"Data Correlation â€“ Request Amendment\"].copy()\n",
    "\n",
    "# === STEP 6: Lookup from gfc_df into base_df_updated ===\n",
    "# (Assuming base_df_updated already exists from Part 1)\n",
    "lookup_cols = [\"DEP_CODE\", \"AID (Billed)\", \"EXCLUDED\", \"EXCLUDED_REASON\"]\n",
    "\n",
    "# VLOOKUP style â†’ keep only first match per EMPLOYEE_ID\n",
    "gfc_unique = gfc_df.drop_duplicates(subset=[\"EMPLOYEE_ID\"], keep=\"first\")\n",
    "\n",
    "# ---- NEW LOGIC: create a fresh block of columns each run ----\n",
    "# Count how many lookup blocks already exist (by counting \"DEP_CODE\" columns)\n",
    "block_count = sum(col.startswith(\"DEP_CODE\") for col in base_df_updated.columns) + 1\n",
    "\n",
    "# Add suffix to distinguish this block\n",
    "suffix = f\"_set{block_count}\"\n",
    "\n",
    "# Add lookup columns for this block\n",
    "for col in lookup_cols:\n",
    "    new_col = col + suffix\n",
    "    base_df_updated[new_col] = base_df_updated[\"PSID\"].map(\n",
    "        gfc_unique.set_index(\"EMPLOYEE_ID\")[col]\n",
    "    )\n",
    "\n",
    "# === STEP 7: Add derived columns ===\n",
    "base_df_updated[\"Con_amd\" + suffix] = (\n",
    "    base_df_updated[\"DEP_CODE\" + suffix].fillna(\"\") + base_df_updated[\"AID (Billed)\" + suffix].fillna(\"\")\n",
    ")\n",
    "base_df_updated[\"Con_Billing\" + suffix] = (\n",
    "    base_df_updated[\"Department ID\"].fillna(\"\") + base_df_updated[\"AID\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "def check_amd(row):\n",
    "    if not row[\"Con_amd\" + suffix] or not row[\"Con_Billing\" + suffix]:\n",
    "        return pd.NA\n",
    "    return row[\"Con_amd\" + suffix] == row[\"Con_Billing\" + suffix]\n",
    "\n",
    "base_df_updated[\"Check_amd\" + suffix] = base_df_updated.apply(check_amd, axis=1)\n",
    "\n",
    "# Blank column for observations\n",
    "base_df_updated[\"Observations_amd\" + suffix] = \"\"\n",
    "\n",
    "# === STEP 8: Apply logic for Observations_amd ===\n",
    "mask1 = base_df_updated[\"Check_amd\" + suffix].isna() & base_df_updated[\"Con_amd\" + suffix].eq(\"\")\n",
    "base_df_updated.loc[mask1, \"Observations_amd\" + suffix] = \"Resource left HSBC\"\n",
    "\n",
    "mask2 = (base_df_updated[\"Category for Static\"] == \"Billed\") & (base_df_updated[\"EXCLUDED\" + suffix] == 0)\n",
    "base_df_updated.loc[mask2, \"Observations_amd\" + suffix] = \"Ok\"\n",
    "\n",
    "mask3 = base_df_updated[\"Category for Static\"].isin([\"Not Billed\", \"Others\"]) & (base_df_updated[\"EXCLUDED\" + suffix] == 1)\n",
    "base_df_updated.loc[mask3, \"Observations_amd\" + suffix] = \"Ok\"\n",
    "\n",
    "# === STEP 9: Export result ===\n",
    "with pd.ExcelWriter(\"Phase3_Part2_Output.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    base_df_updated.to_excel(writer, sheet_name=\"Base Updated\", index=False)\n",
    "    gfc_df.to_excel(writer, sheet_name=\"GFC\", index=False)\n",
    "    amend_df.to_excel(writer, sheet_name=\"Data Correlation â€“ Amendments\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804727e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce4eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c844135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a8a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcafdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a52a805",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Employee Allocation file.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c1bc82ae3e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# === STEP 1: Load files ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgsc_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallocation_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallocation_sheet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"EMPLOYEE_ID\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mbase_df_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_sheet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"PSID\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m                 )\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(path, content, storage_options)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mcontent_or_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     ) as handle:\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Employee Allocation file.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === STEP 1: Load the Employee Allocation file ===\n",
    "alloc_file = \"Employee Allocation file.xlsx\"\n",
    "df = pd.read_excel(alloc_file, sheet_name=\"Employee Allocation\", dtype={\"EMPLOYEE_ID\": str})\n",
    "\n",
    "# === STEP 2: Select ENTITY values [EDPM, HDPG, HDPI, HSEP] ===\n",
    "gfc_df = df[df[\"ENTITY\"].isin([\"EDPM\", \"HDPG\", \"HDPI\", \"HSEP\"])].copy()\n",
    "\n",
    "# === STEP 3: Save this subset into sheet \"GFC\" later ===\n",
    "# (We will export at the end)\n",
    "\n",
    "# === STEP 4: Separate \"Data Correlation â€“ Request Amendment\" records ===\n",
    "amend_df = gfc_df[gfc_df[\"EXCLUDED_REASON\"] == \"Data Correlation â€“ Request Amendment\"].copy()\n",
    "\n",
    "# === STEP 5: Delete these rows from gfc_df ===\n",
    "gfc_df = gfc_df[gfc_df[\"EXCLUDED_REASON\"] != \"Data Correlation â€“ Request Amendment\"].copy()\n",
    "\n",
    "# === STEP 6: Lookup from gfc_df into base_df_updated ===\n",
    "# (Assuming base_df_updated already exists from Part 1)\n",
    "lookup_cols = [\"DEP_CODE\", \"AID (Billed)\", \"EXCLUDED\", \"EXCLUDED_REASON\"]\n",
    "\n",
    "# VLOOKUP style â†’ keep only first match per EMPLOYEE_ID\n",
    "gfc_unique = gfc_df.drop_duplicates(subset=[\"EMPLOYEE_ID\"], keep=\"first\")\n",
    "\n",
    "for col in lookup_cols:\n",
    "    base_df_updated[col] = base_df_updated[\"PSID\"].map(\n",
    "        gfc_unique.set_index(\"EMPLOYEE_ID\")[col]\n",
    "    )\n",
    "\n",
    "# === STEP 7: Add derived columns ===\n",
    "base_df_updated[\"Con_amd\"] = (\n",
    "    base_df_updated[\"DEP_CODE\"].fillna(\"\") + base_df_updated[\"AID (Billed)\"].fillna(\"\")\n",
    ")\n",
    "base_df_updated[\"Con_Billing\"] = (\n",
    "    base_df_updated[\"Department ID\"].fillna(\"\") + base_df_updated[\"AID\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "def check_amd(row):\n",
    "    if not row[\"Con_amd\"] or not row[\"Con_Billing\"]:\n",
    "        return pd.NA\n",
    "    return row[\"Con_amd\"] == row[\"Con_Billing\"]\n",
    "\n",
    "base_df_updated[\"Check_amd\"] = base_df_updated.apply(check_amd, axis=1)\n",
    "\n",
    "# Blank column for observations\n",
    "base_df_updated[\"Observations_amd\"] = \"\"\n",
    "\n",
    "# === STEP 8: Apply logic for Observations_amd ===\n",
    "# Case 1: Check_amd = NA and Con_amd = NA or blank â†’ Resource left HSBC\n",
    "mask1 = base_df_updated[\"Check_amd\"].isna() & base_df_updated[\"Con_amd\"].eq(\"\")\n",
    "base_df_updated.loc[mask1, \"Observations_amd\"] = \"Resource left HSBC\"\n",
    "\n",
    "# Case 2: Category for Static = Billed and EXCLUDED = 0 â†’ Ok\n",
    "mask2 = (base_df_updated[\"Category for Static\"] == \"Billed\") & (base_df_updated[\"EXCLUDED\"] == 0)\n",
    "base_df_updated.loc[mask2, \"Observations_amd\"] = \"Ok\"\n",
    "\n",
    "# Case 3: Category for Static in [Not Billed, Others] and EXCLUDED = 1 â†’ Ok\n",
    "mask3 = base_df_updated[\"Category for Static\"].isin([\"Not Billed\", \"Others\"]) & (base_df_updated[\"EXCLUDED\"] == 1)\n",
    "base_df_updated.loc[mask3, \"Observations_amd\"] = \"Ok\"\n",
    "\n",
    "# === STEP 9: Export result ===\n",
    "with pd.ExcelWriter(\"Phase3_Part2_Output.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    base_df_updated.to_excel(writer, sheet_name=\"Base Updated\", index=False)\n",
    "    gfc_df.to_excel(writer, sheet_name=\"GFC\", index=False)\n",
    "    amend_df.to_excel(writer, sheet_name=\"Data Correlation â€“ Amendments\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b577c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: output1.xlsx\n",
      "Processing: output2.xlsx\n",
      "Processing: output3.xlsx\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import win32com.client as win32\n",
    "\n",
    "# Prompt for folder selection\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "folder_path = filedialog.askdirectory(title=\"Select Folder with Excel Files\")\n",
    "\n",
    "# Initialize Excel\n",
    "excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "excel.Visible = False  # Set to True if you want to see Excel working\n",
    "\n",
    "# Loop through Excel files\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\") or filename.endswith(\".xlsm\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing: {filename}\")\n",
    "        try:\n",
    "            wb = excel.Workbooks.Open(file_path)\n",
    "\n",
    "            sheet_count = wb.Sheets.Count\n",
    "            if sheet_count >= 3:\n",
    "                wb.Sheets(3).Activate()  # Activate 3rd sheet\n",
    "                wb.Sheets(2).Activate()  # Then activate 2nd sheet\n",
    "\n",
    "            wb.Save()\n",
    "            wb.Close(SaveChanges=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Quit Excel\n",
    "excel.Quit()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "# 1. Excel-style rounding function\n",
    "def round_half_up(series: pd.Series, decimals: int = 2) -> pd.Series:\n",
    "    return series.apply(lambda x: float(Decimal(str(x)).quantize(Decimal('1.' + '0'*decimals), rounding=ROUND_HALF_UP)))\n",
    "\n",
    "# 2. Step 1: Filter \"Billed\" and calculate % without rounding\n",
    "def calculate_fte_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df = df[df['Category for Static'].str.lower() == 'billed'].copy()\n",
    "    total_fte_by_aid = billed_df.groupby('AID')['No. of FTE'].sum()\n",
    "    billed_df['Total FTE'] = billed_df['AID'].map(total_fte_by_aid)\n",
    "    billed_df['Sum of %'] = (billed_df['No. of FTE'] / billed_df['Total FTE']) * 100\n",
    "    return billed_df\n",
    "\n",
    "# 3. Step 2: Create pivot table\n",
    "def create_fte_pivot(billed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df.fillna(\"Missing\", inplace=True)\n",
    "\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Billing Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "\n",
    "    summary_df = billed_df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Sum of %': 'sum'\n",
    "    })\n",
    "\n",
    "    summary_df.rename(columns={'No. of FTE': 'Sum of No. of FTE'}, inplace=True)\n",
    "    summary_df['Sum of % (unrounded)'] = summary_df['Sum of %']\n",
    "    summary_df['Sum of % (rounded)'] = round_half_up(summary_df['Sum of %'])\n",
    "\n",
    "    # Replace 'Missing' back to blanks for output\n",
    "    summary_df['BP CC1'] = summary_df['BP CC1'].replace(\"Missing\", \"\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# 4. Step 3: Adjustment function to ensure total = 100\n",
    "def adjust_sum_of_percent(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    summary_df = summary_df.copy()\n",
    "    summary_df['Final Sum of %'] = summary_df['Sum of % (rounded)']  # start with rounded\n",
    "\n",
    "    adjusted_rows = []\n",
    "    for aid, group in summary_df.groupby('AID'):\n",
    "        rounded_sum = group['Sum of % (rounded)'].sum()\n",
    "        delta = round(100.00 - rounded_sum, 2)\n",
    "\n",
    "        # Apply delta to first row only\n",
    "        idx_to_adjust = group.index[0]\n",
    "        summary_df.at[idx_to_adjust, 'Final Sum of %'] += delta\n",
    "        summary_df.at[idx_to_adjust, 'Adjustment'] = delta\n",
    "\n",
    "        # For validation\n",
    "        summary_df.loc[group.index, 'Sum of % Total after Adjustment'] = \\\n",
    "            summary_df.loc[group.index, 'Final Sum of %'].sum()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# 5. Step 4: Main runner function\n",
    "def process_fte_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df = calculate_fte_summary(df)\n",
    "    summary_df = create_fte_pivot(billed_df)\n",
    "    final_df = adjust_sum_of_percent(summary_df)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sum_of_percent(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensures that Sum of % per AID totals exactly 100 by adjusting the last row per AID.\n",
    "    \"\"\"\n",
    "    # Sort to ensure consistent order\n",
    "    summary_df = summary_df.sort_values(by=['AID']).reset_index(drop=True)\n",
    "\n",
    "    # Container for adjusted rows\n",
    "    adjusted_rows = []\n",
    "\n",
    "    for aid, group in summary_df.groupby('AID'):\n",
    "        group = group.copy()\n",
    "        rounded_total = group['Sum of %'].sum()\n",
    "        diff = round(100.0 - rounded_total, 2)  # small diff due to rounding\n",
    "\n",
    "        group['Adjustment'] = 0.0\n",
    "\n",
    "        if abs(diff) > 0.001:\n",
    "            # Apply adjustment to last row in this group\n",
    "            last_idx = group.index[-1]\n",
    "            group.at[last_idx, 'Adjustment'] = diff\n",
    "            group.at[last_idx, 'Final Sum of %'] = group.at[last_idx, 'Sum of %'] + diff\n",
    "        else:\n",
    "            group['Final Sum of %'] = group['Sum of %']\n",
    "\n",
    "        adjusted_rows.append(group)\n",
    "\n",
    "    return pd.concat(adjusted_rows).reset_index(drop=True)\n",
    "summary_df = create_fte_pivot(your_df)\n",
    "final_df = adjust_sum_of_percent(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "def calculate_fte_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter only Billed rows (case-insensitive)\n",
    "    billed_df = df[df['Category for Static'].str.lower() == 'billed'].copy()\n",
    "\n",
    "    # Calculate Total FTE per AID\n",
    "    total_fte_by_aid = billed_df.groupby('AID')['No. of FTE'].sum()\n",
    "    billed_df['Total FTE'] = billed_df['AID'].map(total_fte_by_aid)\n",
    "\n",
    "    # Calculate Sum of % = (No. of FTE / Total FTE) * 100\n",
    "    billed_df['Sum of %'] = (billed_df['No. of FTE'] / billed_df['Total FTE']) * 100\n",
    "\n",
    "    return billed_df\n",
    "\n",
    "def round_half_up(series: pd.Series, decimals: int = 2) -> pd.Series:\n",
    "    # Apply Excel-like rounding to a pandas Series\n",
    "    return series.apply(lambda x: float(Decimal(str(x)).quantize(Decimal('1.' + '0' * decimals), rounding=ROUND_HALF_UP)))\n",
    "\n",
    "def create_fte_pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df = calculate_fte_summary(df)\n",
    "\n",
    "    # Fill missing values temporarily to avoid dropping during groupby\n",
    "    billed_df.fillna(\"Missing\", inplace=True)\n",
    "\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Billing Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "\n",
    "    # Aggregate values\n",
    "    summary_df = billed_df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Sum of %': 'sum'\n",
    "    })\n",
    "\n",
    "    # Rename and apply custom rounding\n",
    "    summary_df.rename(columns={'No. of FTE': 'Sum of No. of FTE'}, inplace=True)\n",
    "    summary_df['Sum of % (unrounded)'] = summary_df['Sum of %']\n",
    "    summary_df['Sum of %'] = round_half_up(summary_df['Sum of %'])\n",
    "\n",
    "    # Replace 'Missing' in BP CC1 with blank\n",
    "    summary_df['BP CC1'] = summary_df['BP CC1'].replace(\"Missing\", \"\")\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ab2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_billed_fte_percentage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter only billed rows\n",
    "    billed_df = df[df['Category for Static'].astype(str).str.strip().str.lower() == 'billed'].copy()\n",
    "\n",
    "    # Calculate Total FTE per AID\n",
    "    total_fte_per_aid = billed_df.groupby('AID')['No. of FTE'].sum().rename('Total FTE').reset_index()\n",
    "    billed_df = billed_df.merge(total_fte_per_aid, on='AID', how='left')\n",
    "\n",
    "    # Calculate unrounded percentage\n",
    "    billed_df['Sum of %'] = (billed_df['No. of FTE'] / billed_df['Total FTE']) * 100\n",
    "\n",
    "    return billed_df\n",
    "\n",
    "def round_half_up(number, decimals=2):\n",
    "    factor = 10 ** decimals\n",
    "    return float(int(number * factor + 0.5)) / factor\n",
    "\n",
    "def aggregate_fte_summary(billed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Business Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "\n",
    "    billed_df['Rounded Sum of %'] = billed_df['Sum of %'].apply(lambda x: round_half_up(x, 2))\n",
    "\n",
    "    summary_df = billed_df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Rounded Sum of %': 'sum'\n",
    "    })\n",
    "\n",
    "    summary_df.rename(columns={\n",
    "        'No. of FTE': 'Sum of No. of FTE',\n",
    "        'Rounded Sum of %': 'Sum of %'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def adjust_percentage_to_100(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    final_rows = []\n",
    "    for aid, group in summary_df.groupby('AID'):\n",
    "        temp_df = group.copy()\n",
    "        sum_percent = temp_df['Sum of %'].sum()\n",
    "        diff = round_half_up(100.0 - sum_percent, 2)\n",
    "\n",
    "        temp_df['Adjusted Sum of %'] = temp_df['Sum of %']\n",
    "        if not temp_df.empty:\n",
    "            temp_df.iloc[0, temp_df.columns.get_loc('Adjusted Sum of %')] += diff\n",
    "\n",
    "        temp_df['% Adjustment Done'] = temp_df['Adjusted Sum of %'] - temp_df['Sum of %']\n",
    "        final_rows.append(temp_df)\n",
    "\n",
    "    final_df = pd.concat(final_rows, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Usage Example:\n",
    "# df = pd.read_excel(\"your_file.xlsx\")\n",
    "# billed_df = calculate_billed_fte_percentage(df)\n",
    "# summary_df = aggregate_fte_summary(billed_df)\n",
    "# final_df = adjust_percentage_to_100(summary_df)\n",
    "# print(final_df.head())\n",
    "#option 2################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def filter_billed_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only rows where 'Category for Static' is 'Billed' (case-insensitive).\n",
    "    \"\"\"\n",
    "    return df[df['Category for Static'].astype(str).str.strip().str.lower() == 'billed'].copy()\n",
    "\n",
    "def calculate_percentage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds Total FTE per AID and calculates the unrounded percentage (Sum of %) for each row.\n",
    "    \"\"\"\n",
    "    total_fte = df.groupby('AID')['No. of FTE'].sum().rename('Total FTE').reset_index()\n",
    "    df = df.merge(total_fte, on='AID', how='left')\n",
    "    df['Sum of % (Unrounded)'] = (df['No. of FTE'] / df['Total FTE']) * 100\n",
    "    return df\n",
    "\n",
    "def aggregate_fte_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups data by the required columns and calculates sum of No. of FTE and Sum of % (Unrounded).\n",
    "    \"\"\"\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Business Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "    agg_df = df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Sum of % (Unrounded)': 'sum'\n",
    "    })\n",
    "    return agg_df\n",
    "\n",
    "def round_half_up(number: float, decimals: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Custom rounding function: rounds to the nearest value using round-half-up rule (like Excel).\n",
    "    \"\"\"\n",
    "    factor = 10 ** decimals\n",
    "    return float(int(number * factor + 0.5)) / factor\n",
    "\n",
    "def apply_rounding_and_adjustment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rounds Sum of % and adjusts first row per AID so total becomes exactly 100.\n",
    "    \"\"\"\n",
    "    df['Sum of % (Rounded)'] = df['Sum of % (Unrounded)'].apply(lambda x: round_half_up(x, 2))\n",
    "    df['Adjusted Sum of %'] = df['Sum of % (Rounded)']  # Copy for final column\n",
    "\n",
    "    final_rows = []\n",
    "    for aid, group in df.groupby('AID'):\n",
    "        group = group.copy()\n",
    "        total_rounded = group['Sum of % (Rounded)'].sum()\n",
    "        diff = round_half_up(100.0 - total_rounded, 2)\n",
    "\n",
    "        if not group.empty:\n",
    "            idx = group.index[0]\n",
    "            group.at[idx, 'Adjusted Sum of %'] += diff\n",
    "            group['% Adjustment Done'] = group['Adjusted Sum of %'] - group['Sum of % (Rounded)']\n",
    "\n",
    "        final_rows.append(group)\n",
    "\n",
    "    return pd.concat(final_rows, ignore_index=True)\n",
    "\n",
    "def process_fte_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Complete pipeline to filter, calculate percentage, aggregate, and adjust percentages.\n",
    "    \"\"\"\n",
    "    billed_df = filter_billed_data(df)\n",
    "    billed_df = calculate_percentage(billed_df)\n",
    "    aggregated_df = aggregate_fte_data(billed_df)\n",
    "    final_df = apply_rounding_and_adjustment(aggregated_df)\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa62a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def enrich_billing_entity_info(main_df: pd.DataFrame, last_month_df: pd.DataFrame, ospd_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Normalize BPCC and LE_Description\n",
    "    def normalize_bpcc(series):\n",
    "        return series.astype(str).str.strip().str.lstrip('0').str.upper()\n",
    "\n",
    "    main_df['__BPCC_tmp'] = normalize_bpcc(main_df['Business Partner Cost Center'])\n",
    "    last_month_df['__BPCC_tmp'] = normalize_bpcc(last_month_df['Business Partner Cost Center'])\n",
    "    ospd_df['__BPCC_tmp'] = normalize_bpcc(ospd_df['CC_ID'])\n",
    "\n",
    "    # Normalize LE_Description\n",
    "    ospd_df['__LE_tmp'] = ospd_df['LE_Description'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Initialize columns\n",
    "    if 'Comment' not in main_df.columns:\n",
    "        main_df['Comment'] = \"\"\n",
    "    main_df['Billing Entity name as per OSPD'] = \"\"\n",
    "\n",
    "    # Sets for lookup\n",
    "    bpcc_last_month_set = set(last_month_df['__BPCC_tmp'])\n",
    "    bpcc_ospd_set = set(ospd_df['__BPCC_tmp'])\n",
    "\n",
    "    for idx, row in main_df.iterrows():\n",
    "        bpcc = row['__BPCC_tmp']\n",
    "\n",
    "        # Skip IPAC, NaN, or empty BPCC\n",
    "        if pd.isna(bpcc) or bpcc == \"\" or bpcc == \"IPAC\":\n",
    "            continue\n",
    "\n",
    "        billing_entity = str(row['Billing Entity']).replace(\"_\", \" \").strip().upper()\n",
    "\n",
    "        # Case 1: BPCC not found in OSPD\n",
    "        if bpcc not in bpcc_ospd_set:\n",
    "            main_df.at[idx, 'Comment'] = \"No BPCC found in OSPD.\"\n",
    "            continue\n",
    "\n",
    "        # Case 2: New BPCC (not in last month file)\n",
    "        if bpcc not in bpcc_last_month_set:\n",
    "            matching_ospd_entries = ospd_df[ospd_df['__BPCC_tmp'] == bpcc]['__LE_tmp'].dropna().unique()\n",
    "            combined_le = \" | \".join(sorted(set(matching_ospd_entries)))\n",
    "\n",
    "            main_df.at[idx, 'Billing Entity name as per OSPD'] = combined_le\n",
    "\n",
    "            # Compare with cleaned Billing Entity from main_df\n",
    "            if billing_entity not in matching_ospd_entries:\n",
    "                comment = f\"New BPCC found. Billing Entity mismatch. Expected from OSPD: {combined_le}\"\n",
    "                main_df.at[idx, 'Comment'] = comment\n",
    "\n",
    "    # Cleanup temporary columns\n",
    "    main_df.drop(columns=['__BPCC_tmp'], inplace=True)\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def flag_duplicate_billing_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure required columns exist\n",
    "    name_col = 'Billing Contact Name'\n",
    "    psid_col = 'Billing Contact PS ID'\n",
    "    comment_col = 'Comment'\n",
    "\n",
    "    if name_col not in df.columns or psid_col not in df.columns:\n",
    "        raise KeyError(\"Missing required columns in the dataframe.\")\n",
    "\n",
    "    # Ensure 'Comment' column exists\n",
    "    if comment_col not in df.columns:\n",
    "        df[comment_col] = \"\"\n",
    "\n",
    "    # --------- Case 1: Multiple Names for Same PS ID ---------\n",
    "    psid_grouped = df.groupby(psid_col)[name_col].unique().reset_index()\n",
    "\n",
    "    for _, row in psid_grouped.iterrows():\n",
    "        psid = row[psid_col]\n",
    "        names = row[name_col]\n",
    "\n",
    "        if len(names) <= 1:\n",
    "            continue\n",
    "\n",
    "        normalized_names = [str(n).strip().lower() for n in names]\n",
    "\n",
    "        if all(n == normalized_names[0] for n in normalized_names):\n",
    "            continue  # All names are same after normalization\n",
    "\n",
    "        name_names = [n for n in names if '@' not in str(n)]\n",
    "        email_names = [n for n in names if '@' in str(n)]\n",
    "\n",
    "        is_likely_same_person = False\n",
    "\n",
    "        for name in name_names:\n",
    "            name_words = str(name).lower().split()\n",
    "            for email in email_names:\n",
    "                email_lower = str(email).lower()\n",
    "                if any(word in email_lower for word in name_words):\n",
    "                    is_likely_same_person = True\n",
    "                    break\n",
    "            if is_likely_same_person:\n",
    "                break\n",
    "\n",
    "        # Decide the final comment\n",
    "        if is_likely_same_person:\n",
    "            final_comment = 'Duplicate name against same PS ID, might be same person.'\n",
    "        else:\n",
    "            final_comment = 'Same PS ID has unrelated names (likely different people).'\n",
    "\n",
    "        # Apply comment (replace or append)\n",
    "        idxs = df[df[psid_col] == psid].index\n",
    "        for idx in idxs:\n",
    "            existing = str(df.at[idx, comment_col])\n",
    "            if final_comment not in existing:\n",
    "                df.at[idx, comment_col] = final_comment\n",
    "\n",
    "    # --------- Case 2: Multiple PS IDs for Same Name ---------\n",
    "    name_grouped = df.groupby(name_col)[psid_col].nunique().reset_index()\n",
    "    multi_psid_names = name_grouped[name_grouped[psid_col] > 1][name_col]\n",
    "\n",
    "    for name in multi_psid_names:\n",
    "        idxs = df[df[name_col] == name].index\n",
    "        for idx in idxs:\n",
    "            existing = str(df.at[idx, comment_col])\n",
    "            if 'Same contact name appears with multiple PS IDs.' not in existing:\n",
    "                df.at[idx, comment_col] = 'Same contact name appears with multiple PS IDs.'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def flag_duplicate_billing_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure required columns exist\n",
    "    name_col = 'Billing Contact Name'\n",
    "    psid_col = 'Billing Contact PS ID'\n",
    "    comment_col = 'Comment'\n",
    "\n",
    "    if name_col not in df.columns or psid_col not in df.columns:\n",
    "        raise KeyError(\"Missing required columns in the dataframe.\")\n",
    "\n",
    "    # Ensure 'Comment' column exists\n",
    "    if comment_col not in df.columns:\n",
    "        df[comment_col] = \"\"\n",
    "\n",
    "    # --------- Case 1: Multiple Names for Same PS ID ---------\n",
    "    psid_grouped = df.groupby(psid_col)[name_col].unique().reset_index()\n",
    "\n",
    "    for _, row in psid_grouped.iterrows():\n",
    "        psid = row[psid_col]\n",
    "        names = row[name_col]\n",
    "\n",
    "        if len(names) <= 1:\n",
    "            continue  # Only one name, no duplicates\n",
    "\n",
    "        name_names = [n for n in names if '@' not in str(n)]\n",
    "        email_names = [n for n in names if '@' in str(n)]\n",
    "\n",
    "        match_found = False\n",
    "\n",
    "        for name in name_names:\n",
    "            name_words = str(name).lower().split()\n",
    "            for email in email_names:\n",
    "                email_lower = str(email).lower()\n",
    "                if any(word in email_lower for word in name_words):\n",
    "                    match_found = True\n",
    "                    idxs = df[(df[psid_col] == psid) & (df[name_col].isin([name, email]))].index\n",
    "                    for idx in idxs:\n",
    "                        old_comment = str(df.at[idx, comment_col])\n",
    "                        new_comment = 'Duplicate name against same PS ID, might be same person.'\n",
    "                        if new_comment not in old_comment:\n",
    "                            updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                            df.at[idx, comment_col] = updated_comment\n",
    "\n",
    "        if not match_found:\n",
    "            # Add comment for all rows with this PS ID\n",
    "            idxs = df[df[psid_col] == psid].index\n",
    "            for idx in idxs:\n",
    "                old_comment = str(df.at[idx, comment_col])\n",
    "                new_comment = 'Same BC PS ID found against multiple BC names â€“ unlikely to be same person.'\n",
    "                if new_comment not in old_comment:\n",
    "                    updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                    df.at[idx, comment_col] = updated_comment\n",
    "\n",
    "    # --------- Case 2: Multiple PS IDs for Same Name ---------\n",
    "    name_grouped = df.groupby(name_col)[psid_col].nunique().reset_index()\n",
    "    multi_psid_names = name_grouped[name_grouped[psid_col] > 1][name_col]\n",
    "\n",
    "    for name in multi_psid_names:\n",
    "        idxs = df[df[name_col] == name].index\n",
    "        for idx in idxs:\n",
    "            old_comment = str(df.at[idx, comment_col])\n",
    "            new_comment = 'Same contact name appears with multiple PS IDs.'\n",
    "            if new_comment not in old_comment:\n",
    "                updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                df.at[idx, comment_col] = updated_comment\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_fte_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter only Billed rows (case-insensitive)\n",
    "    billed_df = df[df['Category for Static'].str.lower() == 'billed'].copy()\n",
    "\n",
    "    # Calculate Total FTE per AID\n",
    "    total_fte_by_aid = billed_df.groupby('AID')['No. of FTE'].sum()\n",
    "    billed_df['Total FTE'] = billed_df['AID'].map(total_fte_by_aid)\n",
    "\n",
    "    # Calculate Sum of % = (No. of FTE / Total FTE) * 100\n",
    "    billed_df['Sum of %'] = (billed_df['No. of FTE'] / billed_df['Total FTE']) * 100\n",
    "\n",
    "    # Round and adjust % so sum is exactly 100 per AID\n",
    "    def adjust_percentage(group):\n",
    "        group['Sum of %'] = group['Sum of %'].round(2)\n",
    "        total = group['Sum of %'].sum()\n",
    "        diff = round(100.00 - total, 2)\n",
    "\n",
    "        if abs(diff) > 0:  # Only adjust if needed\n",
    "            # Adjust the last row in the group\n",
    "            group.iloc[-1, group.columns.get_loc('Sum of %')] += diff\n",
    "\n",
    "        return group\n",
    "\n",
    "    billed_df = billed_df.groupby('AID', group_keys=False).apply(adjust_percentage)\n",
    "\n",
    "    return billed_df\n",
    "\n",
    "def create_fte_pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df = calculate_fte_summary(df)\n",
    "\n",
    "    # Only fill missing in other fields (not BP CC1 or BP CC2)\n",
    "    columns_to_fill = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Billing Contact PS ID'\n",
    "    ]\n",
    "    billed_df[columns_to_fill] = billed_df[columns_to_fill].fillna(\"Missing\")\n",
    "\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Billing Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "\n",
    "    summary_df = billed_df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Sum of %': 'sum'\n",
    "    })\n",
    "\n",
    "    summary_df['Sum of %'] = summary_df['Sum of %'].round(2)\n",
    "    summary_df.rename(columns={'No. of FTE': 'Sum of No. of FTE'}, inplace=True)\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920546bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_fte_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter only Billed rows (case-insensitive)\n",
    "    billed_df = df[df['Category for Static'].str.lower() == 'billed'].copy()\n",
    "\n",
    "    # Calculate Total FTE per AID\n",
    "    total_fte_by_aid = billed_df.groupby('AID')['No. of FTE'].sum()\n",
    "    billed_df['Total FTE'] = billed_df['AID'].map(total_fte_by_aid)\n",
    "\n",
    "    # Calculate Sum of % = (No. of FTE / Total FTE) * 100\n",
    "    billed_df['Sum of %'] = (billed_df['No. of FTE'] / billed_df['Total FTE']) * 100\n",
    "\n",
    "    return billed_df\n",
    "\n",
    "def create_fte_pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Get summary from your function\n",
    "    billed_df = calculate_fte_summary(df)\n",
    "\n",
    "    # Just round 'Sum of %' to 2 decimals and rename column\n",
    "    billed_df['Sum of %'] = billed_df['Sum of %'].round(2)\n",
    "    billed_df.rename(columns={'No. of FTE': 'Sum of No. of FTE'}, inplace=True)\n",
    "\n",
    "    # Only keep the required columns (no grouping or aggregation)\n",
    "    summary_df = billed_df[\n",
    "        ['AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "         'Billing Contact PS ID', 'BP CC1', 'BP CC2', 'Sum of No. of FTE', 'Sum of %']\n",
    "    ].copy()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "## or\n",
    "\n",
    "def create_fte_pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    billed_df = calculate_fte_summary(df)\n",
    "\n",
    "    # Fill missing values temporarily to avoid dropping during groupby\n",
    "    billed_df.fillna(\"Missing\", inplace=True)\n",
    "\n",
    "    group_cols = [\n",
    "        'AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "        'Billing Contact PS ID', 'BP CC1', 'BP CC2'\n",
    "    ]\n",
    "\n",
    "    summary_df = billed_df.groupby(group_cols, as_index=False).agg({\n",
    "        'No. of FTE': 'sum',\n",
    "        'Sum of %': 'sum'\n",
    "    })\n",
    "\n",
    "    summary_df['Sum of %'] = summary_df['Sum of %'].round(2)\n",
    "    summary_df.rename(columns={'No. of FTE': 'Sum of No. of FTE'}, inplace=True)\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e57c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_fte_summary_table(billed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Rename Total FTE for clarity\n",
    "    billed_df['Sum of No. of FTE'] = billed_df['Total FTE']\n",
    "    \n",
    "    # Round Sum of % to two decimals\n",
    "    billed_df['Sum of %'] = billed_df['Sum of %'].round(2)\n",
    "\n",
    "    # Columns required in final output\n",
    "    cols = ['AID', 'Billing Entity', 'Business Partner Cost Center', 'Currency',\n",
    "            'Billing Contact PS ID', 'BP CC1', 'BP CC2', 'Sum of No. of FTE', 'Sum of %']\n",
    "    output_df = billed_df[cols].copy()\n",
    "\n",
    "    # Add adjustment column initialized as 0.0\n",
    "    output_df['Adjustment Applied'] = 0.0\n",
    "\n",
    "    # Group by AID and check % sum\n",
    "    for aid, group in output_df.groupby('AID'):\n",
    "        percent_sum = group['Sum of %'].sum().round(2)\n",
    "        adjustment_needed = round(100.0 - percent_sum, 2)\n",
    "\n",
    "        if adjustment_needed != 0.0:\n",
    "            # Apply the adjustment to the first row of this AID\n",
    "            idx = group.index[0]\n",
    "            output_df.at[idx, 'Sum of %'] += adjustment_needed\n",
    "            output_df.at[idx, 'Sum of %'] = round(output_df.at[idx, 'Sum of %'], 2)\n",
    "            output_df.at[idx, 'Adjustment Applied'] = adjustment_needed\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf408c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def flag_duplicate_billing_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure required columns exist\n",
    "    name_col = 'Billing Contact Name'\n",
    "    psid_col = 'Billing Contact PS ID'\n",
    "    comment_col = 'Comment'\n",
    "\n",
    "    if name_col not in df.columns or psid_col not in df.columns:\n",
    "        raise KeyError(\"Missing required columns in the dataframe.\")\n",
    "\n",
    "    # Ensure 'Comment' column exists\n",
    "    if comment_col not in df.columns:\n",
    "        df[comment_col] = \"\"\n",
    "\n",
    "    # --------- Case 1: Multiple Names for Same PS ID ---------\n",
    "    psid_grouped = df.groupby(psid_col)[name_col].unique().reset_index()\n",
    "\n",
    "    for _, row in psid_grouped.iterrows():\n",
    "        psid = row[psid_col]\n",
    "        names = row[name_col]\n",
    "\n",
    "        name_names = [n for n in names if '@' not in str(n)]\n",
    "        email_names = [n for n in names if '@' in str(n)]\n",
    "\n",
    "        for name in name_names:\n",
    "            name_words = str(name).lower().split()\n",
    "            for email in email_names:\n",
    "                email_lower = str(email).lower()\n",
    "                if any(word in email_lower for word in name_words):\n",
    "                    idxs = df[(df[psid_col] == psid) & (df[name_col].isin([name, email]))].index\n",
    "                    for idx in idxs:\n",
    "                        old_comment = str(df.at[idx, comment_col])\n",
    "                        new_comment = 'Duplicate name against same PS ID, might be same person.'\n",
    "                        if new_comment not in old_comment:\n",
    "                            updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                            df.at[idx, comment_col] = updated_comment\n",
    "\n",
    "    # --------- Case 2: Multiple PS IDs for Same Name ---------\n",
    "    name_grouped = df.groupby(name_col)[psid_col].nunique().reset_index()\n",
    "    multi_psid_names = name_grouped[name_grouped[psid_col] > 1][name_col]\n",
    "\n",
    "    for name in multi_psid_names:\n",
    "        idxs = df[df[name_col] == name].index\n",
    "        for idx in idxs:\n",
    "            old_comment = str(df.at[idx, comment_col])\n",
    "            new_comment = 'Same contact name appears with multiple PS IDs.'\n",
    "            if new_comment not in old_comment:\n",
    "                updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                df.at[idx, comment_col] = updated_comment\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def flag_duplicate_billing_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flags records where a Billing Contact PSID is linked to multiple Billing Contact Names,\n",
    "    and suggests if the email and name might belong to the same person.\n",
    "    Also flags records where the same Billing Contact Name is used with multiple PSIDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'Billing Contact PSID' not in df.columns or 'Billing Contact name' not in df.columns:\n",
    "        raise KeyError(\"Missing required columns in the dataframe.\")\n",
    "\n",
    "    # Ensure 'Comment' column exists\n",
    "    if 'Comment' not in df.columns:\n",
    "        df['Comment'] = \"\"\n",
    "\n",
    "    # --- Pass 1: Multiple names for same PSID ---\n",
    "    grouped_by_psid = df.groupby('Billing Contact PSID')['Billing Contact name'].unique().reset_index()\n",
    "\n",
    "    for _, row in grouped_by_psid.iterrows():\n",
    "        psid = row['Billing Contact PSID']\n",
    "        names = row['Billing Contact name']\n",
    "\n",
    "        name_names = [n for n in names if '@' not in str(n)]\n",
    "        email_names = [n for n in names if '@' in str(n)]\n",
    "\n",
    "        for name in name_names:\n",
    "            name_words = str(name).lower().split()\n",
    "            for email in email_names:\n",
    "                email_lower = str(email).lower()\n",
    "                if any(word in email_lower for word in name_words):\n",
    "                    idxs = df[(df['Billing Contact PSID'] == psid) & \n",
    "                              (df['Billing Contact name'].isin([name, email]))].index\n",
    "                    for idx in idxs:\n",
    "                        old_comment = str(df.at[idx, 'Comment'])\n",
    "                        new_comment = 'Duplicate name against same PSID, might be same person.'\n",
    "                        if new_comment not in old_comment:\n",
    "                            updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                            df.at[idx, 'Comment'] = updated_comment\n",
    "\n",
    "    # --- Pass 2: Same name for multiple PSIDs ---\n",
    "    grouped_by_name = df.groupby('Billing Contact name')['Billing Contact PSID'].nunique().reset_index()\n",
    "\n",
    "    for _, row in grouped_by_name.iterrows():\n",
    "        name = row['Billing Contact name']\n",
    "        psid_count = row['Billing Contact PSID']\n",
    "        if psid_count > 1:\n",
    "            idxs = df[df['Billing Contact name'] == name].index\n",
    "            for idx in idxs:\n",
    "                old_comment = str(df.at[idx, 'Comment'])\n",
    "                new_comment = 'Same contact name appears against multiple PSIDs.'\n",
    "                if new_comment not in old_comment:\n",
    "                    updated_comment = f\"{old_comment} | {new_comment}\".strip(\" |\")\n",
    "                    df.at[idx, 'Comment'] = updated_comment\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_billing_entities(main_df, last_month_df):\n",
    "    # Create working copies\n",
    "    df = main_df.copy()\n",
    "    last_df = last_month_df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    bpcc_col = 'Business Partner Cost Center'\n",
    "    billing_col = 'Billing Entity'\n",
    "\n",
    "    # Add helper lowercase columns for internal logic\n",
    "    df['bpcc_lower'] = df[bpcc_col].astype(str).str.strip().str.lower()\n",
    "    df['billing_lower'] = df[billing_col].astype(str).str.strip().str.lower()\n",
    "    last_df['bpcc_lower'] = last_df[bpcc_col].astype(str).str.strip().str.lower()\n",
    "    last_df['billing_lower'] = last_df[billing_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Create mapping of last month's BPCC -> billing entity\n",
    "    last_month_map = last_df.drop_duplicates('bpcc_lower').set_index('bpcc_lower')['billing_lower'].to_dict()\n",
    "    last_month_orig_case_map = last_df.drop_duplicates('bpcc_lower').set_index('bpcc_lower')[billing_col].to_dict()\n",
    "\n",
    "    # Create result columns\n",
    "    df['Original Billing Entity'] = df[billing_col]\n",
    "    df['Comments'] = \"\"\n",
    "\n",
    "    # Step 1: Normalize billing entity values based on last month's file\n",
    "    grouped = df.groupby('bpcc_lower')['billing_lower'].unique()\n",
    "\n",
    "    for bpcc, billing_list in grouped.items():\n",
    "        if len(billing_list) > 1 and bpcc in last_month_map:\n",
    "            correct = last_month_map[bpcc]\n",
    "            correct_case = last_month_orig_case_map[bpcc]\n",
    "\n",
    "            for billing in billing_list:\n",
    "                if billing != correct:\n",
    "                    # Check if correct is a substring\n",
    "                    if correct in billing or billing in correct:\n",
    "                        # Replace in df\n",
    "                        mask = (df['bpcc_lower'] == bpcc) & (df['billing_lower'] == billing)\n",
    "                        df.loc[mask, billing_col] = correct_case\n",
    "                        df.loc[mask, 'Comments'] = (\n",
    "                            \"Billing Entity replaced based on last month file. Original: \" +\n",
    "                            df.loc[mask, 'Original Billing Entity']\n",
    "                        )\n",
    "                    else:\n",
    "                        # Mark as multiple conflicting entities\n",
    "                        mask = (df['bpcc_lower'] == bpcc) & (df['billing_lower'] == billing)\n",
    "                        df.loc[mask, 'Comments'] = \"Multiple billing entities found (not matched to last month value)\"\n",
    "\n",
    "    # Step 2: Check for new combinations (bpcc + billing entity)\n",
    "    df['bpcc_billing_combo'] = df['bpcc_lower'] + '|' + df[billing_col].astype(str).str.strip().str.lower()\n",
    "    last_df['bpcc_billing_combo'] = last_df['bpcc_lower'] + '|' + last_df[billing_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    last_combos = set(last_df['bpcc_billing_combo'])\n",
    "\n",
    "    # Only check BPCCs not starting with '9'\n",
    "    mask_non9 = ~df[bpcc_col].astype(str).str.strip().str.startswith('9')\n",
    "    mask_new_combo = ~df['bpcc_billing_combo'].isin(last_combos)\n",
    "\n",
    "    df.loc[mask_non9 & mask_new_combo, 'Comments'] = df.loc[mask_non9 & mask_new_combo, 'Comments'].apply(\n",
    "        lambda x: (x + \" | \" if x else \"\") + \"New billing cc found\"\n",
    "    )\n",
    "\n",
    "    # Clean up helper columns\n",
    "    df.drop(columns=['bpcc_lower', 'billing_lower', 'bpcc_billing_combo'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_new_bpce_combinations(main_df, last_month_df):\n",
    "    # Clean for matching\n",
    "    def clean_for_key(series):\n",
    "        return series.astyp`e(str).str.strip().str.lstrip('0')\n",
    "\n",
    "    # Create temp cleaned BPCC keys\n",
    "    main_df['Temp_BPCC'] = clean_for_key(main_df['Business Partner Cost Center'])\n",
    "    last_month_df['Temp_BPCC'] = clean_for_key(last_month_df['Business Partner Cost Center'])\n",
    "\n",
    "    # Create lookup dictionary from last_month_df\n",
    "    bpcc_billing_map = last_month_df.dropna(subset=['Billing Entity']).drop_duplicates('Temp_BPCC').set_index('Temp_BPCC')['Billing Entity'].to_dict()\n",
    "\n",
    "    # Track old values and apply update\n",
    "    if 'Comment' not in main_df.columns:\n",
    "        main_df['Comment'] = ''\n",
    "\n",
    "    updated_rows = main_df['Temp_BPCC'].isin(bpcc_billing_map.keys())\n",
    "    main_df.loc[updated_rows, 'Old Billing Entity'] = main_df.loc[updated_rows, 'Billing Entity']\n",
    "    main_df.loc[updated_rows, 'Billing Entity'] = main_df.loc[updated_rows, 'Temp_BPCC'].map(bpcc_billing_map)\n",
    "    main_df.loc[updated_rows, 'Comment'] += main_df.loc[updated_rows].apply(\n",
    "        lambda row: f\"Billing Entity updated from '{row['Old Billing Entity']}' to '{row['Billing Entity']}'\", axis=1\n",
    "    )\n",
    "\n",
    "    # Create key after update\n",
    "    main_df['BPCC_Billing_Key'] = main_df['Temp_BPCC'] + main_df['Billing Entity'].astype(str).str.strip()\n",
    "    last_month_df['BPCC_Billing_Key'] = last_month_df['Temp_BPCC'] + last_month_df['Billing Entity'].astype(str).str.strip()\n",
    "\n",
    "    unmatched_mask = ~main_df['BPCC_Billing_Key'].isin(last_month_df['BPCC_Billing_Key'])\n",
    "\n",
    "    # Add comment for unmatched\n",
    "    comment_text = 'Business Partner Cost Center & Billing Entity not found in last month file'\n",
    "    main_df.loc[unmatched_mask & main_df['Comment'].eq(''), 'Comment'] = comment_text\n",
    "    main_df.loc[unmatched_mask & main_df['Comment'].ne('') & ~main_df['Comment'].str.contains(comment_text, case=False), 'Comment'] += ' | ' + comment_text\n",
    "\n",
    "    # Clean up\n",
    "    main_df.drop(columns=['Temp_BPCC', 'BPCC_Billing_Key', 'Old Billing Entity'], errors='ignore', inplace=True)\n",
    "    last_month_df.drop(columns=['Temp_BPCC', 'BPCC_Billing_Key'], inplace=True)\n",
    "\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d641a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "  BP ccenter\n",
      "0     00123 \n",
      "1       0456\n",
      "2        NaN\n",
      "3    000789 \n",
      "4    abc123 \n",
      "5       None\n",
      "6      0000 \n",
      "\n",
      "After trimming spaces (leading zeros retained, NaNs intact):\n",
      "  BP ccenter\n",
      "0      00123\n",
      "1       0456\n",
      "2        NaN\n",
      "3     000789\n",
      "4     abc123\n",
      "5       None\n",
      "6       0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'BP ccenter': [' 00123 ', ' 0456', np.nan, '000789 ', ' abc123 ', None, ' 0000 ']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# Trim spaces but keep leading zeros and NaNs as NaN\n",
    "df['BP ccenter'] = df['BP ccenter'].apply(lambda x: str(x).strip() if pd.notnull(x) else x)\n",
    "\n",
    "print(\"\\nAfter trimming spaces (leading zeros retained, NaNs intact):\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_billing_entity_mismatches(main_df):\n",
    "    # Step 1: Create cleaned versions from both columns\n",
    "    def clean_entity(entity):\n",
    "        entity = str(entity).strip()\n",
    "        return entity.rsplit('_', 1)[-1].strip().lower()\n",
    "    \n",
    "    main_df['Billing Entity (cleaned)'] = main_df['Billing Entity'].apply(clean_entity)\n",
    "    main_df['Billing Entity from OSPD (cleaned)'] = main_df['Billing Entity name as per OSPD'].apply(clean_entity)\n",
    "\n",
    "    # Step 2: Compare cleaned columns (case-insensitive)\n",
    "    mismatch_mask = (\n",
    "        (main_df['Billing Entity (cleaned)'].notna()) &\n",
    "        (main_df['Billing Entity from OSPD (cleaned)'].notna()) &\n",
    "        (main_df['Billing Entity (cleaned)'] != main_df['Billing Entity from OSPD (cleaned)'])\n",
    "    )\n",
    "\n",
    "    # Step 3: Add a comment where mismatches occur\n",
    "    main_df['Billing Entity Mismatch Comment'] = ''\n",
    "    main_df.loc[mismatch_mask, 'Billing Entity Mismatch Comment'] = (\n",
    "        'Mismatch after cleaning: '\n",
    "        'Billing Entity = \"' + main_df['Billing Entity'] + '\", '\n",
    "        'OSPD = \"' + main_df['Billing Entity name as per OSPD'] + '\"'\n",
    "    )\n",
    "\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_currency_column(df):\n",
    "    \"\"\"\n",
    "    Adds a 'Currency' column based on GFOC Country and Country columns.\n",
    "    If both countries match and the country is in predefined list, assign local currency.\n",
    "    Otherwise, assign 'USD'.\n",
    "    \"\"\"\n",
    "    # Mapping dictionary\n",
    "    country_currency_map = {\n",
    "        \"Mexico\": \"MXN\",\n",
    "        \"India\": \"INR\",\n",
    "        \"Poland\": \"PLN\",\n",
    "        \"China\": \"CNY\"\n",
    "    }\n",
    "\n",
    "    def determine_currency(row):\n",
    "        if pd.isna(row['GFOC Country']) or pd.isna(row['Country']):\n",
    "            return \"USD\"\n",
    "        if row['GFOC Country'].strip().lower() == row['Country'].strip().lower():\n",
    "            return country_currency_map.get(row['Country'].strip(), \"USD\")\n",
    "        else:\n",
    "            return \"USD\"\n",
    "\n",
    "    df['Currency'] = df.apply(determine_currency, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_multiple_billing_entities(main_df):\n",
    "    \"\"\"\n",
    "    Detects and resolves mismatches in Billing Entity for the same Business Partner Cost Center.\n",
    "    - Replaces mismatching entries if substring logic applies.\n",
    "    - Flags all rows when mismatch cannot be resolved.\n",
    "    \n",
    "    Returns:\n",
    "    - updated_df: DataFrame with corrected Billing Entity and comments\n",
    "    - unresolved_rows_df: DataFrame with unresolved mismatches for logging or review\n",
    "    \"\"\"\n",
    "    df = main_df.copy()\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df['Business Partner Cost Center'] = df['Business Partner Cost Center'].astype(str)\n",
    "    df['Billing Entity'] = df['Billing Entity'].astype(str)\n",
    "\n",
    "    # Create comment column if not present\n",
    "    comment_col = 'Billing Entity Validation Comment'\n",
    "    if comment_col not in df.columns:\n",
    "        df[comment_col] = ''\n",
    "\n",
    "    # Track unresolved rows\n",
    "    unresolved_rows = []\n",
    "\n",
    "    # Identify BPCCs linked to multiple billing entities\n",
    "    multi_bpccs = df.groupby('Business Partner Cost Center')['Billing Entity'].nunique()\n",
    "    multi_bpccs = multi_bpccs[multi_bpccs > 1].index\n",
    "\n",
    "    for bpcc in multi_bpccs:\n",
    "        subset = df[df['Business Partner Cost Center'] == bpcc]\n",
    "        entities = subset['Billing Entity'].str.lower().unique()\n",
    "        entity_counts = subset['Billing Entity'].str.lower().value_counts()\n",
    "\n",
    "        # Try to find a good candidate (underscore & appears multiple times)\n",
    "        correct_candidates = [e for e in entity_counts.index if '_' in e]\n",
    "        if correct_candidates:\n",
    "            correct_entity = correct_candidates[0]\n",
    "            correct_original = subset[\n",
    "                subset['Billing Entity'].str.lower() == correct_entity\n",
    "            ]['Billing Entity'].iloc[0]\n",
    "\n",
    "            mismatch_unresolved = False\n",
    "\n",
    "            for ent in entities:\n",
    "                if ent == correct_entity:\n",
    "                    continue\n",
    "                if ent in correct_entity:\n",
    "                    mask = (\n",
    "                        (df['Business Partner Cost Center'] == bpcc) &\n",
    "                        (df['Billing Entity'].str.lower() == ent)\n",
    "                    )\n",
    "                    old_vals = df.loc[mask, 'Billing Entity']\n",
    "                    df.loc[mask, 'Billing Entity'] = correct_original\n",
    "                    df.loc[mask, comment_col] += (\n",
    "                        \"Billing Entity replaced from '\" +\n",
    "                        old_vals + \"' to '\" + correct_original +\n",
    "                        \"' based on substring match logic.\"\n",
    "                    )\n",
    "                else:\n",
    "                    mismatch_unresolved = True\n",
    "\n",
    "            if mismatch_unresolved:\n",
    "                mask_all = (df['Business Partner Cost Center'] == bpcc)\n",
    "                df.loc[mask_all, comment_col] += (\n",
    "                    \"Multiple distinct Billing Entities found against same BPCC.\"\n",
    "                )\n",
    "                unresolved_rows.append(df[mask_all])\n",
    "        else:\n",
    "            # No good candidate at all\n",
    "            mask_all = (df['Business Partner Cost Center'] == bpcc)\n",
    "            df.loc[mask_all, comment_col] += (\n",
    "                \"Multiple distinct Billing Entities found against same BPCC.\"\n",
    "            )\n",
    "            unresolved_rows.append(df[mask_all])\n",
    "\n",
    "    # Combine all unresolved into one DataFrame\n",
    "    if unresolved_rows:\n",
    "        unresolved_rows_df = pd.concat(unresolved_rows, ignore_index=True)\n",
    "    else:\n",
    "        unresolved_rows_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    return df, unresolved_rows_df\n",
    "\n",
    "\n",
    "final_df, unresolved_df = fix_multiple_billing_entities(main_df)\n",
    "\n",
    "# Save if needed\n",
    "with pd.ExcelWriter('output.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    unresolved_df.to_excel(writer, sheet_name='Unresolved Billing Entities', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_multiple_billing_entities(main_df):\n",
    "    \"\"\"\n",
    "    Detects and resolves cases where the same Business Partner Cost Center (BPCC)\n",
    "    is associated with multiple Billing Entities. If a mismatched entity is found\n",
    "    to be a substring of the more common one (typically containing an underscore),\n",
    "    it is replaced and commented.\n",
    "    \"\"\"\n",
    "    # Make a copy for safe manipulation\n",
    "    df = main_df.copy()\n",
    "    \n",
    "    # Ensure string format\n",
    "    df['Business Partner Cost Center'] = df['Business Partner Cost Center'].astype(str)\n",
    "    df['Billing Entity'] = df['Billing Entity'].astype(str)\n",
    "\n",
    "    # Group by BPCC and find those with multiple billing entities\n",
    "    multi_entity_bpcc = df.groupby('Business Partner Cost Center')['Billing Entity'].nunique()\n",
    "    multi_entity_bpcc = multi_entity_bpcc[multi_entity_bpcc > 1].index\n",
    "\n",
    "    # Create comment column if not already present\n",
    "    comment_col = 'Billing Entity Validation Comment'\n",
    "    if comment_col not in df.columns:\n",
    "        df[comment_col] = ''\n",
    "\n",
    "    for bpcc in multi_entity_bpcc:\n",
    "        subset = df[df['Business Partner Cost Center'] == bpcc]\n",
    "        entities = subset['Billing Entity'].str.lower().unique()\n",
    "\n",
    "        # Count frequencies to find the most frequent entity with underscores\n",
    "        entity_counts = subset['Billing Entity'].str.lower().value_counts()\n",
    "        correct_entities = [e for e in entity_counts.index if '_' in e]\n",
    "        if not correct_entities:\n",
    "            continue  # No candidate \"correct\" entries to fix against\n",
    "\n",
    "        correct_entity = correct_entities[0]  # Most frequent with \"_\"\n",
    "        correct_entity_original_case = subset[\n",
    "            subset['Billing Entity'].str.lower() == correct_entity\n",
    "        ]['Billing Entity'].iloc[0]\n",
    "\n",
    "        # Find mismatched entities that are substrings of correct one\n",
    "        for ent in entities:\n",
    "            if ent == correct_entity:\n",
    "                continue\n",
    "            if ent in correct_entity:\n",
    "                # Find rows with this entity and fix\n",
    "                match_mask = (\n",
    "                    (df['Business Partner Cost Center'] == bpcc) &\n",
    "                    (df['Billing Entity'].str.lower() == ent)\n",
    "                )\n",
    "                old_value = df.loc[match_mask, 'Billing Entity']\n",
    "                df.loc[match_mask, 'Billing Entity'] = correct_entity_original_case\n",
    "                df.loc[match_mask, comment_col] += (\n",
    "                    \"Billing Entity replaced from '\" +\n",
    "                    old_value + \"' to '\" + correct_entity_original_case +\n",
    "                    \"' based on substring match logic.\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # Otherwise comment as unresolved mismatch\n",
    "                mismatch_mask = (\n",
    "                    (df['Business Partner Cost Center'] == bpcc) &\n",
    "                    (df['Billing Entity'].str.lower() == ent)\n",
    "                )\n",
    "                df.loc[mismatch_mask, comment_col] += (\n",
    "                    \"Multiple distinct Billing Entities found against same BPCC.\"\n",
    "                )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b365dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_new_bpce_combinations(main_df, last_month_df):\n",
    "    # Clean for concatenation purpose only (strip, lstrip zeroes)\n",
    "    def clean_for_key(series):\n",
    "        return series.astype(str).str.strip().str.lstrip('0')\n",
    "\n",
    "    # Create temporary cleaned keys for matching\n",
    "    main_df['Temp_BPCC'] = clean_for_key(main_df['Business Partner Cost Center'])\n",
    "    last_month_df['Temp_BPCC'] = clean_for_key(last_month_df['Business Partner Cost Center'])\n",
    "\n",
    "    main_df['BPCC_Billing_Key'] = main_df['Temp_BPCC'] + main_df['Billing Entity'].astype(str).str.strip()\n",
    "    last_month_df['BPCC_Billing_Key'] = last_month_df['Temp_BPCC'] + last_month_df['Billing Entity'].astype(str).str.strip()\n",
    "\n",
    "    # Identify unmatched combinations\n",
    "    unmatched_mask = ~main_df['BPCC_Billing_Key'].isin(last_month_df['BPCC_Billing_Key'])\n",
    "\n",
    "    # Comment logic\n",
    "    comment_text = 'Business Partner Cost Center & Billing Entity not found in last month file'\n",
    "    if 'Comment' not in main_df.columns:\n",
    "        main_df['Comment'] = ''\n",
    "\n",
    "    main_df.loc[unmatched_mask & main_df['Comment'].eq(''), 'Comment'] = comment_text\n",
    "    main_df.loc[unmatched_mask & main_df['Comment'].ne('') & ~main_df['Comment'].str.contains(comment_text, case=False), 'Comment'] += ' | ' + comment_text\n",
    "\n",
    "    # Drop temporary keys\n",
    "    main_df.drop(columns=['Temp_BPCC', 'BPCC_Billing_Key'], inplace=True)\n",
    "    last_month_df.drop(columns=['Temp_BPCC', 'BPCC_Billing_Key'], inplace=True)\n",
    "\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_currency_column(main_df):\n",
    "    # Define country-to-currency mapping\n",
    "    country_currency_map = {\n",
    "        'MEXICO': 'MXN',\n",
    "        'INDIA': 'INR',\n",
    "        'POLAND': 'PLN',\n",
    "        'CHINA': 'CNY'\n",
    "    }\n",
    "\n",
    "    # Standardize and compare country columns\n",
    "    def determine_currency(row):\n",
    "        gfoc_country = str(row.get('GFOC Country', '')).strip().upper()\n",
    "        country = str(row.get('Country', '')).strip().upper()\n",
    "\n",
    "        if gfoc_country == country:\n",
    "            return country_currency_map.get(gfoc_country, 'USD')\n",
    "        else:\n",
    "            return 'USD'\n",
    "\n",
    "    # Apply logic to each row\n",
    "    main_df['Currency'] = main_df.apply(determine_currency, axis=1)\n",
    "    return main_df\n",
    "main_df = add_currency_column(main_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_duplicate_billing_names(df):\n",
    "    if 'Comment2' not in df.columns:\n",
    "        df['Comment2'] = ''\n",
    "    \n",
    "    def extract_name_tokens(text):\n",
    "        if pd.isna(text):\n",
    "            return set()\n",
    "        text = str(text).strip()\n",
    "        if '@' in text:\n",
    "            text = text.split('@')[0]  # only local part of email\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)  # remove digits/symbols\n",
    "        tokens = text.lower().split()\n",
    "        return set(tokens)\n",
    "\n",
    "    # Group by PSID\n",
    "    psid_groups = df.groupby('Billing Contact PSID')\n",
    "\n",
    "    for psid, group in psid_groups:\n",
    "        names = group['Billing Contact Name'].dropna().unique()\n",
    "        if len(names) <= 1:\n",
    "            continue  # no conflict\n",
    "\n",
    "        # Try to see if all names seem to refer to same person\n",
    "        base_tokens = [extract_name_tokens(name) for name in names]\n",
    "\n",
    "        conflict = False\n",
    "        for i in range(len(base_tokens)):\n",
    "            for j in range(i + 1, len(base_tokens)):\n",
    "                # If intersection is empty => likely different persons\n",
    "                if base_tokens[i].isdisjoint(base_tokens[j]):\n",
    "                    conflict = True\n",
    "                    break\n",
    "            if conflict:\n",
    "                break\n",
    "\n",
    "        if conflict:\n",
    "            df.loc[group.index, 'Comment2'] = df.loc[group.index, 'Comment2'].replace('', 'Billing contact has duplicate names', regex=False)\n",
    "            df.loc[group.index, 'Comment2'] = df.loc[group.index, 'Comment2'].apply(lambda x: x if 'Billing contact has duplicate names' in x else f\"{x} | Billing contact has duplicate names\")\n",
    "\n",
    "    return df\n",
    "main_df = detect_duplicate_billing_names(main_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_billing_contact_psid(df):\n",
    "    def format_psid(val):\n",
    "        val_str = str(val).strip()\n",
    "        if val_str.upper() == 'IPAC':\n",
    "            return val_str  # Leave as is\n",
    "        return val_str.zfill(8) if val_str.isdigit() else val_str\n",
    "\n",
    "    df['Billing Contact PSID'] = df['Billing Contact PSID'].apply(format_psid)\n",
    "    return df\n",
    "# Format the column in your main input file (main_df)\n",
    "main_df = format_billing_contact_psid(main_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_multiple_billing_entities(df):\n",
    "    # Ensure columns are strings\n",
    "    df['Business Partner Cost Center'] = df['Business Partner Cost Center'].astype(str)\n",
    "    df['Billing Entity'] = df['Billing Entity'].astype(str)\n",
    "    \n",
    "    # Create a mapping of cost center to unique billing entities\n",
    "    cc_entity_group = df.groupby('Business Partner Cost Center')['Billing Entity'].nunique().reset_index()\n",
    "    cc_entity_group = cc_entity_group[cc_entity_group['Billing Entity'] > 1]\n",
    "\n",
    "    # List of cost centers with multiple billing entities\n",
    "    conflict_ccs = cc_entity_group['Business Partner Cost Center'].tolist()\n",
    "\n",
    "    # Initialize 'Comment2' column if 'Comment' already exists\n",
    "    if 'Comment' in df.columns:\n",
    "        if 'Comment2' not in df.columns:\n",
    "            df['Comment2'] = ''\n",
    "\n",
    "    # Apply comments row-wise\n",
    "    def comment_logic(row):\n",
    "        if row['Business Partner Cost Center'] in conflict_ccs:\n",
    "            if row['Business Partner Cost Center'].startswith('9'):\n",
    "                return \"Billing cost center begins with digit 9.\"\n",
    "            else:\n",
    "                return \"Multiple billing entities found.\"\n",
    "        return \"\"\n",
    "\n",
    "    # Add comments accordingly\n",
    "    if 'Comment2' in df.columns:\n",
    "        df['Comment2'] = df.apply(lambda row: comment_logic(row), axis=1)\n",
    "    else:\n",
    "        df['Comment'] = df.apply(\n",
    "            lambda row: row['Comment'] + \" | \" + comment_logic(row) if row['Business Partner Cost Center'] in conflict_ccs and row['Comment'] else comment_logic(row),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "main_df = check_multiple_billing_entities(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cb9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before refresh:\n",
      "    PSID     Name      Entity Location\n",
      "0   101    Alice  OldEntity1  OldLoc1\n",
      "1   102      Bob  NewEntity2  NewLoc2\n",
      "2   103  Charlie  OldEntity3  OldLoc3\n",
      "3   104    David  OldEntity4  OldLoc4\n",
      "\n",
      "After refresh:\n",
      "    PSID     Name      Entity Location\n",
      "0   101    Alice  OldEntity1  OldLoc1\n",
      "1   102      Bob  NewEntity2  NewLoc2\n",
      "2   103  Charlie  NewEntity3  NewLoc3\n",
      "3   104    David  NewEntity4  NewLoc4\n"
     ]
    }
   ],
   "source": [
    "def add_billing_entity_name_from_ospd(main_df, ospd_df):\n",
    "    # Ensure both keys are strings for consistent merge\n",
    "    main_df['Business Partner Cost Center'] = main_df['Business Partner Cost Center'].astype(str).str.lstrip('0').str.upper()\n",
    "    ospd_df['CC_ID'] = ospd_df['CC_ID'].astype(str).str.lstrip('0').str.upper()\n",
    "\n",
    "    # Merge to fetch LE_Description as new column\n",
    "    main_df = main_df.merge(\n",
    "        ospd_df[['CC_ID', 'LE_Description']].rename(columns={\n",
    "            'CC_ID': 'Business Partner Cost Center',\n",
    "            'LE_Description': 'Billing Entity name as per OSPD'\n",
    "        }),\n",
    "        on='Business Partner Cost Center',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return main_df\n",
    "main_df = add_billing_entity_name_from_ospd(main_df, ospd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ae93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reposition_psid_and_position_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Repositions 'PSID' column to column D (index 3) and 'Position ID' to column E (index 4).\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame where 'PSID' and 'Position ID' columns exist.\n",
    "\n",
    "    Returns:\n",
    "    - Modified DataFrame with swapped column positions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cols = df.columns.tolist()\n",
    "\n",
    "        if 'PSID' not in cols or 'Position ID' not in cols:\n",
    "            raise ValueError(\"Both 'PSID' and 'Position ID' must be present in the dataframe.\")\n",
    "\n",
    "        # Remove them from the list\n",
    "        cols.remove('PSID')\n",
    "        cols.remove('Position ID')\n",
    "\n",
    "        # Insert at desired positions\n",
    "        cols.insert(3, 'PSID')         # Column D = index 3\n",
    "        cols.insert(4, 'Position ID')  # Column E = index 4\n",
    "\n",
    "        df = df[cols]\n",
    "        print(\"âœ… Successfully repositioned 'PSID' to column D and 'Position ID' to column E.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error while repositioning columns:\", e)\n",
    "        raise\n",
    "        \n",
    "        \n",
    "def refresh_columns_e_to_z(main_df: pd.DataFrame, gha_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Refreshes columns E to Z in main_df using the latest data from gha_df based on PSID.\n",
    "\n",
    "    Parameters:\n",
    "    - main_df: DataFrame containing the main input file (already filtered on PSID).\n",
    "    - gha_df: DataFrame of the combined GHA file.\n",
    "\n",
    "    Returns:\n",
    "    - Updated main_df with refreshed columns E to Z.\n",
    "    \"\"\"\n",
    "\n",
    "    import string\n",
    "\n",
    "    try:\n",
    "        # Get actual column names for Excel columns E to Z\n",
    "        all_columns = main_df.columns.tolist()\n",
    "        start_col = 4  # Column E is index 4\n",
    "        end_col = 25   # Column Z is index 25 (inclusive)\n",
    "\n",
    "        if len(all_columns) < end_col + 1:\n",
    "            raise ValueError(\"Main input file does not have enough columns to cover E to Z (25 columns).\")\n",
    "\n",
    "        cols_to_refresh = all_columns[start_col:end_col + 1]\n",
    "\n",
    "        # Ensure PSID is in both dataframes\n",
    "        if 'PSID' not in main_df.columns or 'PSID' not in gha_df.columns:\n",
    "            raise KeyError(\"'PSID' column must exist in both main_df and gha_df.\")\n",
    "\n",
    "        # Prepare gha_df with only PSID + the refresh columns (ensure those columns exist in GHA)\n",
    "        missing_cols = [col for col in cols_to_refresh if col not in gha_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"The following columns to refresh are missing in GHA file: {', '.join(missing_cols)}\")\n",
    "\n",
    "        refresh_data = gha_df[['PSID'] + cols_to_refresh].drop_duplicates(subset='PSID')\n",
    "\n",
    "        # Merge on PSID to bring in new values\n",
    "        main_df = main_df.drop(columns=cols_to_refresh)\n",
    "        main_df = main_df.merge(refresh_data, on='PSID', how='left')\n",
    "\n",
    "        print(f\"âœ… Columns E to Z refreshed successfully.\")\n",
    "        return main_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error while refreshing columns E to Z:\", e)\n",
    "        raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "main_df = reposition_psid_and_position_id(main_df)\n",
    "main_df = refresh_columns_e_to_z(main_df, combined_gha_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4123a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "import sys\n",
    "\n",
    "# Initialize Tkinter root\n",
    "Tk().withdraw()\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Load Two GHA Files ---\n",
    "    print(\"Select the FIRST GHA file\")\n",
    "    gha_file_1 = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
    "    \n",
    "    print(\"Select the SECOND GHA file\")\n",
    "    gha_file_2 = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
    "\n",
    "    # Read both GHA files\n",
    "    gha_df1 = pd.read_excel(gha_file_1)\n",
    "    gha_df2 = pd.read_excel(gha_file_2)\n",
    "\n",
    "    # Combine them\n",
    "    combined_gha_df = pd.concat([gha_df1, gha_df2], ignore_index=True)\n",
    "\n",
    "    # Make sure PSID column exists\n",
    "    if 'PSID' not in combined_gha_df.columns:\n",
    "        print(\"Error: 'PSID' column not found in GHA files.\")\n",
    "        input(\"Press Enter to exit...\")\n",
    "        sys.exit()\n",
    "\n",
    "    combined_psids = combined_gha_df['PSID'].astype(str).unique()\n",
    "\n",
    "    # --- Step 2: Load Main Input File ---\n",
    "    print(\"Select the MAIN INPUT file (from last phase)\")\n",
    "    main_file = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
    "    \n",
    "    main_df = pd.read_excel(main_file)\n",
    "\n",
    "    if 'PSID' not in main_df.columns:\n",
    "        print(f\"'PSID' column not found in main input file: {main_file}\")\n",
    "        input(\"Press Enter to exit...\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Filter only rows where PSID is present in final GHA\n",
    "    main_df['PSID'] = main_df['PSID'].astype(str)\n",
    "    filtered_main_df = main_df[main_df['PSID'].isin(combined_psids)].copy()\n",
    "\n",
    "    print(f\"Filtered rows: {len(main_df) - len(filtered_main_df)} rows removed.\")\n",
    "    \n",
    "    # OPTIONAL: Save filtered result to a temp file for next steps\n",
    "    output_path = filedialog.asksaveasfilename(defaultextension=\".xlsx\", title=\"Save Filtered Main Input File\")\n",
    "    filtered_main_df.to_excel(output_path, index=False)\n",
    "    print(f\"Filtered file saved at: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "    input(\"Press Enter to exit...\")\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55331bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "main input file for this phase is somewhat simplar to what we produced in last phase... as few new columns or some information given in that file will be changes my multiple ppl... after that we need to process that file. ok so thats the input file. \n",
    "delete all the psid lines which are not in the latest gha file for the gha file we have to combine the two ghas that will be selected by the user in the beginning of the file .\n",
    "\n",
    "\n",
    "There are 5 input files one is the main input file which we created in the last phase and the second file is the combination of 2 GHA files that the user will select in the starting of the program and then we have to combine both of them one after another and then it will become a final GHA file that we will refer to. Third is the cognos report, that again user will select in the beginning of the code.  Fourth is the last month billing file. Fifth is the ospd file , which will be in .xlsb format(load only CC-List sheet and columns from that:  CC_ID, LE_Description.\n",
    "1. delete all those rows from main input file where psid is not present in final gha file.\n",
    "2. Refresh all columns data from column e to column Z in main input file basis the PSID from the final gha file.\n",
    "3. in the main input file , update AID column values, fetch AVP, Manager Operations from cognos report based on Cost center of main input file with Cost Code column in cognos report.\n",
    "4. Create a copy of the  column â€˜Billed/Not Billedâ€™ And  name this new column In main input file as â€˜Category for Staticâ€™. We have to update the values of this column as follows:\n",
    "\ta). if column Employee Class is either â€˜Internâ€™ or â€˜Externalâ€™ and Billed/Not Billed is â€˜Not billedâ€™ then â€˜Category for Staticâ€™  should be set to â€˜Othersâ€™\n",
    "\tb).if column â€˜GCB Gradeâ€™ is â€˜3â€™ or â€˜MDâ€™ and column â€˜Billed/Not Billedâ€™ is â€˜Not billedâ€™ then â€˜Category for Staticâ€™  should be set to â€˜Othersâ€™.\n",
    "c). if column â€˜GFOC Countryâ€™= â€˜Electronic Data Process Mexicoâ€™ and column â€˜Billing Entityâ€™ =â€™GR_MX_Electronic Data Process Mexico, S.A de C.V.â€™  and columm â€˜Billed/Not Billedâ€™=â€™Billedâ€™ then â€˜Category for Staticâ€™  should be set to â€˜Not Billedâ€™.  Simialr to this we have 4 more pairs to be checked for true and in all these cases we have to update â€˜Category for Staticâ€™  should be set to â€˜Not Billedâ€™.  \n",
    "5. from main input file column â€˜Business Partner Cost Centerâ€™ column remove â€˜-â€˜ and any preceding zeroes. And then concatenate the resultant col values with â€˜Billing Entityâ€™ col values. Similar operation should be performed in last month billing file also (which will be uploaded by the user in the beginning of the program.  After concatenation do a lookup based onthese columns and find any new concatenated values which is not present in last month billing file. If any records are found then : update the Comment column (create Comment column if does not exist) with the comment : business partner Cost center and billing entity not found in last month file (or any better comment to state this situation). \n",
    "6. add a new column to main input file AS â€˜Billing Entity name as per OSPD. For this based on â€˜Business Partner Cost Centerâ€™ in main input file, fetch LE_Description col values from ospd basis CC_ID column of ospd file. \n",
    "7. now fetch all unique pairs of â€˜Business Partner Cost Centerâ€™ and Billing Entity. Make sure that each â€˜Business Partner Cost Centerâ€™ corresponds to only one Billing Entity value. If and â€˜Business Partner Cost Centerâ€™ has more that one billing entity against it, then add comment â€˜multiple billing entities found.â€™  If already there is a comment against this row, we can add a new column comment2 and then add comment2 to it. (as I need to keep all comments). Only multiple billing enities are legit where cost center starts withâ€™9â€™.  Anyways we still need to comment, but this time addâ€¦ biling cost center begins with digit 9.  Make sure â€˜Business Partner Cost Centerâ€™  is of simple text type at the end. \n",
    "8. â€˜Billing Contact PSIDâ€™ column in main input file should be 0f 8 digts, prefix 0 if needed (excluding entries where its IPAC. \n",
    "9. Eachâ€™ Billing Contact name should have exactly one billing contact PSID against it.  Add comments to those where more than one psid is found. Comment as â€œbilling contact has duplicate namesâ€ . Here point to be noted is that there are chances that the values in the column Billing Contact name has email id mentioned in some of the rows and in some rows itâ€™s the actual name, we shall write a code to identify  if a billing contact psid has more than one billin contact name values then try to identify if that email is of same person or not, as there are 90% chances that email will contain alteast one word from the name for example John Scena is the name, and inother row with same billing contact psid the name is Scena.J@abcs.com... Then we should be able to figure out that these \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of expected columns in the DataFrame (this should be the exact column names you expect)\n",
    "expected_columns = ['psid', 'category', ' spoc', 'entity manager ps id', 'col2', 'col3 ']\n",
    "\n",
    "def check_column_names(df):\n",
    "    # Capture columns from the input DataFrame\n",
    "    input_columns = df.columns.tolist()\n",
    "\n",
    "    # Compare expected columns with input columns\n",
    "    missing_columns = [col for col in expected_columns if col not in input_columns]\n",
    "    extra_columns = [col for col in input_columns if col not in expected_columns]\n",
    "\n",
    "    # Raise an error if there are missing or extra columns\n",
    "    if missing_columns or extra_columns:\n",
    "        error_message = \"\"\n",
    "        \n",
    "        if missing_columns:\n",
    "            missing_columns_str = \", \".join(missing_columns)\n",
    "            error_message += f\"The following expected columns are missing or mismatched: {missing_columns_str}\\n\"\n",
    "        \n",
    "        if extra_columns:\n",
    "            extra_columns_str = \", \".join(extra_columns)\n",
    "            error_message += f\"The following columns are extra and not expected: {extra_columns_str}\\n\"\n",
    "        \n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    print(\"All expected columns are present and matched.\")\n",
    "\n",
    "# Simulate loading an input file (replace with actual file reading logic)\n",
    "input_data = {\n",
    "    'PSID': [1, 2, 3],\n",
    "    'Category': ['New', 'Old', 'New'],\n",
    "    ' spoc': [None, 'SPOC1', 'SPOC2'],\n",
    "    'Entity Manager PS ID': ['EM1', 'EM2', 'EM3'],\n",
    "    'col2': ['Value1', 'Value2', 'Value3'],\n",
    "    'col3 ': ['Data1', 'Data2', 'Data3']\n",
    "}\n",
    "df = pd.DataFrame(input_data)\n",
    "\n",
    "# Check column names in the DataFrame\n",
    "try:\n",
    "    check_column_names(df)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_logs_to_excel(workbook_path):\n",
    "    wb = openpyxl.load_workbook(workbook_path)\n",
    "    if 'Logs' in wb.sheetnames:\n",
    "        del wb['Logs']\n",
    "    ws = wb.create_sheet(\"Logs\")\n",
    "\n",
    "    # Sort logs\n",
    "    logs.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Header\n",
    "    ws.append([\"Log Description\", \"Value\"])\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")\n",
    "    bold_font = Font(bold=True)\n",
    "\n",
    "    for cell in ws[1]:\n",
    "        cell.fill = header_fill\n",
    "        cell.font = bold_font\n",
    "\n",
    "    # Data rows\n",
    "    for _, desc, val in logs:\n",
    "        ws.append([desc, val])\n",
    "\n",
    "    # Borders\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "    for row in ws.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.border = border\n",
    "\n",
    "    # Set column width\n",
    "    for col in range(1, 3):\n",
    "        ws.column_dimensions[get_column_letter(col)].width = 25\n",
    "\n",
    "    wb.save(workbook_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# Load the workbook and select the worksheet\n",
    "wb = load_workbook(\"your_output_file.xlsx\")\n",
    "ws = wb[\"Sheet1\"]  # change sheet name if needed\n",
    "\n",
    "# Define styles\n",
    "header_fill = PatternFill(start_color='ADD8E6', end_color='ADD8E6', fill_type='solid')  # Light Blue\n",
    "bold_font = Font(bold=True)\n",
    "thin_border = Border(\n",
    "    left=Side(style='thin'), \n",
    "    right=Side(style='thin'), \n",
    "    top=Side(style='thin'), \n",
    "    bottom=Side(style='thin')\n",
    ")\n",
    "\n",
    "# Get max rows and columns\n",
    "max_row = ws.max_row\n",
    "max_col = ws.max_column\n",
    "\n",
    "# Apply styles\n",
    "for row in ws.iter_rows(min_row=1, max_row=max_row, max_col=max_col):\n",
    "    for cell in row:\n",
    "        # Borders for all cells\n",
    "        cell.border = thin_border\n",
    "        # Header styling\n",
    "        if cell.row == 1:\n",
    "            cell.fill = header_fill\n",
    "            cell.font = bold_font\n",
    "\n",
    "# Set column widths and apply autofilter\n",
    "for col in range(1, max_col + 1):\n",
    "    col_letter = get_column_letter(col)\n",
    "    ws.column_dimensions[col_letter].width = 25\n",
    "\n",
    "# Apply autofilter\n",
    "ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"your_output_file.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3e8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# Load the workbook and select the sheet\n",
    "file_path = \"data.xlsx\"a\n",
    "\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb[\"Sheet3\"]\n",
    "\n",
    "# Define the dropdown options\n",
    "dropdown_options = ['Option1', 'Option2', 'Option3', 'Option4', 'Option5', 'Option6']\n",
    "dropdown_formula = '\"' + ','.join(dropdown_options) + '\"'\n",
    "\n",
    "# Create data validation object\n",
    "dv = DataValidation(type=\"list\", formula1=dropdown_formula, showDropDown=False, showErrorMessage=True)\n",
    "dv.error = \"Please select a valid option from the list.\"\n",
    "dv.errorTitle = \"Invalid Selection\"\n",
    "\n",
    "# Column letter for \"Reason for non billable\" (D)\n",
    "col_letter = \"D\"\n",
    "\n",
    "# Apply dropdown while keeping existing values\n",
    "for row in range(2, ws.max_row + 1):  # Assuming row 1 is the header\n",
    "    cell = ws[f\"{col_letter}{row}\"]\n",
    "    \n",
    "    # If the cell is empty, set \"Select\"\n",
    "    if cell.value is None or str(cell.value).strip() == \"\":\n",
    "        cell.value = \"Select\"\n",
    "    \n",
    "    # Apply data validation to enforce selection from the list\n",
    "    dv.add(cell)\n",
    "\n",
    "# Add validation to the sheet\n",
    "ws.add_data_validation(dv)\n",
    "\n",
    "# Save the file\n",
    "wb.save(file_path)\n",
    "wb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"your_file.xlsx\"  # Update with your actual file path\n",
    "\n",
    "# Read the required columns from respective sheets\n",
    "xls = pd.ExcelFile(file_path)\n",
    "rtn_values = xls.parse(\"RTN\")[\"RTN\"].dropna().unique()\n",
    "country_values = xls.parse(\"Location\")[\"Country\"].dropna().unique()\n",
    "gcb_values = xls.parse(\"GCB\")[\"GCB\"].dropna().unique()\n",
    "\n",
    "# Generate all possible combinations\n",
    "combinations = list(product(rtn_values, country_values, gcb_values))\n",
    "\n",
    "# Create a DataFrame\n",
    "output_df = pd.DataFrame(combinations, columns=[\"RTN\", \"Country\", \"GCB\"])\n",
    "\n",
    "# Load the existing workbook\n",
    "wb = load_workbook(file_path)\n",
    "\n",
    "# Remove the existing \"Output\" sheet if it exists\n",
    "if \"Output\" in wb.sheetnames:\n",
    "    wb.remove(wb[\"Output\"])\n",
    "\n",
    "# Create a new sheet\n",
    "ws = wb.create_sheet(\"Output\")\n",
    "\n",
    "# Write column headers\n",
    "ws.append([\"RTN\", \"Country\", \"GCB\"])\n",
    "\n",
    "# Write data rows\n",
    "for row in output_df.itertuples(index=False, name=None):\n",
    "    ws.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Output sheet created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed79abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Big Grid Stack AOP YEAR  \\\n",
      "0                              Inter Boundary Changes     FY25   \n",
      "1   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "2     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "3     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "4     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "5                              Inter Boundary Changes     FY25   \n",
      "6                              Inter Boundary Changes     FY25   \n",
      "7                              Inter Boundary Changes     FY25   \n",
      "8                              Inter Boundary Changes     FY25   \n",
      "9                              Inter Boundary Changes     FY25   \n",
      "10                             Inter Boundary Changes     FY25   \n",
      "11                             Inter Boundary Changes     FY25   \n",
      "12                             Inter Boundary Changes     FY25   \n",
      "13                             Inter Boundary Changes     FY25   \n",
      "14                             Inter Boundary Changes     FY25   \n",
      "\n",
      "   Business Framework RTN Code Business Framework Business Framework Group  \\\n",
      "0                          RTN                                               \n",
      "1                          RTN                                               \n",
      "2                          RTN                                               \n",
      "3                          RTN                                               \n",
      "4                          RTN                                               \n",
      "5                          RTN                                               \n",
      "6                          RTN                                               \n",
      "7                          RTN                                               \n",
      "8                          RTN                                               \n",
      "9                          RTN                                               \n",
      "10                         RTN                                               \n",
      "11                         RTN                                               \n",
      "12                         RTN                                               \n",
      "13                         RTN                                               \n",
      "14                         RTN                                               \n",
      "\n",
      "     Country GCB Emp. Type Hiring Source FTE Start Month WPB      GBM CMB  \\\n",
      "0   Country1   4       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "1   Country2   4       FTE      External  -1  02/28/2025  0%  100.00%  0%   \n",
      "2   Country3   7       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "3   Country3  na       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "4   Country3  na       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "5   Country6  GM       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "6   Country6   3       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "7   Country6   3       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "8   Country6   6       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "9   Country6   6       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "10  Country6   6       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "11  Country6   7       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "12  Country6   7       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "13  Country6   7       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "14  Country6   7       FTE      External   1  01/31/2025  0%  100.00%  0%   \n",
      "\n",
      "   Diver   Entity Description  Position ID  \n",
      "0         Entity1   Example 1       1001.0  \n",
      "1         Entity2   Example 2       1002.0  \n",
      "2         Entity3   Example 3       1003.0  \n",
      "3         Entity3   Example 3       1003.0  \n",
      "4         Entity3   Example 3       1003.0  \n",
      "5         Entity6   Example 6       1004.0  \n",
      "6         Entity6   Example 6       1004.0  \n",
      "7         Entity6   Example 6       1004.0  \n",
      "8         Entity6   Example 6       1004.0  \n",
      "9         Entity6   Example 6       1004.0  \n",
      "10        Entity6   Example 6       1004.0  \n",
      "11        Entity6   Example 6       1004.0  \n",
      "12        Entity6   Example 6       1004.0  \n",
      "13        Entity6   Example 6       1004.0  \n",
      "14        Entity6   Example 6       1004.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Example source data with optional 'PID' column\n",
    "data = {\n",
    "    \"FTE\": [1, -1, 3, 0, None, 10],  # Note: Third row has invalid FTE for PID logic\n",
    "    \"Type\": [\"BC-TT\", \"Investments\", \"Saves\", \"Other\", \"Saves\", \"BC-TT\"],\n",
    "    \"GM\": [0, 0, 0, 0, 0, 1],\n",
    "    \"MD\": [0, 0, 0, 0, 0, 0],\n",
    "    \"3\": [0, 0, 0, 0, 0, 2],\n",
    "    \"4\": [2.5, -4, 0, 0, 0, 0],\n",
    "    \"5\": [0, 0, 0, 0, 0, 0],\n",
    "    \"6\": [3, -6, 0, 0, 0, 3],\n",
    "    \"7\": [1, -1, 1, 0, 0, 4],\n",
    "    \"8\": [0, -1.3, 0, 0, 0, 0],\n",
    "    \"Entity\": [\"Entity1\", \"Entity2\", \"Entity3\", \"Entity4\", \"Entity5\", \"Entity6\"],\n",
    "    \"Description\": [\"Example 1\", \"Example 2\", \"Example 3\", \"Example 4\", \"Example 5\", \"Example 6\"],\n",
    "    \"Country\": [\"Country1\", \"Country2\", \"Country3\", \"Country4\", \"Country5\", \"Country6\"],\n",
    "    \"RTN\": [\"RTN 123 - Example\", \"RTN 456 - Sample\", \"RTN 789 - Test\", \"RTN 101 - Case\", \"RTN 112 - Trial\", \"RTN 113 - Check\"],\n",
    "    \"PID\": [1001, 1002, 1003, None, None, 1004]  # PID is optional\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "\n",
    "# Columns to check for GCB values\n",
    "gcb_columns = [\"GM\", \"MD\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# Clean and convert FTE column\n",
    "source_df[\"FTE\"] = pd.to_numeric(source_df[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "# Check if 'PID' exists in source_df\n",
    "if \"PID\" in source_df.columns:\n",
    "    # Identify rows where PID exists but FTE is not 1 or -1\n",
    "    invalid_pid_fte = source_df[(source_df[\"PID\"].notnull()) & (~source_df[\"FTE\"].isin([1, -1]))]\n",
    "\n",
    "    if not invalid_pid_fte.empty:\n",
    "        # Show a message box to notify the user about the issue\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the main window\n",
    "        messagebox.showerror(\n",
    "            \"FTE Mismatch\",\n",
    "            \"There is a mismatch between PID and FTE values. \\nFTE should be exactly 1 or -1 when PID is present.\"\n",
    "        )\n",
    "        root.destroy()\n",
    "    else:\n",
    "        # Only keep valid rows\n",
    "        source_df = source_df[(source_df[\"PID\"].isnull()) | (source_df[\"FTE\"].isin([1, -1]))]\n",
    "\n",
    "# Filter rows where FTE is valid and does not equal 0\n",
    "filtered_df = source_df[\n",
    "    source_df[\"FTE\"].notnull() & (source_df[\"FTE\"] != 0) & (source_df[\"Type\"].isin([\"BC-TT\", \"Investments\", \"Saves\"]))\n",
    "]\n",
    "\n",
    "# Create the template dataframe with required columns\n",
    "template_columns = [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\", \"Position ID\"\n",
    "] if \"PID\" in source_df.columns else [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\"\n",
    "]\n",
    "\n",
    "template_df = pd.DataFrame(columns=template_columns)\n",
    "\n",
    "# Process each row in the filtered dataframe\n",
    "for _, row in filtered_df.iterrows():\n",
    "    fte_value = row[\"FTE\"]\n",
    "    duplicate_fte = -1 if fte_value < 0 else 1  # Determine positive or negative FTE\n",
    "    abs_fte = abs(fte_value)\n",
    "    int_part = math.floor(abs_fte)  # Integer part of FTE\n",
    "    fractional_part = abs_fte - int_part  # Fractional part of FTE\n",
    "\n",
    "    # Calculate GCB distribution\n",
    "    gcb_distribution = []\n",
    "    for col in gcb_columns:\n",
    "        value = pd.to_numeric(row[col], errors=\"coerce\")  # Convert to numeric, handle non-numeric gracefully\n",
    "        if not pd.isna(value) and value != 0:\n",
    "            gcb_distribution.extend([col] * abs(math.ceil(value)))\n",
    "\n",
    "    # Ensure GCB distribution matches the FTE rows\n",
    "    total_required_rows = int_part + (1 if fractional_part > 0 else 0)\n",
    "    if len(gcb_distribution) < total_required_rows:\n",
    "        gcb_distribution.extend([\"na\"] * (total_required_rows - len(gcb_distribution)))\n",
    "\n",
    "    # Dynamically calculate Start Month\n",
    "    current_year = 2025\n",
    "    if fte_value > 0:\n",
    "        start_month = datetime.date(current_year, 1, 31).strftime(\"%m/%d/%Y\")  # Example logic for positive FTE\n",
    "    elif fte_value < 0:\n",
    "        start_month = datetime.date(current_year, 2, 28).strftime(\"%m/%d/%Y\")  # Example logic for negative FTE\n",
    "    else:\n",
    "        start_month = datetime.date(current_year, 12, 31).strftime(\"%m/%d/%Y\")  # Default fallback\n",
    "\n",
    "    # Create rows in the template\n",
    "    for i in range(total_required_rows):\n",
    "        current_fte = (\n",
    "            fractional_part if i == total_required_rows - 1 and fractional_part > 0 else 1\n",
    "        ) * duplicate_fte\n",
    "\n",
    "        big_grid_stack = (\n",
    "            \"New Perm Position (within FRP) - Staff Drawdown\" if current_fte > 0\n",
    "            else \"Forecast program/Other Saves-Saves Forecast Tracker\"\n",
    "        )\n",
    "        if row[\"Type\"] == \"BC-TT\":\n",
    "            big_grid_stack = \"Inter Boundary Changes\"\n",
    "\n",
    "        new_row = {\n",
    "            \"Big Grid Stack\": big_grid_stack,\n",
    "            \"AOP YEAR\": \"FY25\",\n",
    "            \"Business Framework RTN Code\": row[\"RTN\"].split()[0],\n",
    "            \"Business Framework\": \"\",\n",
    "            \"Business Framework Group\": \"\",\n",
    "            \"Country\": row[\"Country\"],\n",
    "            \"GCB\": gcb_distribution[i],\n",
    "            \"Emp. Type\": \"FTE\",\n",
    "            \"Hiring Source\": \"External\",\n",
    "            \"FTE\": current_fte,\n",
    "            \"Start Month\": start_month,\n",
    "            \"WPB\": \"0%\",\n",
    "            \"GBM\": \"100.00%\",\n",
    "            \"CMB\": \"0%\",\n",
    "            \"Diver\": \"\",\n",
    "            \"Entity\": row[\"Entity\"],\n",
    "            \"Description\": row[\"Description\"],\n",
    "        }\n",
    "\n",
    "        # Add 'Position ID' if 'PID' exists\n",
    "        if \"PID\" in source_df.columns:\n",
    "            new_row[\"Position ID\"] = row[\"PID\"] if pd.notna(row[\"PID\"]) else \"\"\n",
    "\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the result\n",
    "print(template_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "base_df = pd.DataFrame({\n",
    "    'Position ID': [101, 102, 103, 104, 101],\n",
    "    'Category': ['Existing', 'New', 'Existing', 'Existing', 'Existing'],\n",
    "    'Col_A': [10, 20, 30, 40, 50], \n",
    "    'Col_B': ['A', 'B', 'C', 'D', 'E']\n",
    "})\n",
    "\n",
    "gha_df = pd.DataFrame({\n",
    "    'Position ID': [101, 103, 104],\n",
    "    'New_Col_A': [100, 300, 400], \n",
    "    'New_Col_B': ['X', 'Y', 'Z']\n",
    "})\n",
    "\n",
    "# Define columns to update (Map GHA column names to Base column names)\n",
    "update_cols = {'New_Col_A': 'Col_A', 'New_Col_B': 'Col_B'}\n",
    "gha_df = gha_df.rename(columns=update_cols)\n",
    "\n",
    "# Filter only 'Existing' records\n",
    "existing_mask = base_df['Category'] == 'Existing'\n",
    "existing_df = base_df[existing_mask]\n",
    "\n",
    "# Merge with GHA data on 'Position ID'\n",
    "updated_data = existing_df.merge(gha_df, on='Position ID', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Ensure duplicates have the same updated value\n",
    "for old_col in update_cols.values():\n",
    "    new_col = old_col + '_new'\n",
    "    updated_data[old_col] = updated_data.groupby('Position ID')[new_col].transform('first')\n",
    "\n",
    "# Drop extra columns\n",
    "updated_data = updated_data.drop(columns=[col + '_new' for col in update_cols.values()])\n",
    "\n",
    "# Replace updated rows in base_df\n",
    "base_df.loc[existing_mask, update_cols.values()] = updated_data[update_cols.values()]\n",
    "\n",
    "print(base_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import win32com.client as win32\n",
    "\n",
    "# File Paths\n",
    "input_file = r\"C:\\path\\to\\Reportnames.xlsx\"  # Change this path\n",
    "template_file = r\"C:\\path\\to\\Template.xlsx\"  # Change this path\n",
    "output_folder = r\"C:\\path\\to\\Output\"  # Change this path\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Read Reportnames.xlsx using pandas\n",
    "df = pd.read_excel(input_file, usecols=[\"RTN CODE CHILD\", \"Report Name\"])\n",
    "\n",
    "# Launch Excel\n",
    "excel = win32.Dispatch(\"Excel.Application\")\n",
    "excel.Visible = False  # Set True for debugging\n",
    "excel.DisplayAlerts = False  # Prevent popups\n",
    "\n",
    "try:\n",
    "    for index, row in df.iterrows():\n",
    "        rtn_code = row[\"RTN CODE CHILD\"]\n",
    "        report_name = row[\"Report Name\"]\n",
    "        output_file = os.path.join(output_folder, f\"{report_name}.xlsx\")\n",
    "\n",
    "        if os.path.exists(output_file):  # Skip if file already exists\n",
    "            print(f\"Skipping {report_name}.xlsx (already exists)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {report_name}\")\n",
    "\n",
    "        # Open the template file\n",
    "        wb = excel.Workbooks.Open(template_file)\n",
    "        ws = wb.Sheets(\"Setup\")\n",
    "\n",
    "        # Insert values in Setup sheet\n",
    "        ws.Range(\"D3\").Value = rtn_code\n",
    "        ws.Range(\"D4\").Value = report_name\n",
    "\n",
    "        # Refresh workbook (with timeout handling)\n",
    "        start_time = time.time()\n",
    "        excel.CalculateFull()  # Refresh all data\n",
    "        while time.time() - start_time < 30:  # Timeout of 30 sec\n",
    "            time.sleep(1)  \n",
    "\n",
    "        # Remove formulas by copying and pasting as values\n",
    "        for sheet in wb.Sheets:\n",
    "            sheet.Cells.Copy()\n",
    "            sheet.Cells.PasteSpecial(Paste=win32.constants.xlPasteValues)\n",
    "\n",
    "        excel.CutCopyMode = False  # Clear clipboard to prevent errors\n",
    "\n",
    "        # Save final file\n",
    "        wb.SaveAs(output_file)\n",
    "        wb.Close(SaveChanges=False)\n",
    "\n",
    "        print(f\"âœ… Saved: {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    excel.Quit()\n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input/output paths\n",
    "input_folder = \"Input\"\n",
    "output_file = \"JML_Report.xlsx\"\n",
    "input_file = os.path.join(input_folder, \"your_excel_file.xlsx\")  # Update your filename\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file, sheet_name=\"Headcount Employee Detail\", dtype=str)\n",
    "\n",
    "# Convert required columns to string for lookups\n",
    "df[\"Employee ID\"] = df[\"Employee ID\"].astype(str)\n",
    "df[\"Functional Manager Employee ID\"] = df[\"Functional Manager Employee ID\"].astype(str)\n",
    "df[\"BF Level 2 Name\"] = df[\"BF Level 2 Name\"].astype(str)\n",
    "\n",
    "# Create a lookup dictionary for quick access\n",
    "lookup_dict = df.set_index(\"Employee ID\")[[\"Functional Manager Employee ID\", \"BF Level 2 Name\"]].to_dict(\"index\")\n",
    "\n",
    "# Step 1: Filter for 'Central Managed Services' employees\n",
    "df_cms = df[df[\"BF Level 2 Name\"] == \"Central Managed Services\"].copy()\n",
    "\n",
    "# Initialize columns for tracking hierarchy\n",
    "for i in range(1, 6):  # Up to 5 levels\n",
    "    df_cms[f\"Manager{i}\"] = \"\"\n",
    "    df_cms[f\"Check{i}\"] = \"\"\n",
    "\n",
    "# Function to recursively fetch Functional Manager and BF Level\n",
    "def track_manager_hierarchy(emp_id, max_level=5):\n",
    "    managers = []\n",
    "    bf_levels = []\n",
    "    \n",
    "    for i in range(1, max_level + 1):\n",
    "        manager_id = lookup_dict.get(emp_id, {}).get(\"Functional Manager Employee ID\")\n",
    "        bf_level = lookup_dict.get(emp_id, {}).get(\"BF Level 2 Name\")\n",
    "\n",
    "        if not manager_id or pd.isna(manager_id):\n",
    "            break  # Stop if no manager exists\n",
    "        \n",
    "        managers.append(manager_id)\n",
    "        bf_levels.append(bf_level)\n",
    "\n",
    "        # Move to the next level (next manager)\n",
    "        emp_id = manager_id  \n",
    "\n",
    "    return managers, bf_levels\n",
    "\n",
    "# Step 2: Iterate over each employee and fetch hierarchy details\n",
    "for index, row in df_cms.iterrows():\n",
    "    emp_id = row[\"Employee ID\"]\n",
    "    managers, bf_levels = track_manager_hierarchy(emp_id, max_level=5)\n",
    "\n",
    "    for i, (manager, bf_level) in enumerate(zip(managers, bf_levels), start=1):\n",
    "        df_cms.at[index, f\"Manager{i}\"] = manager\n",
    "        df_cms.at[index, f\"Check{i}\"] = bf_level\n",
    "\n",
    "# Step 3: Identify rows where any \"Check\" column has 'Finance'\n",
    "check_cols = [f\"Check{i}\" for i in range(1, 6)]\n",
    "df_cms[\"Updated BF Level2\"] = df_cms[check_cols].apply(lambda x: \"CMS Finance\" if \"Finance\" in x.values else \"\", axis=1)\n",
    "\n",
    "# Step 4: Save full df_cms (with all original + Check/Manager columns) in \"CMS_Details\"\n",
    "with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "    df_cms.to_excel(writer, sheet_name=\"CMS_Details\", index=False)  # Save full CMS details for reference\n",
    "\n",
    "    # Step 5: Filter rows where 'Updated BF Level2' is 'CMS Finance'\n",
    "    cms_df = df_cms[df_cms[\"Updated BF Level2\"] == \"CMS Finance\"].copy()\n",
    "\n",
    "    # Step 6: Read original sheet again for Finance rows\n",
    "    df_finance = df[df[\"BF Level 2 Name\"] == \"Finance\"].copy()\n",
    "    df_finance[\"Updated BF Level2\"] = \"Finance\"\n",
    "\n",
    "    # Step 7: Remove extra columns before appending to `df_finance`\n",
    "    cms_df_cleaned = cms_df[df_finance.columns]\n",
    "\n",
    "    # Step 8: Reset index types for proper concatenation\n",
    "    cms_df_cleaned = cms_df_cleaned.reset_index(drop=True)\n",
    "    df_finance = df_finance.reset_index(drop=True)\n",
    "\n",
    "    # Step 9: Concatenate cleaned `cms_df` with `df_finance`\n",
    "    final_df = pd.concat([df_finance, cms_df_cleaned], ignore_index=True)\n",
    "\n",
    "    # Step 10: Save the final `JML Report`\n",
    "    final_df.to_excel(writer, sheet_name=\"JML Report\", index=False)\n",
    "\n",
    "print(\"âœ… Process completed successfully! The final file is saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Files\n",
    "output_file = pd.read_excel('Output.xlsx', sheet_name='Sheet1')\n",
    "input_file = pd.read_excel('Input.xlsx', sheet_name='Sheet1')\n",
    "mapping_file = pd.ExcelFile('Mapping.xlsx')\n",
    "ospd_file = pd.read_excel('OSPD.xlsb', sheet_name='CC-List')\n",
    "\n",
    "# Load Mapping Sheets\n",
    "gr_mapping = mapping_file.parse('GR')\n",
    "anaplan_mapping = mapping_file.parse('Anaplan')\n",
    "country_mapping = mapping_file.parse('Country Mapping')\n",
    "\n",
    "# Step 2: Add 'Classification' to Output File\n",
    "output_file = output_file.merge(gr_mapping[['Role Type', 'Classification']], on='Role Type', how='left')\n",
    "\n",
    "# Step 3: Add 'Classification' to Input File\n",
    "input_file = input_file.merge(anaplan_mapping[['Position class', 'Classification']], left_on='Role Type', right_on='Position class', how='left')\n",
    "\n",
    "# Step 4: Add 'Country' to Input File based on 'Country R3'\n",
    "input_file = input_file.merge(country_mapping[['Country R3', 'Country']], on='Country R3', how='left')\n",
    "\n",
    "# Step 5: Add 'RTN Code' from OSPD file\n",
    "input_file = input_file.merge(ospd_file[['L4_BF_Description', 'L4_BF_ID']], left_on='Business Framework', right_on='L4_BF_Description', how='left')\n",
    "input_file.rename(columns={'L4_BF_ID': 'RTN Code'}, inplace=True)\n",
    "\n",
    "# Step 6: Create Unique Key for Input File\n",
    "input_file['Unique Key'] = input_file['Classification'] + ' ' + input_file['RTN Code'] + ' ' + input_file['Country']\n",
    "\n",
    "# Step 7: Create Unique Key for Output File\n",
    "output_file['Unique Key'] = output_file['Classification'] + ' ' + output_file['RTN Code L4'] + ' ' + output_file['Country']\n",
    "\n",
    "# Step 8: Create Secondary Keys\n",
    "input_file['Secondary Key'] = input_file['RTN Code'] + ' ' + input_file['Country']\n",
    "output_file['Secondary Key'] = output_file['RTN Code L4'] + ' ' + output_file['Country']\n",
    "\n",
    "# Step 9: Lookup Data from Input to Output File\n",
    "lookup_cols = ['Business Framework', 'Country', 'Classification']\n",
    "out_cols = ['A_L4', 'A_Country', 'A_Role Type']\n",
    "output_file[out_cols] = output_file[['Unique Key']].merge(input_file[['Unique Key'] + lookup_cols], on='Unique Key', how='left')[lookup_cols]\n",
    "\n",
    "# Step 10: Aggregate Cost & FTE based on Unique and Secondary Keys\n",
    "agg_data = input_file.groupby('Unique Key').agg({'Cost': 'sum', 'FTE': 'sum'}).reset_index()\n",
    "output_file = output_file.merge(agg_data, on='Unique Key', how='left').rename(columns={'Cost': 'A_DC', 'FTE': 'A_FTE'})\n",
    "\n",
    "# Aggregate for Secondary Key\n",
    "agg_data_sec = input_file.groupby('Secondary Key').agg({'Cost': 'sum', 'FTE': 'sum'}).reset_index()\n",
    "output_file = output_file.merge(agg_data_sec, on='Secondary Key', how='left').rename(columns={'Cost': 'Total_DC', 'FTE': 'Total_FTE'})\n",
    "\n",
    "# Save the final processed Output File\n",
    "output_file.to_excel('Processed_Output.xlsx', index=False)\n",
    "\n",
    "print(\"Processing completed. Output saved as 'Processed_Output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input and output file paths\n",
    "input_folder = \"Input\"\n",
    "input_file = os.path.join(input_folder, \"your_excel_file.xlsx\")  # Replace with actual filename\n",
    "output_file = \"JML_Report.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(input_file, sheet_name=\"Headcount Employee Detail\", dtype=str)  # Read everything as text\n",
    "\n",
    "# Filter only \"Central Managed Services\"\n",
    "df_cms = df[df[\"BF Level 2 Name\"] == \"Central Managed Services\"].copy()\n",
    "\n",
    "# Initialize columns for tracking manager levels\n",
    "manager_cols = [\"Check\", \"Manager\", \"Check2\", \"Manager2\", \"Check3\", \"Manager3\", \"Check4\", \"Manager4\", \"Check5\", \"Manager5\", \"Check6\"]\n",
    "\n",
    "# Add empty columns\n",
    "for col in manager_cols:\n",
    "    df_cms[col] = \"\"\n",
    "\n",
    "# Create a dictionary for fast lookups (Employee ID â†’ [Manager ID, BF Level])\n",
    "lookup_dict = df.set_index(\"Employee ID\")[[\"Functional Manager Employee ID\", \"BF Level 2 Name\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Function to track hierarchy up to 5 levels\n",
    "def track_manager_hierarchy(emp_id):\n",
    "    manager_data = {\"Check\": \"\", \"Manager\": \"\", \"Check2\": \"\", \"Manager2\": \"\", \"Check3\": \"\", \"Manager3\": \"\", \"Check4\": \"\", \"Manager4\": \"\", \"Check5\": \"\", \"Manager5\": \"\", \"Check6\": \"\"}\n",
    "    current_emp = emp_id\n",
    "\n",
    "    for i in range(1, 7):  # Loop for levels 1 to 6\n",
    "        if current_emp in lookup_dict:\n",
    "            manager_id = lookup_dict[current_emp][\"Functional Manager Employee ID\"]\n",
    "            bf_level = lookup_dict[current_emp][\"BF Level 2 Name\"]\n",
    "\n",
    "            # Save in corresponding columns\n",
    "            if i == 1:\n",
    "                manager_data[\"Check\"] = bf_level\n",
    "                manager_data[\"Manager\"] = manager_id\n",
    "            else:\n",
    "                manager_data[f\"Check{i}\"] = bf_level\n",
    "                manager_data[f\"Manager{i}\"] = manager_id\n",
    "            \n",
    "            # Move up the hierarchy\n",
    "            current_emp = manager_id\n",
    "        else:\n",
    "            break  # Stop if no manager found\n",
    "\n",
    "    return pd.Series(manager_data)\n",
    "\n",
    "# Apply the function to populate manager levels\n",
    "df_cms[manager_cols] = df_cms[\"Employee ID\"].apply(track_manager_hierarchy)\n",
    "\n",
    "# Identify rows where any \"Check\" column has 'Finance'\n",
    "df_cms[\"Updated BF Level2\"] = df_cms[[\"Check\", \"Check2\", \"Check3\", \"Check4\", \"Check5\", \"Check6\"]].apply(lambda x: \"CMS Finance\" if \"Finance\" in x.values else \"\", axis=1)\n",
    "\n",
    "# Keep only relevant rows\n",
    "cms_df = df_cms[df_cms[\"Updated BF Level2\"] == \"CMS Finance\"]\n",
    "\n",
    "# Read Finance rows from original file\n",
    "df_finance = df[df[\"BF Level 2 Name\"] == \"Finance\"].copy()\n",
    "df_finance[\"Updated BF Level2\"] = \"Finance\"\n",
    "\n",
    "# Append cms_df rows to finance rows\n",
    "final_df = pd.concat([df_finance, cms_df], ignore_index=True)\n",
    "\n",
    "# Save to a new Excel file with sheet name \"JML Report\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    final_df.to_excel(writer, sheet_name=\"JML Report\", index=False)\n",
    "\n",
    "print(\"âœ… JML Report generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the yellow fill for highlighting\n",
    "yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
    "\n",
    "# Load your Excel file\n",
    "file_path = \"your_file.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Ensure all expected columns are present\n",
    "required_columns = ['Original Date', 'Date', 'Type', 'Description']  # Add more columns as needed\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"  # Add missing columns with blank values\n",
    "\n",
    "# Function to safely convert date values\n",
    "def safe_convert_date(date_value):\n",
    "    if pd.isnull(date_value):  # Handle missing values\n",
    "        return None\n",
    "    if isinstance(date_value, datetime):  # Already a datetime object\n",
    "        return date_value\n",
    "    if isinstance(date_value, str):  # Try parsing strings\n",
    "        for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\"):  # Common date formats\n",
    "            try:\n",
    "                return datetime.strptime(date_value, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None  # Return None if conversion fails\n",
    "\n",
    "# Ensure Date and Original Date are in proper datetime format\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = df['Date'].apply(safe_convert_date)\n",
    "if 'Original Date' in df.columns:\n",
    "    df['Original Date'] = df['Original Date'].apply(safe_convert_date)\n",
    "\n",
    "# Function to process Date changes\n",
    "def process_dates(row, current_date):\n",
    "    original_date = row.get('Original Date')\n",
    "    date = row.get('Date')\n",
    "    \n",
    "    # Check if dates are valid\n",
    "    if pd.isna(date) or (original_date and (original_date.year < current_date.year or original_date.month < current_date.month)):\n",
    "        return current_date.replace(day=1) - pd.Timedelta(days=1)  # Last day of current month\n",
    "    return date\n",
    "\n",
    "# Get today's date\n",
    "today = pd.Timestamp.now()\n",
    "current_month_last_date = today.replace(day=1) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Apply logic to update Date column and highlight changes\n",
    "for index, row in df.iterrows():\n",
    "    updated_date = process_dates(row, current_month_last_date)\n",
    "    if updated_date != row['Date']:  # Track changes\n",
    "        df.at[index, 'Date'] = updated_date\n",
    "        ws.cell(row=index + 2, column=df.columns.get_loc('Date') + 1).value = updated_date  # Update cell value\n",
    "        ws.cell(row=index + 2, column=df.columns.get_loc('Date') + 1).fill = yellow_fill  # Highlight updated cell\n",
    "\n",
    "# Handle 'Type' and 'Description' specific logic only if the columns exist\n",
    "if 'Type' in df.columns:\n",
    "    # Example logic for 'Type'\n",
    "    df['Type'] = df['Type'].apply(lambda x: x.upper() if pd.notna(x) else \"\")\n",
    "if 'Description' in df.columns:\n",
    "    # Example logic for 'Description'\n",
    "    df['Description'] = df['Description'].fillna(\"No description available\")\n",
    "\n",
    "# Save the updated Excel file\n",
    "wb.save(\"updated_file.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e945c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Example source data\n",
    "data = {\n",
    "    \"FTE\": [6.5, -12.3, 3, 0, None, 10],\n",
    "    \"Type\": [\"BC-TT\", \"Investments\", \"Saves\", \"Other\", \"Saves\", \"BC-TT\"],\n",
    "    \"GM\": [0, 0, 0, 0, 0, 1],\n",
    "    \"MD\": [0, 0, 0, 0, 0, 0],\n",
    "    \"3\": [0, 0, 0, 0, 0, 2],\n",
    "    \"4\": [2.5, -4, 0, 0, 0, 0],\n",
    "    \"5\": [0, 0, 0, 0, 0, 0],\n",
    "    \"6\": [3, -6, 0, 0, 0, 3],\n",
    "    \"7\": [1, -1, 1, 0, 0, 4],\n",
    "    \"8\": [0, -1.3, 0, 0, 0, 0],\n",
    "    \"Entity\": [\"Entity1\", \"Entity2\", \"Entity3\", \"Entity4\", \"Entity5\", \"Entity6\"],\n",
    "    \"Description\": [\"Example 1\", \"Example 2\", \"Example 3\", \"Example 4\", \"Example 5\", \"Example 6\"],\n",
    "    \"Country\": [\"Country1\", \"Country2\", \"Country3\", \"Country4\", \"Country5\", \"Country6\"],\n",
    "    \"RTN\": [\"RTN 123 - Example\", \"RTN 456 - Sample\", \"RTN 789 - Test\", \"RTN 101 - Case\", \"RTN 112 - Trial\", \"RTN 113 - Check\"]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "\n",
    "# Columns to check for GCB values\n",
    "gcb_columns = [\"GM\", \"MD\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# Clean and convert FTE column\n",
    "source_df[\"FTE\"] = pd.to_numeric(source_df[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "# Filter rows where FTE is valid and does not equal 0\n",
    "filtered_df = source_df[\n",
    "    source_df[\"FTE\"].notnull() & (source_df[\"FTE\"] != 0) & (source_df.get(\"Type\", \"\").isin([\"BC-TT\", \"Investments\", \"Saves\"]))\n",
    "]\n",
    "\n",
    "# Create the template dataframe with required columns\n",
    "template_columns = [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\"\n",
    "]\n",
    "template_df = pd.DataFrame(columns=template_columns)\n",
    "\n",
    "# Process each row in the filtered dataframe\n",
    "for _, row in filtered_df.iterrows():\n",
    "    fte_value = row[\"FTE\"]\n",
    "    duplicate_fte = -1 if fte_value < 0 else 1  # Determine positive or negative FTE\n",
    "    abs_fte = abs(fte_value)\n",
    "    int_part = math.floor(abs_fte)  # Integer part of FTE\n",
    "    fractional_part = abs_fte - int_part  # Fractional part of FTE\n",
    "\n",
    "    # Calculate GCB distribution\n",
    "    gcb_distribution = []\n",
    "    for col in gcb_columns:\n",
    "        value = pd.to_numeric(row.get(col), errors=\"coerce\")  # Convert to numeric, handle non-numeric gracefully\n",
    "        if not pd.isna(value) and value != 0:\n",
    "            gcb_distribution.extend([col] * abs(math.ceil(value)))\n",
    "\n",
    "    # Ensure GCB distribution matches the FTE rows\n",
    "    total_required_rows = int_part + (1 if fractional_part > 0 else 0)\n",
    "    if len(gcb_distribution) < total_required_rows:\n",
    "        gcb_distribution.extend([\"na\"] * (total_required_rows - len(gcb_distribution)))\n",
    "\n",
    "    # Dynamically calculate Start Month\n",
    "    current_year = 2025\n",
    "    if fte_value > 0:\n",
    "        start_month = datetime.date(current_year, 1, 31)  # Example logic for positive FTE\n",
    "    elif fte_value < 0:\n",
    "        start_month = datetime.date(current_year, 2, 28)  # Example logic for negative FTE\n",
    "    else:\n",
    "        start_month = datetime.date(current_year, 12, 31)  # Default fallback\n",
    "\n",
    "    # Create rows in the template\n",
    "    for i in range(total_required_rows):\n",
    "        current_fte = (\n",
    "            fractional_part if i == total_required_rows - 1 and fractional_part > 0 else 1\n",
    "        ) * duplicate_fte\n",
    "\n",
    "        big_grid_stack = (\n",
    "            \"New Perm Position (within FRP) - Staff Drawdown\" if current_fte > 0\n",
    "            else \"Forecast program/Other Saves-Saves Forecast Tracker\"\n",
    "        )\n",
    "        if row.get(\"Type\") == \"BC-TT\":\n",
    "            big_grid_stack = \"Inter Boundary Changes\"\n",
    "\n",
    "        new_row = {\n",
    "            \"Big Grid Stack\": big_grid_stack,\n",
    "            \"AOP YEAR\": \"FY25\",\n",
    "            \"Business Framework RTN Code\": row[\"RTN\"].split()[0],\n",
    "            \"Business Framework\": \"\",\n",
    "            \"Business Framework Group\": \"\",\n",
    "            \"Country\": row.get(\"Country\", \"\"),\n",
    "            \"GCB\": gcb_distribution[i],\n",
    "            \"Emp. Type\": \"FTE\",\n",
    "            \"Hiring Source\": \"External\",\n",
    "            \"FTE\": current_fte,\n",
    "            \"Start Month\": start_month.strftime(\"%m/%d/%Y\"),\n",
    "            \"WPB\": \"0%\",\n",
    "            \"GBM\": \"100.00%\",\n",
    "            \"CMB\": \"0%\",\n",
    "            \"Diver\": \"\",\n",
    "            \"Entity\": row.get(\"Entity\", \"\"),\n",
    "            \"Description\": row.get(\"Description\", \"No description available\"),\n",
    "        }\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the result\n",
    "print(template_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "def adjust_and_highlight_dates(input_file, output_file):\n",
    "    # Read the Excel file into a dataframe\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Get today's date, current year, and current month\n",
    "    today = datetime.datetime.today()\n",
    "    current_year = today.year\n",
    "    current_month = today.month\n",
    "\n",
    "    # Convert the Date column to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # Preserve the original dates for comparison\n",
    "    df['Original Date'] = df['Date']\n",
    "\n",
    "    # Iterate through the dataframe and update dates if necessary\n",
    "    for index, row in df.iterrows():\n",
    "        original_date = row['Original Date']\n",
    "        if pd.notna(original_date):  # Ensure the date is valid\n",
    "            # Check if the year is not current or the month is before the current month\n",
    "            if original_date.year != current_year or original_date.month < current_month:\n",
    "                # Get the last day of the current month\n",
    "                last_day_of_month = datetime.date(current_year, current_month, calendar.monthrange(current_year, current_month)[1])\n",
    "                df.at[index, 'Date'] = last_day_of_month  # Update the date\n",
    "\n",
    "    # Save the updated dataframe to the Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    # Open the workbook to apply formatting\n",
    "    wb = load_workbook(output_file)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Apply yellow highlight to cells in the \"Date\" column where changes were made\n",
    "    yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Original Date'] != row['Date']:  # Compare original and updated date\n",
    "            excel_row = index + 2  # Offset for Excel's 1-based index and header row\n",
    "            ws.cell(row=excel_row, column=2).fill = yellow_fill  # Column 2 corresponds to the 'Date' column\n",
    "\n",
    "    # Save the workbook with highlighted changes\n",
    "    wb.save(output_file)\n",
    "    print(\"Dates adjusted and changes highlighted where necessary.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"input_dates.xlsx\"  # Replace with your input file\n",
    "output_file = \"output_dates.xlsx\"  # Replace with your output file\n",
    "adjust_and_highlight_dates(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47208d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl.styles import Alignment, Font, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def format_excel_with_headers(file_path):\n",
    "    # Load the workbook\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    \n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        \n",
    "        # Insert the sheet name as a heading in A1\n",
    "        sheet.insert_rows(1)\n",
    "        sheet['A1'] = sheet_name\n",
    "        sheet.merge_cells(start_row=1, start_column=1, end_row=1, end_column=sheet.max_column)\n",
    "        sheet['A1'].font = Font(size=14, bold=True)\n",
    "        sheet['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        sheet['A1'].fill = PatternFill(start_color=\"B0C4DE\", end_color=\"B0C4DE\", fill_type=\"solid\")  # Light Blue\n",
    "        \n",
    "        # Apply formatting to the header row (now at row 2)\n",
    "        header_row = 2\n",
    "        for col in range(1, sheet.max_column + 1):\n",
    "            cell = sheet.cell(row=header_row, column=col)\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # Light Blue\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        \n",
    "        # Set column widths\n",
    "        for col in range(1, sheet.max_column + 1):\n",
    "            max_length = 0\n",
    "            column = get_column_letter(col)\n",
    "            for row in range(1, sheet.max_row + 1):\n",
    "                cell = sheet[f\"{column}{row}\"]\n",
    "                try:\n",
    "                    if cell.value:\n",
    "                        max_length = max(max_length, len(str(cell.value)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            adjusted_width = max_length + 2\n",
    "            sheet.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    # Save the updated workbook\n",
    "    workbook.save(file_path)\n",
    "    print(f\"Workbook formatted and saved at: {file_path}\")\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"organization_structure.xlsx\"\n",
    "format_excel_with_headers(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91098aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import openpyxl\n",
    "\n",
    "def add_table_slide(presentation, sheet_name, sheet):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    title = slide.shapes.title\n",
    "    title.text = f\"Data for {sheet_name}\"\n",
    "\n",
    "    # Get the data\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(values_only=True)]\n",
    "    rows, cols = len(data), len(data[0])\n",
    "\n",
    "    # Add table\n",
    "    left = Inches(1.0)\n",
    "    top = Inches(1.5)\n",
    "    width = Inches(8.0)\n",
    "    height = Inches(5.0)\n",
    "    table = slide.shapes.add_table(rows, cols, left, top, width, height).table\n",
    "\n",
    "    # Fill the table with data\n",
    "    for i, row in enumerate(data):\n",
    "        for j, value in enumerate(row):\n",
    "            cell = table.cell(i, j)\n",
    "            cell.text = str(value) if value else \"\"\n",
    "            cell.text_frame.paragraphs[0].font.size = Pt(10)\n",
    "\n",
    "    # Format header row\n",
    "    for cell in table.rows[0].cells:\n",
    "        cell.text_frame.paragraphs[0].font.bold = True\n",
    "        cell.fill.solid()\n",
    "        cell.fill.fore_color.rgb = RGBColor(173, 216, 230)  # Light Blue\n",
    "\n",
    "def generate_table_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        add_table_slide(presentation, sheet_name, sheet)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Example Usage\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_table.pptx\"\n",
    "generate_table_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.util import Inches, Pt\n",
    "import openpyxl\n",
    "\n",
    "def add_org_chart_slide(presentation, sheet_name, hierarchy):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    title = slide.shapes.title\n",
    "    title.text = f\"Organizational Chart: {sheet_name}\"\n",
    "\n",
    "    # Add shapes for hierarchy\n",
    "    top_margin = 1.5  # Inches\n",
    "    left_margin = 1.0  # Inches\n",
    "    shape_width = 2.0  # Inches\n",
    "    shape_height = 1.0  # Inches\n",
    "    level_gap = 1.5  # Vertical gap between levels\n",
    "    horizontal_gap = 2.5  # Horizontal gap between shapes\n",
    "\n",
    "    # Recursive function to draw hierarchy\n",
    "    def draw_hierarchy(manager, x, y, slide):\n",
    "        shape = slide.shapes.add_shape(\n",
    "            MSO_SHAPE.RECTANGLE,\n",
    "            Inches(x), Inches(y),\n",
    "            Inches(shape_width), Inches(shape_height)\n",
    "        )\n",
    "        shape.text = manager\n",
    "        shape.text_frame.paragraphs[0].font.size = Pt(12)\n",
    "        return shape\n",
    "\n",
    "    y = top_margin\n",
    "    for manager1, sub_hierarchy in hierarchy.items():\n",
    "        shape1 = draw_hierarchy(manager1, left_margin, y, slide)\n",
    "        x = left_margin\n",
    "        for manager2, employees in sub_hierarchy.items():\n",
    "            y += level_gap\n",
    "            shape2 = draw_hierarchy(manager2, x + horizontal_gap, y, slide)\n",
    "            for i, employee in enumerate(employees):\n",
    "                draw_hierarchy(employee, x + horizontal_gap * (i + 2), y + level_gap, slide)\n",
    "\n",
    "def generate_org_chart_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        hierarchy = extract_hierarchy(sheet)\n",
    "        add_org_chart_slide(presentation, sheet_name, hierarchy)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Example Usage\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_chart.pptx\"\n",
    "generate_org_chart_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from pptx import Presentation\n",
    "from pptx.util import Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "def extract_hierarchy(sheet):\n",
    "    hierarchy = {}\n",
    "    for row in sheet.iter_rows(min_row=3, values_only=True):  # Start reading data from row 3\n",
    "        level1, level2, level3 = row[0:3], row[3:6], row[6:9]\n",
    "        manager1 = \" | \".join([str(x) for x in level1 if x])\n",
    "        manager2 = \" | \".join([str(x) for x in level2 if x])\n",
    "        employee = \" | \".join([str(x) for x in level3 if x])\n",
    "\n",
    "        if manager1 not in hierarchy:\n",
    "            hierarchy[manager1] = {}\n",
    "        if manager2 not in hierarchy[manager1]:\n",
    "            hierarchy[manager1][manager2] = []\n",
    "        hierarchy[manager1][manager2].append(employee)\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def add_slide_with_chart(presentation, sheet_name, hierarchy):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    # Add title\n",
    "    title = slide.shapes.title\n",
    "    title.text = sheet_name\n",
    "    title.text_frame.paragraphs[0].font.bold = True\n",
    "    title.text_frame.paragraphs[0].font.size = Pt(24)\n",
    "\n",
    "    # Add text box for the org chart\n",
    "    left = Pt(50)\n",
    "    top = Pt(100)\n",
    "    width = Pt(800)\n",
    "    height = Pt(500)\n",
    "    text_box = slide.shapes.add_textbox(left, top, width, height)\n",
    "    text_frame = text_box.text_frame\n",
    "    text_frame.word_wrap = True\n",
    "    text_frame.margin_left = Pt(10)\n",
    "    text_frame.margin_top = Pt(10)\n",
    "\n",
    "    # Build the org chart text\n",
    "    text_frame.text = f\"Organizational Chart for {sheet_name}\\n\"\n",
    "    for manager1, sub_hierarchy in hierarchy.items():\n",
    "        p = text_frame.add_paragraph()\n",
    "        p.text = manager1\n",
    "        p.font.bold = True\n",
    "        p.font.size = Pt(16)\n",
    "        p.font.color.rgb = RGBColor(0, 51, 102)  # Dark blue\n",
    "\n",
    "        for manager2, employees in sub_hierarchy.items():\n",
    "            p2 = text_frame.add_paragraph()\n",
    "            p2.text = f\"    {manager2}\"\n",
    "            p2.font.bold = False\n",
    "            p2.font.size = Pt(14)\n",
    "            p2.font.color.rgb = RGBColor(0, 102, 0)  # Green\n",
    "\n",
    "            for employee in employees:\n",
    "                p3 = text_frame.add_paragraph()\n",
    "                p3.text = f\"        {employee}\"\n",
    "                p3.font.size = Pt(12)\n",
    "                p3.font.color.rgb = RGBColor(0, 0, 0)  # Black\n",
    "\n",
    "def generate_organizational_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        hierarchy = extract_hierarchy(sheet)\n",
    "        add_slide_with_chart(presentation, sheet_name, hierarchy)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Input Excel and output PPT file paths\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_presentation.pptx\"\n",
    "\n",
    "# Generate the presentation\n",
    "generate_organizational_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4364122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def format_sheet(sheet):\n",
    "    # Set the sheet name as the title in cell A1\n",
    "    sheet_title = sheet.title\n",
    "    sheet.merge_cells('A1:I1')  # Adjust range based on your data width\n",
    "    sheet['A1'] = sheet_title\n",
    "    sheet['A1'].font = Font(bold=True, size=14)\n",
    "    sheet['A1'].alignment = Alignment(horizontal='center')\n",
    "\n",
    "    # Style the header row\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # Light blue background\n",
    "    header_font = Font(bold=True)\n",
    "    header_alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "    # Assuming row 2 is the header row\n",
    "    for cell in sheet[2]:  # Header row is the second row\n",
    "        cell.fill = header_fill\n",
    "        cell.font = header_font\n",
    "        cell.alignment = header_alignment\n",
    "\n",
    "    # Set fixed column width\n",
    "    fixed_width = 20  # Set desired width\n",
    "    for col in sheet.columns:\n",
    "        col_letter = get_column_letter(col[0].column)  # Get column letter\n",
    "        sheet.column_dimensions[col_letter].width = fixed_width\n",
    "\n",
    "def format_workbook(file_path, output_path):\n",
    "    # Load the workbook\n",
    "    wb = load_workbook(file_path)\n",
    "\n",
    "    # Format each sheet\n",
    "    for sheet in wb.worksheets:\n",
    "        format_sheet(sheet)\n",
    "\n",
    "    # Save the formatted workbook\n",
    "    wb.save(output_path)\n",
    "    print(f\"Formatted file saved as: {output_path}\")\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"organization_structure.xlsx\"\n",
    "output_file = \"formatted_organization_structure.xlsx\"\n",
    "\n",
    "# Call the function to format the workbook\n",
    "format_workbook(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Load the input Excel file\n",
    "input_file = \"your_input_file.xlsx\"  # Replace with your input file path\n",
    "output_file = \"organization_structure.xlsx\"  # Output file path\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Ensure consistent column names (case-insensitive matching)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Unique countries\n",
    "countries = df['Work Location Country/Territory Name'].dropna().unique()\n",
    "\n",
    "# Create a new workbook\n",
    "wb = Workbook()\n",
    "wb.remove(wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find subordinates\n",
    "def find_subordinates(df, manager_name):\n",
    "    return df[df['Entity Manager Employee Name'] == manager_name]\n",
    "\n",
    "# Process each country\n",
    "for country in countries:\n",
    "    # Filter data for the country\n",
    "    country_df = df[df['Work Location Country/Territory Name'] == country]\n",
    "    # Get unique BF Level 4 Names for the country\n",
    "    bf_level_4_names = country_df['BF Level 4 Name'].dropna().unique()\n",
    "\n",
    "    # Create a sheet for this country\n",
    "    for bf_level_4_name in bf_level_4_names:\n",
    "        # Filter data for the BF Level 4 Name\n",
    "        bf_df = country_df[country_df['BF Level 4 Name'] == bf_level_4_name]\n",
    "\n",
    "        # Sort data by Global Career Band (MD > 3 > 4 > ... > 8)\n",
    "        bf_df['Global Career Band'] = bf_df['Global Career Band'].astype(str)  # Ensure values are strings\n",
    "        sorted_bf_df = bf_df.sort_values('Global Career Band', key=lambda col: col.map(lambda x: 'MD' if x == 'MD' else int(x) if x.isdigit() else 999))\n",
    "\n",
    "        # Prepare data for three levels\n",
    "        rows = []\n",
    "        for _, level_1 in sorted_bf_df.iterrows():\n",
    "            level_1_details = [level_1['Employee Name'], level_1['Global Career Band'], level_1['Position Title']]\n",
    "            level_2_df = find_subordinates(df, level_1['Employee Name'])\n",
    "\n",
    "            if not level_2_df.empty:\n",
    "                first_level_1 = True  # To track if Level 1 details have been added\n",
    "                for _, level_2 in level_2_df.iterrows():\n",
    "                    level_2_details = [level_2['Employee Name'], level_2['Global Career Band'], level_2['Position Title']]\n",
    "                    level_3_df = find_subordinates(df, level_2['Employee Name'])\n",
    "\n",
    "                    if not level_3_df.empty:\n",
    "                        first_level_2 = True  # To track if Level 2 details have been added\n",
    "                        for _, level_3 in level_3_df.iterrows():\n",
    "                            level_3_details = [level_3['Employee Name'], level_3['Global Career Band'], level_3['Position Title']]\n",
    "                            rows.append((level_1_details if first_level_1 else [\"\", \"\", \"\"]) +\n",
    "                                        (level_2_details if first_level_2 else [\"\", \"\", \"\"]) +\n",
    "                                        level_3_details)\n",
    "                            first_level_1 = False\n",
    "                            first_level_2 = False\n",
    "                    else:\n",
    "                        rows.append((level_1_details if first_level_1 else [\"\", \"\", \"\"]) + level_2_details + [\"\", \"\", \"\"])\n",
    "                        first_level_1 = False\n",
    "            else:\n",
    "                rows.append(level_1_details + [\"\", \"\", \"\"] + [\"\", \"\", \"\"])\n",
    "\n",
    "        # Write data to the sheet\n",
    "        sheet_name = f\"{bf_level_4_name} - {country}\"\n",
    "        sheet_name = sheet_name[:31]  # Excel sheet names have a max length of 31\n",
    "        sheet = wb.create_sheet(title=sheet_name)\n",
    "        # Write headers\n",
    "        sheet.append([\"Level 1 Employee Name\", \"Global Career Band\", \"Position Title\",\n",
    "                      \"Level 2 Employee Name\", \"Global Career Band\", \"Position Title\",\n",
    "                      \"Level 3 Employee Name\", \"Global Career Band\", \"Position Title\"])\n",
    "        # Write rows\n",
    "        for row in rows:\n",
    "            sheet.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file)\n",
    "\n",
    "print(f\"Organization structure saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Big Grid Stack AOP YEAR  \\\n",
      "0     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "1     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "2     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "3     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "4     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "5     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "6     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "7   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "8   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "9   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "10  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "11  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "12  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "13  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "14  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "15  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "16  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "17  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "18  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "19  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "20    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "21    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "22    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "23    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "24    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "25    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "26    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "27    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "28    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "29    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "30    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "31    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "32    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "\n",
      "   Business Framework RTN Code Business Framework Business Framework Group  \\\n",
      "0                          RTN                                               \n",
      "1                          RTN                                               \n",
      "2                          RTN                                               \n",
      "3                          RTN                                               \n",
      "4                          RTN                                               \n",
      "5                          RTN                                               \n",
      "6                          RTN                                               \n",
      "7                          RTN                                               \n",
      "8                          RTN                                               \n",
      "9                          RTN                                               \n",
      "10                         RTN                                               \n",
      "11                         RTN                                               \n",
      "12                         RTN                                               \n",
      "13                         RTN                                               \n",
      "14                         RTN                                               \n",
      "15                         RTN                                               \n",
      "16                         RTN                                               \n",
      "17                         RTN                                               \n",
      "18                         RTN                                               \n",
      "19                         RTN                                               \n",
      "20                         RTN                                               \n",
      "21                         RTN                                               \n",
      "22                         RTN                                               \n",
      "23                         RTN                                               \n",
      "24                         RTN                                               \n",
      "25                         RTN                                               \n",
      "26                         RTN                                               \n",
      "27                         RTN                                               \n",
      "28                         RTN                                               \n",
      "29                         RTN                                               \n",
      "30                         RTN                                               \n",
      "31                         RTN                                               \n",
      "32                         RTN                                               \n",
      "\n",
      "     Country GCB Emp. Type Hiring Source  FTE Start Month WPB      GBM CMB  \\\n",
      "0   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "1   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "2   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "3   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "4   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "5   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "6   Country1   7       FTE      External  0.5  01/31/2025  0%  100.00%  0%   \n",
      "7   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "8   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "9   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "10  Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "11  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "12  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "13  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "14  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "15  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "16  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "17  Country2   7       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "18  Country2   8       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "19  Country2  na       FTE      External -0.3  01/31/2025  0%  100.00%  0%   \n",
      "20  Country3   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "21  Country3  na       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "22  Country3  na       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "23  Country6  GM       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "24  Country6   3       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "25  Country6   3       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "26  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "27  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "28  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "29  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "30  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "31  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "32  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "\n",
      "   Diver   Entity Description  \n",
      "0         Entity1   Example 1  \n",
      "1         Entity1   Example 1  \n",
      "2         Entity1   Example 1  \n",
      "3         Entity1   Example 1  \n",
      "4         Entity1   Example 1  \n",
      "5         Entity1   Example 1  \n",
      "6         Entity1   Example 1  \n",
      "7         Entity2   Example 2  \n",
      "8         Entity2   Example 2  \n",
      "9         Entity2   Example 2  \n",
      "10        Entity2   Example 2  \n",
      "11        Entity2   Example 2  \n",
      "12        Entity2   Example 2  \n",
      "13        Entity2   Example 2  \n",
      "14        Entity2   Example 2  \n",
      "15        Entity2   Example 2  \n",
      "16        Entity2   Example 2  \n",
      "17        Entity2   Example 2  \n",
      "18        Entity2   Example 2  \n",
      "19        Entity2   Example 2  \n",
      "20        Entity3   Example 3  \n",
      "21        Entity3   Example 3  \n",
      "22        Entity3   Example 3  \n",
      "23        Entity6   Example 6  \n",
      "24        Entity6   Example 6  \n",
      "25        Entity6   Example 6  \n",
      "26        Entity6   Example 6  \n",
      "27        Entity6   Example 6  \n",
      "28        Entity6   Example 6  \n",
      "29        Entity6   Example 6  \n",
      "30        Entity6   Example 6  \n",
      "31        Entity6   Example 6  \n",
      "32        Entity6   Example 6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Example source data\n",
    "data = {\n",
    "    \"FTE\": [6.5, -12.3, 3, 0, None, 10],\n",
    "    \"Type\": [\"BC-TT\", \"Investments\", \"Saves\", \"Other\", \"Saves\", \"BC-TT\"],\n",
    "    \"GM\": [0, 0, 0, 0, 0, 1],\n",
    "    \"MD\": [0, 0, 0, 0, 0, 0],\n",
    "    \"3\": [0, 0, 0, 0, 0, 2],\n",
    "    \"4\": [2.5, -4, 0, 0, 0, 0],\n",
    "    \"5\": [0, 0, 0, 0, 0, 0],\n",
    "    \"6\": [3, -6, 0, 0, 0, 3],\n",
    "    \"7\": [1, -1, 1, 0, 0, 4],\n",
    "    \"8\": [0, -1.3, 0, 0, 0, 0],\n",
    "    \"Entity\": [\"Entity1\", \"Entity2\", \"Entity3\", \"Entity4\", \"Entity5\", \"Entity6\"],\n",
    "    \"Description\": [\"Example 1\", \"Example 2\", \"Example 3\", \"Example 4\", \"Example 5\", \"Example 6\"],\n",
    "    \"Country\": [\"Country1\", \"Country2\", \"Country3\", \"Country4\", \"Country5\", \"Country6\"],\n",
    "    \"RTN\": [\"RTN 123 - Example\", \"RTN 456 - Sample\", \"RTN 789 - Test\", \"RTN 101 - Case\", \"RTN 112 - Trial\", \"RTN 113 - Check\"]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "\n",
    "# Columns to check for GCB values\n",
    "gcb_columns = [\"GM\", \"MD\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# Clean and convert FTE column\n",
    "source_df[\"FTE\"] = pd.to_numeric(source_df[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "# Filter rows where FTE is valid and does not equal 0\n",
    "filtered_df = source_df[\n",
    "    source_df[\"FTE\"].notnull() & (source_df[\"FTE\"] != 0) & (source_df[\"Type\"].isin([\"BC-TT\", \"Investments\", \"Saves\"]))\n",
    "]\n",
    "\n",
    "# Create the template dataframe with required columns\n",
    "template_columns = [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\"\n",
    "]\n",
    "template_df = pd.DataFrame(columns=template_columns)\n",
    "\n",
    "# Process each row in the filtered dataframe\n",
    "for _, row in filtered_df.iterrows():\n",
    "    fte_value = row[\"FTE\"]\n",
    "    duplicate_fte = -1 if fte_value < 0 else 1  # Determine positive or negative FTE\n",
    "    abs_fte = abs(fte_value)\n",
    "    int_part = math.floor(abs_fte)  # Integer part of FTE\n",
    "    fractional_part = abs_fte - int_part  # Fractional part of FTE\n",
    "\n",
    "    # Calculate GCB distribution\n",
    "    gcb_distribution = []\n",
    "    for col in gcb_columns:\n",
    "        value = pd.to_numeric(row[col], errors=\"coerce\")  # Convert to numeric, handle non-numeric gracefully\n",
    "        if not pd.isna(value) and value != 0:\n",
    "            gcb_distribution.extend([col] * abs(math.ceil(value)))\n",
    "\n",
    "    # Ensure GCB distribution matches the FTE rows\n",
    "    total_required_rows = int_part + (1 if fractional_part > 0 else 0)\n",
    "    if len(gcb_distribution) < total_required_rows:\n",
    "        gcb_distribution.extend([\"na\"] * (total_required_rows - len(gcb_distribution)))\n",
    "\n",
    "    # Dynamically calculate Start Month\n",
    "    current_year = 2025\n",
    "    if fte_value > 0:\n",
    "        start_month = datetime.date(current_year, 1, 31).strftime(\"%m/%d/%Y\")  # Example logic for positive FTE\n",
    "    elif fte_value < 0:\n",
    "        start_month = datetime.date(current_year, 2, 28).strftime(\"%m/%d/%Y\")  # Example logic for negative FTE\n",
    "    else:\n",
    "        start_month = datetime.date(current_year, 12, 31).strftime(\"%m/%d/%Y\")  # Default fallback\n",
    "\n",
    "    # Create rows in the template\n",
    "    for i in range(total_required_rows):\n",
    "        current_fte = (\n",
    "            fractional_part if i == total_required_rows - 1 and fractional_part > 0 else 1\n",
    "        ) * duplicate_fte\n",
    "\n",
    "        big_grid_stack = (\n",
    "            \"New Perm Position (within FRP) - Staff Drawdown\" if current_fte > 0\n",
    "            else \"Forecast program/Other Saves-Saves Forecast Tracker\"\n",
    "        )\n",
    "        if row[\"Type\"] == \"BC-TT\":\n",
    "            big_grid_stack = \"Inter Boundary Changes\"\n",
    "\n",
    "        new_row = {\n",
    "            \"Big Grid Stack\": big_grid_stack,\n",
    "            \"AOP YEAR\": \"FY25\",\n",
    "            \"Business Framework RTN Code\": row[\"RTN\"].split()[0],\n",
    "            \"Business Framework\": \"\",\n",
    "            \"Business Framework Group\": \"\",\n",
    "            \"Country\": row[\"Country\"],\n",
    "            \"GCB\": gcb_distribution[i],\n",
    "            \"Emp. Type\": \"FTE\",\n",
    "            \"Hiring Source\": \"External\",\n",
    "            \"FTE\": current_fte,\n",
    "            \"Start Month\": start_month,\n",
    "            \"WPB\": \"0%\",\n",
    "            \"GBM\": \"100.00%\",\n",
    "            \"CMB\": \"0%\",\n",
    "            \"Diver\": \"\",\n",
    "            \"Entity\": row[\"Entity\"],\n",
    "            \"Description\": row[\"Description\"],\n",
    "        }\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the result\n",
    "print(template_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_hdpi_file(input_file, month_name):\n",
    "    \"\"\"\n",
    "    Process the HDPI Excel file.\n",
    "    Modify this function to include your specific processing logic.\n",
    "    \"\"\"\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Perform your processing logic here\n",
    "    # Example: Add a new column for demonstration\n",
    "    df[\"Processed\"] = f\"Processed for {month_name}\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_files_in_folder(ifolder, ofolder):\n",
    "    \"\"\"\n",
    "    Processes all Excel files in the given folder.\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(ofolder, exist_ok=True)\n",
    "\n",
    "    # Get the list of files in the input folder\n",
    "    files = [f for f in os.listdir(ifolder) if f.endswith(\".xlsx\")]\n",
    "\n",
    "    for file_name in files:\n",
    "        try:\n",
    "            # Extract the file path\n",
    "            input_file_path = os.path.join(ifolder, file_name)\n",
    "\n",
    "            # Extract the month name (last three letters before \".xlsx\")\n",
    "            month_name = file_name.split()[-1][:3]\n",
    "\n",
    "            # Load and process the file\n",
    "            processed_df = process_hdpi_file(input_file_path, month_name)\n",
    "\n",
    "            # Save the processed file with the desired name in the output folder\n",
    "            output_file_name = f\"HDPI {month_name}.xlsx\"\n",
    "            output_file_path = os.path.join(ofolder, output_file_name)\n",
    "            processed_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Define input and output folder paths\n",
    "ifolder = r\"C:\\path\\to\\input\\folder\"  # Replace with your input folder path\n",
    "ofolder = r\"C:\\path\\to\\output\\folder\"  # Replace with your output folder path\n",
    "\n",
    "# Process all files in the folder\n",
    "process_files_in_folder(ifolder, ofolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6ebf19",
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, 'Microsoft Word', 'Command failed', 'wdmain11.chm', 36966, -2146824090), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-97dbbe50c67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdocx_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\\\Users\\\\KS\\\\Thesis.docx\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\\\Users\\\\KS\\\\Thesis_test.pdf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mconvert_docx_to_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocx_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-97dbbe50c67d>\u001b[0m in \u001b[0;36mconvert_docx_to_pdf\u001b[1;34m(docx_path, pdf_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Save as PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveAs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFileFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 17 corresponds to the PDF format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Close the document and quit Word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mSaveAs\u001b[1;34m(self, FileName, FileFormat, LockComments, Password, AddToRecentFiles, WritePassword, ReadOnlyRecommended, EmbedTrueTypeFonts, SaveNativePictureFormat, SaveFormsData, SaveAsAOCELetter, Encoding, InsertLineBreaks, AllowSubstitutions, LineEnding, AddBiDiMarks)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, 'Microsoft Word', 'Command failed', 'wdmain11.chm', 36966, -2146824090), None)"
     ]
    }
   ],
   "source": [
    "import win32com.client\n",
    "\n",
    "def convert_docx_to_pdf(docx_path, pdf_path):\n",
    "    # Initialize Word application\n",
    "    word = win32com.client.Dispatch(\"Word.Application\")\n",
    "    word.Visible = False  # Run in the background\n",
    "\n",
    "    # Open the .docx file\n",
    "    doc = word.Documents.Open(docx_path)\n",
    "\n",
    "    # Save as PDF\n",
    "    doc.SaveAs(pdf_path, FileFormat=17)  # 17 corresponds to the PDF format\n",
    "\n",
    "    # Close the document and quit Word\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "\n",
    "# Example usage\n",
    "docx_file = r\"C:\\\\Users\\\\KS\\\\Thesis.docx\"\n",
    "pdf_file = r\"C:\\\\Users\\\\KS\\\\Thesis_test.pdf\"\n",
    "convert_docx_to_pdf(docx_file, pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aa5cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Billing Contact Name  Billing Contact PS ID  Correct Billing Contact PS ID\n",
      "1                Alice                    102                            101\n",
      "3                Alice                    103                            101\n",
      "Filtered Duplicate PSID sheet with mismatched PS IDs has been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Sample data for spocdf and olddf (replace these with actual data loading as needed)\n",
    "spocdf = pd.DataFrame({\n",
    "    'Billing Contact Name': ['Alice', 'Alice', 'Bob', 'Alice', 'Charlie', 'Charlie'],\n",
    "    'Billing Contact PS ID': [101, 102, 201, 103, 301, 301]\n",
    "})\n",
    "\n",
    "olddf = pd.DataFrame({\n",
    "    'Billing Contact Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Billing Contact PS ID': [101, 201, 301]  # Assuming these are the correct PS IDs\n",
    "})\n",
    "\n",
    "# Step 1: Merge spocdf with olddf to add the correct PS ID as a reference\n",
    "merged_df = spocdf.merge(\n",
    "    olddf,\n",
    "    on='Billing Contact Name',\n",
    "    how='left',\n",
    "    suffixes=('', '_correct')\n",
    ")\n",
    "\n",
    "# Step 2: Filter for rows where the 'Billing Contact PS ID' does not match the 'Correct Billing Contact PS ID'\n",
    "# Retain only records where there is a mismatch\n",
    "mismatched_psid_df = merged_df[merged_df['Billing Contact PS ID'] != merged_df['Billing Contact PS ID_correct']].copy()\n",
    "\n",
    "# Rename the correct column for clarity\n",
    "mismatched_psid_df.rename(columns={'Billing Contact PS ID_correct': 'Correct Billing Contact PS ID'}, inplace=True)\n",
    "\n",
    "# # Step 3: Write the filtered DataFrame with mismatched PS IDs to a new sheet in the workbook\n",
    "# file_path = 'your_workbook.xlsx'  # Replace with your actual file path\n",
    "# workbook = load_workbook(file_path)\n",
    "\n",
    "# # Check if 'Duplicate PSID' sheet exists, and delete if it does\n",
    "# if 'Duplicate PSID' in workbook.sheetnames:\n",
    "#     del workbook['Duplicate PSID']\n",
    "# workbook.save(file_path)  # Save after deleting to ensure it's applied\n",
    "\n",
    "# # Append the new data to the workbook in 'Duplicate PSID'\n",
    "# with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "#     mismatched_psid_df.to_excel(writer, sheet_name='Duplicate PSID', index=False)\n",
    "print(mismatched_psid_df)\n",
    "print(\"Filtered Duplicate PSID sheet with mismatched PS IDs has been created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "685ec183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A2    C             SC\n",
      "0  101  new  lookup_value1\n",
      "1  102  old         value2\n",
      "2  103  new  lookup_value3\n",
      "3  104  old         value4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A2': [101, 102, 103, 104],\n",
    "    'C': ['new', 'old', 'new', 'old'],\n",
    "    'SC': [None, 'value2', None, 'value4']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'AA2': [101, 102, 103, 104],\n",
    "    'SC': ['lookup_value1', 'lookup_value2', 'lookup_value3', 'lookup_value4']\n",
    "})\n",
    "\n",
    "# Step 1: Create a mask for rows where 'C' is 'new' and 'SC' is blank (None or NaN)\n",
    "mask = (df1['C'] == 'new') & (df1['SC'].isna())\n",
    "\n",
    "# Step 2: Create a lookup dictionary from df2 for easy access\n",
    "lookup_dict = df2.set_index('AA2')['SC'].to_dict()\n",
    "\n",
    "# Step 3: Use np.where() to update 'SC' column conditionally\n",
    "df1['SC'] = np.where(\n",
    "    mask,  # Only update where the condition is True\n",
    "    df1['A2'].map(lookup_dict).fillna(df1['SC']),  # Map A2 to the lookup dict and fill NaN with existing SC\n",
    "    df1['SC']  # Retain the existing SC values\n",
    ")\n",
    "\n",
    "# Output the updated df1\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60536d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A   C    E     D\n",
      "0  101  X1  Y11  None\n",
      "1  102  X2   Y2  None\n",
      "2  103  X3   Y1  None\n",
      "3  104  X4   Y4  None\n",
      "     A   C    E LookupValue\n",
      "0  101  X1  Y11        Val1\n",
      "1  103  X3   Y2        Val3\n",
      "2  105  X5   Y4        Val5\n",
      "3  106  X6   Y1        Val6\n",
      "     A   C    E     D\n",
      "0  101  X1  Y11  Val1\n",
      "1  102  X2   Y2  Val3\n",
      "2  103  X3   Y1  Val3\n",
      "3  104  X4   Y4  Val5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame 1 (df1)\n",
    "data1 = {'A': [101, 102, 103, 104],\n",
    "         'C': ['X1', 'X2', 'X3', 'X4'],\n",
    "         'E': ['Y11', 'Y2', 'Y1', 'Y4'],\n",
    "         'D': [None, None, None, None]}  # Initially empty Column D\n",
    "df1 = pd.DataFrame(data1)\n",
    "print(df1)\n",
    "# Sample DataFrame 2 (df2)\n",
    "data2 = {'A': [101, 103, 105, 106],\n",
    "         'C': ['X1', 'X3', 'X5', 'X6'],\n",
    "         'E': ['Y11', 'Y2', 'Y4', 'Y1'],\n",
    "         'LookupValue': ['Val1', 'Val3', 'Val5', 'Val6']}  # Column with values to fetch\n",
    "df2 = pd.DataFrame(data2)\n",
    "print(df2)\n",
    "\n",
    "# Step 1: First Lookup - Based on Column 'C'\n",
    "df1 = pd.merge(df1, df2[['C', 'LookupValue']], how='left', on='C')  # Merge on 'C'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Use `mask` to fill NaN in 'D' only\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Step 2: Second Lookup - Based on Column 'E'\n",
    "df1 = pd.merge(df1, df2[['E', 'LookupValue']], how='left', on='E')  # Merge on 'E'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Update only NaNs in 'D'\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Step 3: Third Lookup - Based on Column 'A'\n",
    "df1 = pd.merge(df1, df2[['A', 'LookupValue']], how='left', on='A')  # Merge on 'A'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Update only remaining NaNs in 'D'\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Final Output\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c87da5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jan_2024_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-14917c0930e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Call the function to update the DataFrame based on the presence in each monthly file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mcheck_employee_presence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# Output the final DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-14917c0930e9>\u001b[0m in \u001b[0;36mcheck_employee_presence\u001b[1;34m(df, month_files)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmonth_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Read the month file (assuming it's a CSV file or similar, replace with correct method if different format)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmonth_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming each file contains 'Employee ID' column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Get list of employee IDs present in this month's file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jan_2024_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to generate columns for all months of the year\n",
    "def generate_monthly_columns(year_suffix, num_years=1):\n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    columns = []\n",
    "    \n",
    "    for i in range(num_years):\n",
    "        for month in months:\n",
    "            columns.append(f\"{month} {year_suffix + i}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "# Function to update dataframe with monthly data from files\n",
    "def update_monthly_data(df, folder_path, year_suffix):\n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    current_month = datetime.now().month  # Get the current month (1-12)\n",
    "    \n",
    "    for month_idx in range(current_month):\n",
    "        month_name = months[month_idx]  # Get month name\n",
    "        col_name = f\"{month_name} {year_suffix}\"  # Column name to update\n",
    "        \n",
    "        # Read corresponding file for the current month if exists\n",
    "        file_path = os.path.join(folder_path, f\"{month_name}_{year_suffix}.csv\")  # Assuming CSV format\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            monthly_data = pd.read_csv(file_path)  # Adjust if files are in Excel\n",
    "            \n",
    "            # Assuming you have a common column like 'Position ID' to merge data\n",
    "            if 'Position ID' in monthly_data.columns:\n",
    "                # Merge or update existing data based on 'Position ID'\n",
    "                df = df.merge(monthly_data[['Position ID', 'Value']], on='Position ID', how='left')\n",
    "                \n",
    "                # Assign merged 'Value' to the corresponding column (e.g., \"Jan 24\")\n",
    "                df[col_name] = df['Value']\n",
    "                df = df.drop(columns=['Value'])  # Remove the temporary 'Value' column\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 1: Generate columns for one year\n",
    "columns_2024 = generate_monthly_columns(24)\n",
    "\n",
    "# Step 2: Create an empty dataframe with 'Position ID' and the generated columns\n",
    "df = pd.DataFrame(columns=['Position ID'] + columns_2024)\n",
    "\n",
    "# Step 3: Simulate loading data from previous step (e.g., from GHA)\n",
    "# Assuming 'Position ID' column is already there\n",
    "# df = pd.read_csv('previous_step_data.csv')  # Or however you load it\n",
    "\n",
    "# Step 4: Update DataFrame with monthly data up to the current month\n",
    "folder_path = 'path_to_your_files'  # Specify the folder where monthly files are stored\n",
    "df = update_monthly_data(df, folder_path, 24)\n",
    "\n",
    "# Step 5: Save the final updated DataFrame to Excel or CSV\n",
    "df.to_csv('final_output.csv', index=False)  # Adjust for Excel if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8013ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved with drop-down in column 'Gender'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [24, 30, 22],\n",
    "        'Gender': ['Female', 'Male', 'Male']}  # We will apply drop-down for Gender\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to an Excel file\n",
    "output_path = 'output_with_dropdown.xlsx'\n",
    "\n",
    "# Create a workbook and add the dataframe to the sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Write the DataFrame to the worksheet\n",
    "for row in dataframe_to_rows(df, index=False, header=True):\n",
    "    ws.append(row)\n",
    "\n",
    "# Define the list of allowed values for the drop-down\n",
    "drop_down_list = ['Male', 'Female', 'Other']\n",
    "\n",
    "# Create a DataValidation object for the drop-down list\n",
    "dv = DataValidation(type=\"list\", formula1=f'\"{\",\".join(drop_down_list)}\"', showDropDown=False)\n",
    "\n",
    "# Add the data validation to the 'Gender' column (assuming it is in column C, starting from C2)\n",
    "ws.add_data_validation(dv)\n",
    "dv.add(f\"C2:C{len(df) + 1}\")  # Apply validation to the Gender column for all rows\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"Excel file saved with drop-down in column 'Gender'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541cec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved with a drop-down in the 'Gender' column for all rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [24, 30, 22],\n",
    "        'Gender': ['Female', 'Male', '']}  # Leave empty so users can select from the dropdown\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a workbook and add the dataframe to the sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Write the DataFrame to the worksheet\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), 1):\n",
    "    ws.append(row)\n",
    "\n",
    "# Define the list of allowed values for the drop-down\n",
    "drop_down_list = ['Male', 'Female', 'Other']\n",
    "\n",
    "# Create a DataValidation object for the drop-down list\n",
    "dv = DataValidation(type=\"list\", formula1=f'\"{\",\".join(drop_down_list)}\"', showDropDown=False)\n",
    "\n",
    "# Optional: Add an input prompt when the user selects the cell\n",
    "dv.prompt = \"Select Gender from the list\"\n",
    "dv.promptTitle = \"Gender Selection\"\n",
    "\n",
    "# Apply the data validation to the \"Gender\" column (assuming it's column C)\n",
    "ws.add_data_validation(dv)\n",
    "\n",
    "# Extend the drop-down for all rows in the \"Gender\" column (starting from C2)\n",
    "max_row = len(df) + 100  # Adjust the number of rows if needed\n",
    "dv.add(f\"C2:C{max_row}\")  # Apply the drop-down from C2 to C{max_row}\n",
    "\n",
    "# Save the workbook\n",
    "output_path = 'output_with_dropdown_visible.xlsx'\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"Excel file saved with a drop-down in the 'Gender' column for all rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb279e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0448e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Business Framework Country R3  GCB\n",
      "0          Global CMB         UK    0\n",
      "1          Global CMB         UK    1\n",
      "2          Global CMB         UK    2\n",
      "3          Global CMB         UK    3\n",
      "4          Global CMB         UK    4\n",
      "5          Global CMB         UK    5\n",
      "6          Global CMB         UK    6\n",
      "7          Global CMB         UK    7\n",
      "8     Other Framework         US    0\n",
      "9     Other Framework         US    1\n",
      "10    Other Framework         US    2\n",
      "11    Other Framework         US    3\n",
      "12    Other Framework         US    4\n",
      "13    Other Framework         US    5\n",
      "14    Other Framework         US    6\n",
      "15    Other Framework         US    7\n",
      "16                wpb         IN    0\n",
      "17                wpb         IN    1\n",
      "18                wpb         IN    2\n",
      "19                wpb         IN    3\n",
      "20                wpb         IN    4\n",
      "21                wpb         IN    5\n",
      "22                wpb         IN    6\n",
      "23                wpb         IN    7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is in a CSV file or an Excel file\n",
    "# Replace 'your_file.xlsx' with the path to your Excel file if you're reading from one\n",
    "# df = pd.read_excel('your_file.xlsx')\n",
    "\n",
    "# Sample DataFrame to simulate the input\n",
    "data = {\n",
    "    'Business Framework': ['Global CMB', 'Other Framework','wpb'],  # Sample values\n",
    "    'Country R3': ['UK', 'US','IN'],  # Sample values\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare an empty list to collect the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Repeat each row 8 times with GCB values from 0 to 7\n",
    "    for gcb_value in range(8):\n",
    "        # Append the row values with the new GCB value to the list\n",
    "        expanded_rows.append({\n",
    "            'Business Framework': row['Business Framework'],\n",
    "            'Country R3': row['Country R3'],\n",
    "            'GCB': gcb_value\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file or CSV\n",
    "# expanded_df.to_excel('expanded_output.xlsx', index=False)\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Edge WebDriver using basic options\n",
    "options = webdriver.EdgeOptions()\n",
    "\n",
    "# Attempt to launch Edge without specifying the Service\n",
    "driver = webdriver.Edge(options=options)\n",
    "\n",
    "# Step 2: Open a blank tab to avoid the slow homepage\n",
    "driver.execute_script(\"window.open('');\")\n",
    "time.sleep(1)  # Wait for the new tab to open\n",
    "\n",
    "# Step 3: Switch to the new tab\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "# Step 4: Navigate to the target URL in the new tab\n",
    "target_url = 'https://your-target-website.com'  # Replace with your actual URL\n",
    "driver.get(target_url)\n",
    "\n",
    "# Step 5: Close the default homepage tab (which is the first tab)\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.close()\n",
    "\n",
    "# Step 6: Switch back to the target website tab\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# Step 7: Let the browser stay open for some time\n",
    "time.sleep(10)\n",
    "\n",
    "# Finally, close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da873fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Path to your Microsoft Edge WebDriver (Make sure it matches your browser version)\n",
    "edge_driver_path = \"path/to/msedgedriver.exe\"\n",
    "\n",
    "# Set up Edge WebDriver\n",
    "service = Service(edge_driver_path)\n",
    "driver = webdriver.Edge(service=service)\n",
    "\n",
    "# Open the URL\n",
    "driver.get('https://example.com')  # Replace with your actual URL\n",
    "\n",
    "# Wait for the page to load and for the SSO button to be clickable\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'My organization's single sign-on')]\"))\n",
    ")\n",
    "\n",
    "# Locate and click the \"SSO\" button\n",
    "sso_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'My organization's single sign-on')]\")\n",
    "sso_button.click()\n",
    "\n",
    "# Let the SSO process complete\n",
    "# You can wait for the page to load after the login\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"some_element_after_login\"))  # Replace with an element that appears after login\n",
    ")\n",
    "\n",
    "# You can now proceed to interact with the logged-in page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35379e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def options_to_dict(options):\n",
    "    options_dict = {}\n",
    "    for key in dir(options):\n",
    "        if not key.startswith('__'):\n",
    "            options_dict[key] = getattr(options, key)\n",
    "    return options_dict\n",
    "\n",
    "edge_options = Options()\n",
    "edge_options.use_chromium = True\n",
    "\n",
    "options_dict = options_to_dict(edge_options)\n",
    "options_json = json.dumps(options_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaade20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the browser and SSO settings\n",
    "driver = webdriver.Chrome()  # Replace with your default browser\n",
    "\n",
    "# Open the website and log in using default SSO\n",
    "driver.get(\"https://example.com\")  # Replace with the website URL\n",
    "\n",
    "# Wait for the login process to complete\n",
    "WebDriverWait(driver, 10).until(EC.title_contains(\"Logged in\"))\n",
    "\n",
    "# Navigate to the page with filters\n",
    "driver.get(\"https://example.com/filters\")  # Replace with the filter page URL\n",
    "\n",
    "# Identify and interact with the filter elements\n",
    "filter_elements = driver.find_elements_by_css_selector(\".filter-option\")\n",
    "for element in filter_elements:\n",
    "    # Perform the necessary actions to change the filter options\n",
    "    element.click()  # Replace with the actual action required\n",
    "\n",
    "# Click the ellipsis button to download the file\n",
    "download_button = driver.find_element_by_css_selector(\".download-button\")\n",
    "download_button.click()\n",
    "\n",
    "# Specify the file path and name for the downloaded file\n",
    "file_path = \"/path/to/downloaded/file.csv\"  # Replace with the desired file path and name\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8788a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome(service=Service('/path/to/chromedriver'))\n",
    "\n",
    "# Open the website\n",
    "driver.get('https://example.com')  # Replace with the actual URL\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the filter element by its label text (XPath)\n",
    "filter_element = driver.find_element(By.XPATH, \"//em[text()='BF Level 1 Name']\")\n",
    "\n",
    "# Click on the filter element\n",
    "filter_element.click()\n",
    "\n",
    "# Additional interaction (if needed)\n",
    "\n",
    "# Close the browser after operations\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to map 'Mxx' to actual periods\n",
    "def map_month_to_period(month_str, year):\n",
    "    months_map = {\n",
    "        \"M01\": \"Jan\", \"M02\": \"Feb\", \"M03\": \"Mar\", \"M04\": \"Apr\", \"M05\": \"May\", \n",
    "        \"M06\": \"Jun\", \"M07\": \"Jul\", \"M08\": \"Aug\", \"M09\": \"Sep\", \"M10\": \"Oct\", \n",
    "        \"M11\": \"Nov\", \"M12\": \"Dec\"\n",
    "    }\n",
    "    month_abbr = months_map.get(month_str)\n",
    "    return f\"{month_abbr}-{year}\"\n",
    "\n",
    "# Process HC sheet and populate the 'Paste' sheet\n",
    "def process_hc_sheet(df, paste_df, sheet_name, year):\n",
    "    # Find the last 3 columns with 'Mxx' values for the months\n",
    "    month_cols = [col for col in df.columns if 'M' in col]\n",
    "    if len(month_cols) < 3:\n",
    "        print(f\"Not enough 'Mxx' columns found in {sheet_name}\")\n",
    "        return\n",
    "\n",
    "    # Sort the columns to get the last three\n",
    "    last_3_months = month_cols[-3:]\n",
    "    \n",
    "    for month_col in last_3_months:\n",
    "        month_num = month_col[:3]  # Extract Mxx (e.g., M07)\n",
    "        period = map_month_to_period(month_num, year)\n",
    "        \n",
    "        # Prepare the 'Paste' DataFrame row by row\n",
    "        for _, row in df.iterrows():\n",
    "            paste_row = {\n",
    "                'File Name': sheet_name,\n",
    "                'Entity': row['Entity'],\n",
    "                'Function': row['RTN Level 4'],\n",
    "                'FTE/Contractor': row[month_col],  # Value from the month column\n",
    "                'Period': period,\n",
    "                'Attribute Type': 'MoM',\n",
    "            }\n",
    "            # Append to 'Paste' DataFrame\n",
    "            paste_df = paste_df.append(paste_row, ignore_index=True)\n",
    "    \n",
    "    return paste_df\n",
    "\n",
    "# Main script to process the sheets\n",
    "def consolidate_hc_sheets(sheets_dict, year):\n",
    "    # Initialize the 'Paste' DataFrame\n",
    "    paste_columns = [\n",
    "        'File Name','Level 3','Level 4','Cost Grouping','Cost Type','Finance Region', \n",
    "        'Attribute Type','Period','Cost','FTE/Contractor','Country','Level 3.5','Level 4.5',\n",
    "        'Entity','Function','Mapped Country','MICA'\n",
    "    ]\n",
    "    paste_df = pd.DataFrame(columns=paste_columns)\n",
    "\n",
    "    # Iterate through the sheets\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        if 'HC' in sheet_name:\n",
    "            paste_df = process_hc_sheet(df, paste_df, sheet_name, year)\n",
    "\n",
    "    return paste_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'sheets_dict' contains all loaded sheets as DataFrames\n",
    "sheets_dict = {\n",
    "    'HC_Sheet1': pd.DataFrame(...),  # Replace with actual loaded sheet data\n",
    "    'HC_Sheet2': pd.DataFrame(...),\n",
    "    # Add more sheets as needed\n",
    "}\n",
    "\n",
    "# Call the function to process and consolidate HC sheets\n",
    "final_paste_df = consolidate_hc_sheets(sheets_dict, 2024)\n",
    "print(final_paste_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67995bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyxlsb import open_workbook\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Path to the xlsb file (wb2)\n",
    "wb2_path = 'path_to_wb2.xlsb'\n",
    "\n",
    "# New Excel workbook to store the sheets that are not excluded\n",
    "new_wb = Workbook()\n",
    "new_wb.remove(new_wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find the header row based on 'Entity', 'RTN Level 4', 'Account'\n",
    "def find_header_row(sheet):\n",
    "    header_row = None\n",
    "    # Iterate through rows to find the correct header row\n",
    "    for row_num, row in enumerate(sheet.rows()):\n",
    "        values = [item.v for item in row]\n",
    "        if len(values) >= 3 and values[0] == 'Entity' and values[1] == 'RTN Level 4' and values[2] == 'Account':\n",
    "            header_row = row_num  # Store the correct header row index (0-based)\n",
    "            break\n",
    "    return header_row\n",
    "\n",
    "# Function to process a sheet and clean it up\n",
    "def process_sheet(sheet, sheet_name):\n",
    "    data = []\n",
    "    header_row = find_header_row(sheet)\n",
    "\n",
    "    if header_row is None:\n",
    "        print(f\"Header row not found in sheet: {sheet_name}\")\n",
    "        return None\n",
    "\n",
    "    # Read the sheet into a list of lists (rows of data)\n",
    "    for row in sheet.rows():\n",
    "        data.append([item.v for item in row])\n",
    "\n",
    "    # Convert data to a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check the values of E15 and F15 (columns with index 4 and 5)\n",
    "    try:\n",
    "        E15_val = str(df.iloc[header_row + 1, 4])  # Value in column E (0-indexed, so 4)\n",
    "        F15_val = str(df.iloc[header_row + 1, 5])  # Value in column F (0-indexed, so 5)\n",
    "        print(f\"Debug: E15 value: {E15_val}, F15 value: {F15_val}\")\n",
    "    except IndexError:\n",
    "        E15_val = ''\n",
    "        F15_val = ''\n",
    "        print(f\"Debug: E15 or F15 index out of range\")\n",
    "\n",
    "    # Define possible month headers\n",
    "    month_headers = ['M{:02d}'.format(i) for i in range(1, 13)]  # Generates 'M01' to 'M12'\n",
    "    print(f\"Debug: Month headers: {month_headers}\")\n",
    "\n",
    "    # Prepare headers for columns A to C from row 15\n",
    "    headers_A_to_C = df.iloc[header_row + 1, :3].values  # Columns A to C headers from row 15\n",
    "    print(f\"Debug: Headers A to C: {headers_A_to_C}\")\n",
    "\n",
    "    if E15_val in month_headers and F15_val in month_headers:\n",
    "        # Use row 15 for headers for all columns\n",
    "        headers_D_onwards = df.iloc[header_row + 1, 3:].values  # Columns D onwards headers from row 15\n",
    "        headers = list(headers_A_to_C) + list(headers_D_onwards)\n",
    "        start_data_row = header_row + 2  # Data starts after the header row\n",
    "        print(\"Debug: Using headers from row 15 for columns D onwards\")\n",
    "    else:\n",
    "        # Use row 14 for columns D onwards\n",
    "        headers_D_onwards = df.iloc[header_row, 3:].values  # Columns D onwards headers from row 14\n",
    "        headers = list(headers_A_to_C) + list(headers_D_onwards)\n",
    "        start_data_row = header_row + 1  # Data starts after row 14\n",
    "        print(\"Debug: Using headers from row 14 for columns D onwards\")\n",
    "\n",
    "    # Set the correct headers for the dataframe\n",
    "    df = df.iloc[start_data_row:].reset_index(drop=True)  # Keep only rows after the header\n",
    "    df.columns = headers\n",
    "\n",
    "    # Drop rows where all values are NaN or None\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Open the xlsb workbook and process valid sheets\n",
    "with open_workbook(wb2_path) as wb2:\n",
    "    for sheetname in wb2.sheets:\n",
    "        # Exclude sheets with 'excl', 'Pivot', or 'TM1'\n",
    "        if any(exclude_word.lower() in sheetname.lower() for exclude_word in ['excl', 'pivot', 'tm1']):\n",
    "            print(f\"Skipping sheet: {sheetname}\")\n",
    "            continue\n",
    "\n",
    "        with wb2.get_sheet(sheetname) as sheet:\n",
    "            print(f\"Processing sheet: {sheetname}\")\n",
    "            df = process_sheet(sheet, sheetname)\n",
    "\n",
    "            if df is not None:\n",
    "                # Save DataFrame to a new sheet in the new workbook\n",
    "                new_sheet = new_wb.create_sheet(title=sheetname[:31])  # Sheet names max length is 31 characters\n",
    "\n",
    "                # Write the headers to the sheet\n",
    "                new_sheet.append(df.columns.tolist())\n",
    "\n",
    "                # Write the data rows to the sheet\n",
    "                for row in df.itertuples(index=False, name=None):\n",
    "                    new_sheet.append(row)\n",
    "\n",
    "# Save the new workbook with the non-excluded sheets\n",
    "output_path = 'non_excluded_sheets_with_headers.xlsx'\n",
    "new_wb.save(output_path)\n",
    "print(f\"Workbook with non-excluded sheets and correct headers has been saved to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f78f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyxlsb import open_workbook\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Path to the xlsb file (wb2)\n",
    "wb2_path = 'path_to_wb2.xlsb'\n",
    "\n",
    "# New Excel workbook to store the results\n",
    "new_wb = Workbook()\n",
    "new_wb.remove(new_wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find the header row based on 'Entity', 'RTN Level 4', 'Account'\n",
    "def find_header_row(sheet):\n",
    "    header_row = None\n",
    "    # Iterate through rows to find the correct header row\n",
    "    for row_num, row in enumerate(sheet.rows()):\n",
    "        values = [item.v for item in row]\n",
    "        if len(values) >= 3 and values[0] == 'Entity' and values[1] == 'RTN Level 4' and values[2] == 'Account':\n",
    "            header_row = row_num  # Store the correct header row index (0-based)\n",
    "            break\n",
    "    return header_row\n",
    "\n",
    "# Function to process a sheet and clean it up\n",
    "def process_sheet(sheet, sheet_name):\n",
    "    data = []\n",
    "    header_row = find_header_row(sheet)\n",
    "\n",
    "    if header_row is None:\n",
    "        print(f\"Header row not found in sheet: {sheet_name}\")\n",
    "        return None\n",
    "\n",
    "    # Read the sheet into a list of lists (rows of data)\n",
    "    for row in sheet.rows():\n",
    "        data.append([item.v for item in row])\n",
    "\n",
    "    # Convert data to a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check the values of E15 and F15 (row 15 is 0-based index 14 in pandas)\n",
    "    E15_val = str(df.iloc[header_row + 1, 4])  # Value in column E (0-indexed, so 4)\n",
    "    F15_val = str(df.iloc[header_row + 1, 5])  # Value in column F (0-indexed, so 5)\n",
    "\n",
    "    # Define possible month headers\n",
    "    month_headers = ['M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10', 'M11', 'M12']\n",
    "\n",
    "    # Adjust headers for columns D onwards based on the content of columns E and F\n",
    "    if E15_val in month_headers and F15_val in month_headers:\n",
    "        # Use row 15 for headers (index 14 in pandas)\n",
    "        headers = df.iloc[header_row + 1].values  # Take headers from row 15\n",
    "        start_data_row = header_row + 2  # Data starts after row 15\n",
    "    else:\n",
    "        # Use row 14 for headers (index 13 in pandas)\n",
    "        headers = df.iloc[header_row].values  # Take headers from row 14\n",
    "        start_data_row = header_row + 1  # Data starts after row 14\n",
    "\n",
    "    # Set the correct headers for the dataframe\n",
    "    df.columns = headers\n",
    "    df = df[start_data_row:].reset_index(drop=True)  # Keep only rows after the header\n",
    "\n",
    "    # Drop rows where all values are NaN or None\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Open the xlsb workbook and process valid sheets\n",
    "with open_workbook(wb2_path) as wb2:\n",
    "    for sheetname in wb2.sheets:\n",
    "        # Exclude sheets with 'excl', 'Pivot', or 'TM1'\n",
    "        if any(exclude_word.lower() in sheetname.lower() for exclude_word in ['excl', 'Pivot', 'TM1']):\n",
    "            print(f\"Skipping sheet: {sheetname}\")\n",
    "            continue\n",
    "\n",
    "        with wb2.get_sheet(sheetname) as sheet:\n",
    "            print(f\"Processing sheet: {sheetname}\")\n",
    "            df = process_sheet(sheet, sheetname)\n",
    "            \n",
    "            if df is not None:\n",
    "                # Save DataFrame to a new sheet in the new workbook\n",
    "                new_sheet = new_wb.create_sheet(title=sheetname)\n",
    "                for r in df.itertuples(index=False, name=None):\n",
    "                    new_sheet.append(r)\n",
    "\n",
    "# Save the new workbook with the updated data\n",
    "output_path = 'updated_wb.xlsx'\n",
    "new_wb.save(output_path)\n",
    "print(f\"Data has been written to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc2a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/KS/data.xlsx None None None None None C:/Users/KS/final_output.xlsx None\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "class ExcelSheetSelector:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Excel Sheet Selector\")\n",
    "\n",
    "        # Custom labels for each file selection\n",
    "        self.custom_labels = [\n",
    "            \"Select OFile:\", \"Select CFile:\", \"Select DFile:\", \"Select Fourth File:\",\n",
    "            \"Select Fifth File:\", \"Select Sixth File:\", \"Select Seventh File:\", \"Select Eighth File:\"\n",
    "        ]\n",
    "\n",
    "        # List to hold selected file paths, initially None\n",
    "        self.selected_files = [None] * 8\n",
    "        self.labels = []\n",
    "\n",
    "        # Create custom labels and buttons for selecting files\n",
    "        for i in range(8):\n",
    "            # Create a custom label for each file selection\n",
    "            custom_label = tk.Label(root, text=self.custom_labels[i], width=20, anchor='w')\n",
    "            custom_label.grid(row=i, column=0, padx=10, pady=5)\n",
    "\n",
    "            # Create a label to display the selected file path\n",
    "            label = tk.Label(root, text=\"Not selected\", width=60, anchor='w')\n",
    "            label.grid(row=i, column=1, padx=10, pady=5)\n",
    "            self.labels.append(label)\n",
    "\n",
    "            # Create a button for each file selection\n",
    "            button = tk.Button(root, text=\"Browse\", command=lambda index=i: self.select_file(index))\n",
    "            button.grid(row=i, column=2, padx=10, pady=5)\n",
    "\n",
    "        # Button to confirm file selections\n",
    "        confirm_button = tk.Button(root, text=\"Confirm Selection\", command=self.confirm_selection)\n",
    "        confirm_button.grid(row=8, column=0, columnspan=3, pady=20)\n",
    "\n",
    "        # Predefined variable names for file paths\n",
    "        self.OFile = None\n",
    "        self.CFile = None\n",
    "        self.DFile = None\n",
    "        self.file4 = None\n",
    "        self.file5 = None\n",
    "        self.file6 = None\n",
    "        self.file7 = None\n",
    "        self.file8 = None\n",
    "\n",
    "    def select_file(self, index):\n",
    "        # Open file dialog to select a file\n",
    "        file_path = filedialog.askopenfilename(title=\"Select a File\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "        \n",
    "        if file_path:  # If a file is selected\n",
    "            self.selected_files[index] = file_path  # Save the selected file\n",
    "            self.labels[index].config(text=file_path)  # Update label with the file path\n",
    "\n",
    "    def confirm_selection(self):\n",
    "        # Check how many files have been selected\n",
    "        num_selected = sum(1 for file in self.selected_files if file)\n",
    "\n",
    "        if num_selected == 0:\n",
    "            messagebox.showwarning(\"Warning\", \"No files selected.\")\n",
    "        else:\n",
    "            # Show confirmation dialog with options to Confirm or Select More Files\n",
    "            result = messagebox.askquestion(\n",
    "                \"Confirmation\", \n",
    "                f\"You have selected {num_selected} files. Do you want to proceed?\", \n",
    "                icon='question'\n",
    "            )\n",
    "            \n",
    "            if result == 'yes':\n",
    "                # Directly assign file paths to predefined variables\n",
    "                self.assign_file_paths()\n",
    "                messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "                # Print assigned files for verification\n",
    "                print(self.OFile, self.CFile, self.DFile, self.file4, self.file5, self.file6, self.file7, self.file8)\n",
    "                # Close the GUI\n",
    "                self.root.destroy()\n",
    "            else:\n",
    "                # Allow the user to select more files\n",
    "                messagebox.showinfo(\"Select More\", \"Please select more files.\")\n",
    "\n",
    "    def assign_file_paths(self):\n",
    "        \"\"\"Assigns selected file paths to predefined variables.\"\"\"\n",
    "        self.OFile = self.selected_files[0] if len(self.selected_files) > 0 else None\n",
    "        self.CFile = self.selected_files[1] if len(self.selected_files) > 1 else None\n",
    "        self.DFile = self.selected_files[2] if len(self.selected_files) > 2 else None\n",
    "        self.file4 = self.selected_files[3] if len(self.selected_files) > 3 else None\n",
    "        self.file5 = self.selected_files[4] if len(self.selected_files) > 4 else None\n",
    "        self.file6 = self.selected_files[5] if len(self.selected_files) > 5 else None\n",
    "        self.file7 = self.selected_files[6] if len(self.selected_files) > 6 else None\n",
    "        self.file8 = self.selected_files[7] if len(self.selected_files) > 7 else None\n",
    "\n",
    "# Initialize the Tkinter window and ExcelSheetSelector class\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ExcelSheetSelector(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36d374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def select_file(index):\n",
    "    # Open file dialog to select a file\n",
    "    file_path = filedialog.askopenfilename(title=\"Select a File\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "    \n",
    "    if file_path:  # If a file is selected\n",
    "        selected_files[index] = file_path  # Save the selected file\n",
    "        labels[index].config(text=file_path)  # Update label with the file path\n",
    "\n",
    "def confirm_selection():\n",
    "    # Check how many files have been selected\n",
    "    num_selected = sum(1 for file in selected_files if file)\n",
    "    \n",
    "    if num_selected == 0:\n",
    "        messagebox.showwarning(\"Warning\", \"No files selected.\")\n",
    "    else:\n",
    "        # Show confirmation dialog with options to Confirm or Select More Files\n",
    "        result = messagebox.askquestion(\"Confirmation\", f\"You have selected {num_selected} files. Do you want to proceed?\", icon='question')\n",
    "        \n",
    "        if result == 'yes':\n",
    "            # Proceed and close the application\n",
    "            messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "            root.destroy()\n",
    "        else:\n",
    "            # Allow the user to select more files\n",
    "            messagebox.showinfo(\"Select More\", \"Please select more files.\")\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"File Selector\")\n",
    "\n",
    "# List to hold selected file paths\n",
    "selected_files = [None] * 8\n",
    "labels = []\n",
    "\n",
    "# Create labels and buttons for selecting files\n",
    "for i in range(8):\n",
    "    # Create a label for each file selection\n",
    "    label = tk.Label(root, text=f\"File {i+1}: Not selected\", width=100, anchor='w')\n",
    "    label.grid(row=i, column=0, padx=10, pady=5)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Create a button for each file selection\n",
    "    button = tk.Button(root, text=\"Select File\", command=lambda index=i: select_file(index))\n",
    "    button.grid(row=i, column=1, padx=10, pady=5)\n",
    "\n",
    "# Button to confirm file selections\n",
    "confirm_button = tk.Button(root, text=\"Confirm Selection\", command=confirm_selection)\n",
    "confirm_button.grid(row=8, column=0, columnspan=2, pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def select_files():\n",
    "    # Open the file dialog to select files\n",
    "    file_paths = filedialog.askopenfilenames(title=\"Select Files\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "    \n",
    "    # Update the listbox with selected files\n",
    "    listbox.delete(0, tk.END)\n",
    "    for file in file_paths:\n",
    "        listbox.insert(tk.END, file)\n",
    "    \n",
    "    # Save the selected files to the global variable\n",
    "    selected_files.clear()\n",
    "    selected_files.extend(file_paths)\n",
    "    \n",
    "    # Confirm the number of selected files\n",
    "    if len(selected_files) == 0:\n",
    "        messagebox.showinfo(\"Information\", \"No files selected.\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Information\", f\"You have selected {len(selected_files)} files.\")\n",
    "\n",
    "def confirm_selection():\n",
    "    # Proceed with the application logic\n",
    "    if len(selected_files) > 0:\n",
    "        messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "    else:\n",
    "        messagebox.showwarning(\"Warning\", \"No files selected. Proceeding with the application.\")\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"File Selector\")\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "# Create a Listbox to display selected files\n",
    "listbox = tk.Listbox(root, width=100, height=10)\n",
    "listbox.pack(pady=10)\n",
    "\n",
    "# Add buttons to select files and confirm selection\n",
    "select_button = tk.Button(root, text=\"Select Files\", command=select_files)\n",
    "select_button.pack(pady=5)\n",
    "\n",
    "confirm_button = tk.Button(root, text=\"Confirm Selection\", command=confirm_selection)\n",
    "confirm_button.pack(pady=5)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Staff ID Position Changed                            Control Items  \\\n",
      "0       101             Left                              EUC 1 Name1   \n",
      "1       102        No Change  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "2       102              Yes  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "3       102             Left  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "4       103        No Change                              EUC 5 Name5   \n",
      "\n",
      "  Control Domain  \n",
      "0       EUC only  \n",
      "1            mix  \n",
      "2            mix  \n",
      "3            mix  \n",
      "4       EUC only  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Record Type': ['EUC', 'EUC', 'EUC', 'Other', 'EUC'],\n",
    "    'Record ID': [1, 2, 3, 4, 5],\n",
    "    'Record Name': ['Name1', 'Name2', 'Name3', 'Name4', 'Name5'],\n",
    "    'Staff ID': [101, 102, 102, 102, 103],\n",
    "    'Position Changed': ['Left', 'No Change', 'Yes', 'Left', 'No Change']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to concatenate control items and determine control domain\n",
    "def concatenate_control_items(group):\n",
    "    control_items = '\\n'.join(group['Record Type'] + ' ' + group['Record ID'].astype(str) + ' ' + group['Record Name'])\n",
    "    record_types = group['Record Type'].unique()\n",
    "    if len(record_types) > 1:\n",
    "        control_domain = 'mix'\n",
    "    elif 'EUC' in record_types:\n",
    "        control_domain = 'EUC only'\n",
    "    else:\n",
    "        control_domain = 'Other'\n",
    "    return pd.Series({\n",
    "        'Control Items': control_items,\n",
    "        'Control Domain': control_domain\n",
    "    })\n",
    "\n",
    "# Group by 'Staff ID' and apply the function\n",
    "result_df = df.groupby('Staff ID').apply(concatenate_control_items).reset_index()\n",
    "\n",
    "# Merge the result back to the original DataFrame to maintain original rows\n",
    "final_df = df.drop(columns=['Record Type', 'Record ID', 'Record Name']).drop_duplicates().merge(result_df, on='Staff ID', how='left')\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# Save the result to an Excel file if needed\n",
    "final_df.to_excel('final_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818cefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans file: C:/Users/KS/Start-Data-Analysis.xlsx\n",
      "Animals file: C:/Users/KS/Source.xlsx\n",
      "Plants file: C:/Users/KS/sample.xlsx\n",
      "Minerals file: C:/Users/KS/sam.xlsx\n",
      "Microbes file: C:/Users/KS/sf2_output.xlsx\n",
      "Fungi file: C:/Users/KS/modified_workbook1.xlsx\n",
      "Algae file: C:/Users/KS/modified_workbook.xlsx\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "class ExcelSheetSelector:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Excel Sheet Selector\")\n",
    "\n",
    "        self.file_paths = [None] * 7\n",
    "        self.labels = []\n",
    "\n",
    "        # Specific labels for each file selection\n",
    "        self.label_texts = [\n",
    "            \"Select Humans file:\",\n",
    "            \"Select Animals file:\",\n",
    "            \"Select Plants file:\",\n",
    "            \"Select Minerals file:\",\n",
    "            \"Select Microbes file:\",\n",
    "            \"Select Fungi file:\",\n",
    "            \"Select Algae file:\"\n",
    "        ]\n",
    "\n",
    "        # Create labels and buttons for 7 Excel sheets\n",
    "        for i in range(7):\n",
    "            label = tk.Label(root, text=self.label_texts[i])\n",
    "            label.grid(row=i, column=0, padx=10, pady=5)\n",
    "            \n",
    "            button = tk.Button(root, text=\"Browse\", command=lambda i=i: self.select_file(i))\n",
    "            button.grid(row=i, column=1, padx=10, pady=5)\n",
    "            \n",
    "            file_label = tk.Label(root, text=\"No file selected\")\n",
    "            file_label.grid(row=i, column=2, padx=10, pady=5)\n",
    "            self.labels.append(file_label)\n",
    "        \n",
    "        # Button to check and print selected file paths\n",
    "        self.print_button = tk.Button(root, text=\"Confirm Selections\", command=self.check_and_confirm)\n",
    "        self.print_button.grid(row=7, column=0, columnspan=3, pady=10)\n",
    "\n",
    "    def select_file(self, index):\n",
    "        self.root.withdraw()  # Hide the main window\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
    "        self.root.deiconify()  # Show the main window again\n",
    "        if file_path:\n",
    "            self.file_paths[index] = file_path\n",
    "            self.labels[index].config(text=file_path)\n",
    "\n",
    "    def check_and_confirm(self):\n",
    "        if all(self.file_paths):\n",
    "            self.root.destroy()  # Close the window gracefully\n",
    "        else:\n",
    "            messagebox.showwarning(\"Warning\", \"Please select all files before proceeding.\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "app = ExcelSheetSelector(root)\n",
    "root.mainloop()\n",
    "\n",
    "# After closing the window, capture the file paths into separate variables\n",
    "humans_file, animals_file, plants_file, minerals_file, microbes_file, fungi_file, algae_file = app.file_paths\n",
    "\n",
    "# Print the captured file paths (for verification)\n",
    "print(f\"Humans file: {humans_file}\")\n",
    "print(f\"Animals file: {animals_file}\")\n",
    "print(f\"Plants file: {plants_file}\")\n",
    "print(f\"Minerals file: {minerals_file}\")\n",
    "print(f\"Microbes file: {microbes_file}\")\n",
    "print(f\"Fungi file: {fungi_file}\")\n",
    "print(f\"Algae file: {algae_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32917012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Staff ID                          Control Items Position Changed  \\\n",
      "0         1  Issue 101 A\\nIssue 102 B\\nIssue 104 D              Yes   \n",
      "1         2             Action 103 C\\nAction 105 E              Yes   \n",
      "\n",
      "              Record Details  \n",
      "0  Detail1\\nDetail2\\nDetail4  \n",
      "1           Detail3\\nDetail5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation for demonstration\n",
    "data = {\n",
    "    'Record Type': ['Issue', 'Issue', 'Action', 'Issue', 'Action'],\n",
    "    'Record ID': [101, 102, 103, 104, 105],\n",
    "    'Record Name': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'Record Details': ['Detail1', 'Detail2', 'Detail3', 'Detail4', 'Detail5'],\n",
    "    'Staff ID': [1, 1, 2, 1, 2],\n",
    "    'Position Changed': ['Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to concatenate the desired fields\n",
    "def concatenate_records(group):\n",
    "    return '\\n'.join(group['Record Type'] + ' ' + group['Record ID'].astype(str) + ' ' + group['Record Name'])\n",
    "\n",
    "# Group by 'Staff ID' and apply the concatenation function\n",
    "grouped_df = df.groupby('Staff ID').apply(lambda x: pd.Series({\n",
    "    'Control Items': concatenate_records(x),\n",
    "    'Position Changed': x['Position Changed'].iloc[0]  # Assuming you want to keep the first 'Position Changed' value\n",
    "})).reset_index()\n",
    "\n",
    "# Optional: Include any other columns you want to keep from the original DataFrame\n",
    "# For demonstration, I'm adding 'Record Details' concatenated in the same way\n",
    "grouped_df['Record Details'] = df.groupby('Staff ID')['Record Details'].apply(lambda x: '\\n'.join(x)).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(grouped_df)\n",
    "\n",
    "# Save the final DataFrame to an Excel file\n",
    "# grouped_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ab357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lookup_and_merge(file1_path, file2_path, vOld_path):\n",
    "    # Read the first file into a DataFrame\n",
    "    df1 = pd.read_excel(file1_path)\n",
    "    \n",
    "    # Read the second file into a DataFrame\n",
    "    df2 = pd.read_excel(file2_path)\n",
    "    \n",
    "    # Read the vOld file into a DataFrame\n",
    "    vOld = pd.read_excel(vOld_path)\n",
    "    \n",
    "    # Ensure the Staff ID in both DataFrames are treated as numeric for accurate matching\n",
    "    df1['Employee ID'] = pd.to_numeric(df1['Employee ID'], errors='coerce').astype('Int64')\n",
    "    df2['Staff ID'] = pd.to_numeric(df2['Staff ID'], errors='coerce').astype('Int64')\n",
    "    vOld['Staff ID'] = pd.to_numeric(vOld['Staff ID'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Perform the lookup by merging the DataFrames on Staff ID and Employee ID\n",
    "    merged_df = df2.merge(df1[['Employee ID', 'Position Changed']], \n",
    "                          left_on='Staff ID', right_on='Employee ID', \n",
    "                          how='left')\n",
    "    \n",
    "    # Drop the redundant 'Employee ID' column from the merged DataFrame\n",
    "    merged_df.drop(columns=['Employee ID'], inplace=True)\n",
    "    \n",
    "    # Filter the DataFrame to retain only rows where 'Position Changed' is 'Left' or 'Yes'\n",
    "    filtered_df = merged_df[merged_df['Position Changed'].isin(['Left', 'Yes'])]\n",
    "    \n",
    "    # Ensure 'Staff ID' and 'Position Changed' columns are filled properly\n",
    "    filtered_df['Staff ID'] = filtered_df['Staff ID'].fillna('No ID')\n",
    "    filtered_df['Position Changed'] = filtered_df['Position Changed'].fillna('No Change')\n",
    "    \n",
    "    # Perform the second lookup by merging the filtered_df with vOld on Staff ID\n",
    "    final_df = filtered_df.merge(vOld[['Staff ID', 'Employee Name', 'Employee Business Email Address', 'Global Career Band']], \n",
    "                                 on='Staff ID', \n",
    "                                 how='left')\n",
    "    \n",
    "    # Rename the fetched columns\n",
    "    final_df.rename(columns={\n",
    "        'Employee Name': 'Functional Manager Employee Name',\n",
    "        'Employee Business Email Address': 'Functional Manager Email',\n",
    "        'Global Career Band': 'Functional Manager GCB'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Return the final DataFrame\n",
    "    return final_df\n",
    "\n",
    "# Paths to the Excel files\n",
    "file1_path = 'path_to_file1.xlsx'  # File with Employee ID, Position Changed, etc.\n",
    "file2_path = 'path_to_file2.xlsx'  # File with Record ID, Record Name, Record Details, Staff ID\n",
    "vOld_path = 'path_to_vOld.xlsx'    # File with additional details based on Staff ID\n",
    "\n",
    "# Perform the lookup, merge, and filter\n",
    "final_df = lookup_and_merge(file1_path, file2_path, vOld_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file\n",
    "final_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lookup_and_merge(file1_path, file2_path):\n",
    "    # Read the first file into a DataFrame\n",
    "    df1 = pd.read_excel(file1_path)\n",
    "    \n",
    "    # Read the second file into a DataFrame\n",
    "    df2 = pd.read_excel(file2_path)\n",
    "    \n",
    "    # Ensure the Staff ID in both DataFrames are strings for accurate matching\n",
    "    df1['Employee ID'] = df1['Employee ID'].astype(str)\n",
    "    df2['Staff ID'] = df2['Staff ID'].astype(str)\n",
    "    \n",
    "    # Perform the lookup by merging the DataFrames on Staff ID and Employee ID\n",
    "    merged_df = df2.merge(df1[['Employee ID', 'Position Changed']], \n",
    "                          left_on='Staff ID', right_on='Employee ID', \n",
    "                          how='left')\n",
    "    \n",
    "    # Drop the redundant 'Employee ID' column from the merged DataFrame\n",
    "    merged_df.drop(columns=['Employee ID'], inplace=True)\n",
    "    \n",
    "    # Return the merged DataFrame\n",
    "    return merged_df\n",
    "\n",
    "# Paths to the Excel files\n",
    "file1_path = 'path_to_file1.xlsx'  # File with Employee ID, Position Changed, etc.\n",
    "file2_path = 'path_to_file2.xlsx'  # File with Record ID, Record Name, Record Details, Staff ID\n",
    "\n",
    "# Perform the lookup and merge\n",
    "final_df = lookup_and_merge(file1_path, file2_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# If needed, save the final DataFrame to a new Excel file\n",
    "# final_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e82670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Check and print initial dtypes of columns\n",
    "    print(\"Initial dtypes of vNew:\")\n",
    "    print(vNew.dtypes)\n",
    "    print(\"\\nInitial dtypes of vOld:\")\n",
    "    print(vOld.dtypes)\n",
    "    \n",
    "    # Function to convert columns to int, skipping invalid rows\n",
    "    def convert_to_int(df, column):\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        df.dropna(subset=[column], inplace=True)\n",
    "        df[column] = df[column].astype(int)\n",
    "        return df\n",
    "\n",
    "    # Convert 'Employee ID' and 'Position Number' to integers\n",
    "    vNew = convert_to_int(vNew, 'Employee ID')\n",
    "    vNew = convert_to_int(vNew, 'Position Number')\n",
    "    vOld = convert_to_int(vOld, 'Employee ID')\n",
    "    vOld = convert_to_int(vOld, 'Position Number')\n",
    "    \n",
    "    # Check and print dtypes of columns after coercion attempt\n",
    "    print(\"\\nDtypes of vNew after coercion attempt:\")\n",
    "    print(vNew.dtypes)\n",
    "    print(\"\\nDtypes of vOld after coercion attempt:\")\n",
    "    print(vOld.dtypes)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID using left join\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='left')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID using left join\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='left')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='outer')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_single_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                        record_id_col, record_name_col, record_details_col, record_type_value):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "        record_type_value (str): Value to be placed in 'Record Type' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the specified value\n",
    "    filtered_df['Record Type'] = record_type_value\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add 'Staff ID' and 'Record Type' columns\n",
    "    result_df['Staff ID'] = filtered_df['Staff ID']\n",
    "    result_df['Record Type'] = filtered_df['Record Type']\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def concatenate_files(file_params_list):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_params_list (list of dict): List of dictionaries with parameters for each file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for params in file_params_list:\n",
    "        df = process_single_file(\n",
    "            file_path=params['file_path'],\n",
    "            staff_id_col=params['staff_id_col'],\n",
    "            issue_status_col=params['issue_status_col'],\n",
    "            filter_condition=params['filter_condition'],\n",
    "            record_id_col=params['record_id_col'],\n",
    "            record_name_col=params['record_name_col'],\n",
    "            record_details_col=params['record_details_col'],\n",
    "            record_type_value=params['record_type_value']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_params_list = [\n",
    "    {\n",
    "        'file_path': 'path_to_file1.xlsx',\n",
    "        'staff_id_col': 'Action Owner',\n",
    "        'issue_status_col': 'Issue Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'Issue ID',\n",
    "        'record_name_col': 'Issue Title',\n",
    "        'record_details_col': 'Issue Description',\n",
    "        'record_type_value': 'Issue'\n",
    "    },\n",
    "    {\n",
    "        'file_path': 'path_to_file2.xlsx',\n",
    "        'staff_id_col': 'Owner',\n",
    "        'issue_status_col': 'Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'ID',\n",
    "        'record_name_col': 'Title',\n",
    "        'record_details_col': 'Description',\n",
    "        'record_type_value': 'Action Owner'\n",
    "    },\n",
    "    # Add parameters for other files here\n",
    "]\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = concatenate_files(file_params_list)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                 record_id_col, record_name_col, record_details_col):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the value 'Issue'\n",
    "    filtered_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[staff_id_col, record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        staff_id_col: 'Staff ID',\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add the 'Record Type' column\n",
    "    result_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def process_multiple_files(file_paths, column_mapping, filter_condition):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_paths (list of str): List of paths to input Excel files.\n",
    "        column_mapping (list of dict): List of dictionaries with column mappings for each file.\n",
    "        filter_condition (str): Value to filter records (e.g., 'open').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for file_path, columns in zip(file_paths, column_mapping):\n",
    "        df = process_file(\n",
    "            file_path,\n",
    "            staff_id_col=columns['staff_id'],\n",
    "            issue_status_col=columns['issue_status'],\n",
    "            filter_condition=filter_condition,\n",
    "            record_id_col=columns['record_id'],\n",
    "            record_name_col=columns['record_name'],\n",
    "            record_details_col=columns['record_details']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_paths = [\n",
    "    'path_to_file1.xlsx',\n",
    "    'path_to_file2.xlsx',\n",
    "    'path_to_file3.xlsx',\n",
    "    'path_to_file4.xlsx',\n",
    "    'path_to_file5.xlsx'\n",
    "]\n",
    "\n",
    "column_mapping = [\n",
    "    {\n",
    "        'staff_id': 'Action Owner',  # Column name to extract Staff ID\n",
    "        'issue_status': 'Issue Status',  # Column name for issue status\n",
    "        'record_id': 'Issue ID',  # Column name for Record ID\n",
    "        'record_name': 'Issue Title',  # Column name for Record Name\n",
    "        'record_details': 'Issue Description'  # Column name for Record Details\n",
    "    },\n",
    "    # Add mappings for other files here\n",
    "]\n",
    "\n",
    "filter_condition = 'open'  # Condition to filter records\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = process_multiple_files(file_paths, column_mapping, filter_condition)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to extract Staff ID using regular expression\n",
    "def extract_staff_id(action_owner):\n",
    "    if isinstance(action_owner, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel('path_to_your_file.xlsx')\n",
    "\n",
    "# Ensure \"Action Owner\" column is treated as string\n",
    "df['Action Owner'] = df['Action Owner'].astype(str)\n",
    "\n",
    "# Filter the DataFrame for rows where 'Issue Status' is 'open'\n",
    "open_issues_df = df[df['Issue Status'] == 'open']\n",
    "\n",
    "# Extract Staff ID from \"Action Owner\" column for filtered rows\n",
    "open_issues_df['Staff ID'] = open_issues_df['Action Owner'].apply(extract_staff_id)\n",
    "\n",
    "# Add the 'Record Type' column with value 'Issue'\n",
    "open_issues_df['Record Type'] = 'Issue'\n",
    "\n",
    "# Create the final DataFrame with the required columns\n",
    "result_df = open_issues_df[['Staff ID', 'Record Type', 'Issue ID', 'Issue Title', 'Issue Description']]\n",
    "result_df.rename(columns={\n",
    "    'Issue ID': 'Record ID',\n",
    "    'Issue Title': 'Record Name',\n",
    "    'Issue Description': 'Record Details'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={\n",
    "    'Position Number_new': 'new Position Number',\n",
    "    'Level4_new': 'new Level4',\n",
    "    'Position Number_old': 'old Position Number',\n",
    "    'Level4_old': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Create the \"Position Changed\" column\n",
    "merged_df['Position Changed'] = merged_df.apply(\n",
    "    lambda row: 'New' if pd.isna(row['old Position Number']) or pd.isna(row['old Level4']) else 'No Change', axis=1)\n",
    "\n",
    "# Fetch records from vOld where Employee ID is not in vNew\n",
    "not_in_vNew = vOld[~vOld['Employee ID'].isin(vNew['Employee ID'])]\n",
    "\n",
    "# Rename columns to match the format of merged_df\n",
    "not_in_vNew.rename(columns={\n",
    "    'Position Number': 'old Position Number',\n",
    "    'Level4': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add new columns with default values for records not in vNew\n",
    "not_in_vNew['new Position Number'] = None\n",
    "not_in_vNew['new Level4'] = None\n",
    "not_in_vNew['Position Changed'] = 'Left'\n",
    "\n",
    "# Reorder columns to match the merged_df structure\n",
    "not_in_vNew = not_in_vNew[['Employee ID', 'new Position Number', 'new Level4', 'old Position Number', 'old Level4', 'Position Changed']]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "final_df = pd.concat([merged_df, not_in_vNew], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={'Position Number_new': 'new Position Number',\n",
    "                          'Level4_new': 'new Level4',\n",
    "                          'Position Number_old': 'old Position Number',\n",
    "                          'Level4_old': 'old Level4'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a346553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Create the 'In Scope' column\n",
    "vOld['In Scope'] = ((vOld['Pos Check'] != vOld['Position Number']) | \n",
    "                    (vOld['BF4 Check'] != vOld['BF Level 4 Name']) | \n",
    "                    (vOld['Country Check'] != vOld['Work Location Country/Territory Name'])).apply(lambda x: 'Movement' if x else 'No Movement')\n",
    "\n",
    "# Create 'Position Changed', 'BF Changed', and 'Country Changed' columns\n",
    "vOld['Position Changed'] = vOld.apply(lambda row: 'Left' if row['Pos Check'] == 'left' else ('Yes' if row['Pos Check'] != row['Position Number'] else 'No'), axis=1)\n",
    "vOld['BF Changed'] = vOld.apply(lambda row: 'Left' if row['BF4 Check'] == 'left' else ('Yes' if row['BF4 Check'] != row['BF Level 4 Name'] else 'No'), axis=1)\n",
    "vOld['Country Changed'] = vOld.apply(lambda row: 'Left' if row['Country Check'] == 'left' else ('Yes' if row['Country Check'] != row['Work Location Country/Territory Name'] else 'No'), axis=1)\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the new and old data from the Excel workbooks\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select the required columns\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge the dataframes on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', how='outer', suffixes=('_new', '_old'))\n",
    "\n",
    "# Create the Status column\n",
    "merged_df['Status'] = merged_df.apply(\n",
    "    lambda row: 'new' if pd.isna(row['Position Number_old']) and not pd.isna(row['Position Number_new']) else \n",
    "                ('left' if pd.isna(row['Position Number_new']) and not pd.isna(row['Position Number_old']) else \n",
    "                'existing'), axis=1)\n",
    "\n",
    "# Save the resulting dataframe to a new Excel file\n",
    "merged_df.to_excel('path_to_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'High', 'Low', 'Total']]\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=0, index=False)\n",
    "        \n",
    "        # Load the workbook and access the worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = workbook[sheet_name]\n",
    "\n",
    "        # Initialize start row for writing subtables\n",
    "        start_row = len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create a list to store all subtables\n",
    "        subtables_list = []\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            subtables_list.append((mapped_l3_heading, None))  # Append heading to list\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                subtables_list.append((role_temp_heading, None))  # Append heading to list\n",
    "\n",
    "                # Append the subtable data to the list\n",
    "                subtables_list.append((None, subtable_data))\n",
    "\n",
    "        # Write subtables to Excel with proper gaps\n",
    "        for item in subtables_list:\n",
    "            if item[0]:  # If it's a heading\n",
    "                worksheet.cell(row=start_row, column=1, value=item[0])\n",
    "                start_row += 2  # 2-line gap before next heading or subtable\n",
    "            elif item[1] is not None:  # If it's subtable data\n",
    "                item[1].to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False, header=True)\n",
    "                start_row += len(item[1]) + 1  # 1-line gap after subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901416c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'Total', 'High', 'Low']]\n",
    "\n",
    "        # Initialize start row for writing grouped data\n",
    "        start_row = 0\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "        start_row += len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            worksheet.cell(row=start_row, column=1, value=mapped_l3_heading)\n",
    "\n",
    "            # Move to the next row after writing unique L3 heading\n",
    "            start_row += 2\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                worksheet.cell(row=start_row, column=1, value=role_temp_heading)\n",
    "\n",
    "                # Move to the next row after writing role_temp heading\n",
    "                start_row += 2\n",
    "\n",
    "                # Insert an empty row to separate headings from subtable data\n",
    "                worksheet.cell(row=start_row, column=1, value=\"\")\n",
    "\n",
    "                # Move to the next row after inserting empty row\n",
    "                start_row += 1\n",
    "\n",
    "                # Write the subtable data to the sheet\n",
    "                subtable_data.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "\n",
    "                # Move to the next row after writing subtable data\n",
    "                start_row += len(subtable_data) + 2  # Add extra space after each subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19c99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, None, None, None, None]  # Initially None or some default value\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is still NaN\n",
    "mask = df1['colS'].isna()\n",
    "\n",
    "# Perform the second merge using colA and colPM for the NaN rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the NaN rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5784398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, '', ' ', '   ', '']  # Different types of empty or blank values\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is empty or blank\n",
    "mask = df1['colS'].apply(lambda x: x == '' or x.isspace() if isinstance(x, str) else False)\n",
    "\n",
    "# Perform the second merge using colA and colPM for the empty or blank rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the empty or blank rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44b6bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colS_x</th>\n",
       "      <th>colPM</th>\n",
       "      <th>colS_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  colA  colB colS_x colPM colS_y\n",
       "0    1    10    NaN   NaN    NaN\n",
       "1    2    20    NaN   NaN    NaN\n",
       "2    7    50    NaN     7      a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b765483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS1\n",
      "0    1    10   NaN\n",
      "1    2    20   NaN\n",
      "2    3    30     a\n",
      "3    4    40     c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4],\n",
    "    'colB': [10, 20, 30, 40],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA and colR are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "\n",
    "# Select only the required columns from df2\n",
    "df2_selected = df2[['colR', 'colS']].drop_duplicates(subset='colR')\n",
    "\n",
    "# Merge df1 with df2_selected on colA and colR\n",
    "df_merged = pd.merge(df1, df2_selected, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Assign the values to the new column in df1 and drop any extra columns\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "# Drop the extra merge column if needed (not strictly necessary, but clean)\n",
    "df1.drop(columns=['colR'], inplace=True, errors='ignore')\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d80a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colA colS1\n",
      "0     1   NaN\n",
      "1     2   NaN\n",
      "2     3     a\n",
      "3     4     b\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1\n",
    "data1 = {'colA': [1, 2, 3, 4]}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with non-unique colR values\n",
    "data2 = {'colR': [3, 3, 4, 5, 6, 6], 'colS': ['a', 'b', 'c', 'd', 'e', 'f']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge df1 with df2 on the condition that df1['colA'] matches df2['colR']\n",
    "df_merged = pd.merge(df1, df2, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Select only relevant columns and rename them\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c042a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id colX colY colZ\n",
      "0   6   X6   Y6   Z6\n",
      "1   7   X7   Y7   Z7\n",
      "2   8   X8   Y8   Z8\n",
      "3   1  NaN   B1   C1\n",
      "4   2  NaN   B2   C2\n",
      "5   3  NaN   B3   C3\n",
      "6   4  NaN   B4   C4\n",
      "7   5  NaN   B5   C5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'colA': ['A1', 'A2', 'A3', 'A4', 'A5'],\n",
    "    'colB': ['B1', 'B2', 'B3', 'B4', 'B5'],\n",
    "    'colC': ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'id': [6, 7, 8],\n",
    "    'colX': ['X6', 'X7', 'X8'],\n",
    "    'colY': ['Y6', 'Y7', 'Y8'],\n",
    "    'colZ': ['Z6', 'Z7', 'Z8']\n",
    "})\n",
    "\n",
    "# Columns to copy from df1 and their corresponding columns in df2\n",
    "columns_to_copy = {\n",
    "    'id': 'id',\n",
    "    'colB': 'colY',\n",
    "    'colC': 'colZ'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with the selected columns from df1\n",
    "new_rows = df1[list(columns_to_copy.keys())].copy()\n",
    "\n",
    "# Rename the columns in the new DataFrame to match the column names in df2\n",
    "new_rows.rename(columns=columns_to_copy, inplace=True)\n",
    "\n",
    "# Append the new DataFrame to df2\n",
    "df2 = df2.append(new_rows, ignore_index=True)\n",
    "\n",
    "# Display the updated df2\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
