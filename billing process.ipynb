{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='outer')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_single_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                        record_id_col, record_name_col, record_details_col, record_type_value):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "        record_type_value (str): Value to be placed in 'Record Type' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the specified value\n",
    "    filtered_df['Record Type'] = record_type_value\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add 'Staff ID' and 'Record Type' columns\n",
    "    result_df['Staff ID'] = filtered_df['Staff ID']\n",
    "    result_df['Record Type'] = filtered_df['Record Type']\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def concatenate_files(file_params_list):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_params_list (list of dict): List of dictionaries with parameters for each file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for params in file_params_list:\n",
    "        df = process_single_file(\n",
    "            file_path=params['file_path'],\n",
    "            staff_id_col=params['staff_id_col'],\n",
    "            issue_status_col=params['issue_status_col'],\n",
    "            filter_condition=params['filter_condition'],\n",
    "            record_id_col=params['record_id_col'],\n",
    "            record_name_col=params['record_name_col'],\n",
    "            record_details_col=params['record_details_col'],\n",
    "            record_type_value=params['record_type_value']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_params_list = [\n",
    "    {\n",
    "        'file_path': 'path_to_file1.xlsx',\n",
    "        'staff_id_col': 'Action Owner',\n",
    "        'issue_status_col': 'Issue Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'Issue ID',\n",
    "        'record_name_col': 'Issue Title',\n",
    "        'record_details_col': 'Issue Description',\n",
    "        'record_type_value': 'Issue'\n",
    "    },\n",
    "    {\n",
    "        'file_path': 'path_to_file2.xlsx',\n",
    "        'staff_id_col': 'Owner',\n",
    "        'issue_status_col': 'Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'ID',\n",
    "        'record_name_col': 'Title',\n",
    "        'record_details_col': 'Description',\n",
    "        'record_type_value': 'Action Owner'\n",
    "    },\n",
    "    # Add parameters for other files here\n",
    "]\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = concatenate_files(file_params_list)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e826a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                 record_id_col, record_name_col, record_details_col):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the value 'Issue'\n",
    "    filtered_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[staff_id_col, record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        staff_id_col: 'Staff ID',\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add the 'Record Type' column\n",
    "    result_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def process_multiple_files(file_paths, column_mapping, filter_condition):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_paths (list of str): List of paths to input Excel files.\n",
    "        column_mapping (list of dict): List of dictionaries with column mappings for each file.\n",
    "        filter_condition (str): Value to filter records (e.g., 'open').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for file_path, columns in zip(file_paths, column_mapping):\n",
    "        df = process_file(\n",
    "            file_path,\n",
    "            staff_id_col=columns['staff_id'],\n",
    "            issue_status_col=columns['issue_status'],\n",
    "            filter_condition=filter_condition,\n",
    "            record_id_col=columns['record_id'],\n",
    "            record_name_col=columns['record_name'],\n",
    "            record_details_col=columns['record_details']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_paths = [\n",
    "    'path_to_file1.xlsx',\n",
    "    'path_to_file2.xlsx',\n",
    "    'path_to_file3.xlsx',\n",
    "    'path_to_file4.xlsx',\n",
    "    'path_to_file5.xlsx'\n",
    "]\n",
    "\n",
    "column_mapping = [\n",
    "    {\n",
    "        'staff_id': 'Action Owner',  # Column name to extract Staff ID\n",
    "        'issue_status': 'Issue Status',  # Column name for issue status\n",
    "        'record_id': 'Issue ID',  # Column name for Record ID\n",
    "        'record_name': 'Issue Title',  # Column name for Record Name\n",
    "        'record_details': 'Issue Description'  # Column name for Record Details\n",
    "    },\n",
    "    # Add mappings for other files here\n",
    "]\n",
    "\n",
    "filter_condition = 'open'  # Condition to filter records\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = process_multiple_files(file_paths, column_mapping, filter_condition)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a668e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to extract Staff ID using regular expression\n",
    "def extract_staff_id(action_owner):\n",
    "    if isinstance(action_owner, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel('path_to_your_file.xlsx')\n",
    "\n",
    "# Ensure \"Action Owner\" column is treated as string\n",
    "df['Action Owner'] = df['Action Owner'].astype(str)\n",
    "\n",
    "# Filter the DataFrame for rows where 'Issue Status' is 'open'\n",
    "open_issues_df = df[df['Issue Status'] == 'open']\n",
    "\n",
    "# Extract Staff ID from \"Action Owner\" column for filtered rows\n",
    "open_issues_df['Staff ID'] = open_issues_df['Action Owner'].apply(extract_staff_id)\n",
    "\n",
    "# Add the 'Record Type' column with value 'Issue'\n",
    "open_issues_df['Record Type'] = 'Issue'\n",
    "\n",
    "# Create the final DataFrame with the required columns\n",
    "result_df = open_issues_df[['Staff ID', 'Record Type', 'Issue ID', 'Issue Title', 'Issue Description']]\n",
    "result_df.rename(columns={\n",
    "    'Issue ID': 'Record ID',\n",
    "    'Issue Title': 'Record Name',\n",
    "    'Issue Description': 'Record Details'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={\n",
    "    'Position Number_new': 'new Position Number',\n",
    "    'Level4_new': 'new Level4',\n",
    "    'Position Number_old': 'old Position Number',\n",
    "    'Level4_old': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Create the \"Position Changed\" column\n",
    "merged_df['Position Changed'] = merged_df.apply(\n",
    "    lambda row: 'New' if pd.isna(row['old Position Number']) or pd.isna(row['old Level4']) else 'No Change', axis=1)\n",
    "\n",
    "# Fetch records from vOld where Employee ID is not in vNew\n",
    "not_in_vNew = vOld[~vOld['Employee ID'].isin(vNew['Employee ID'])]\n",
    "\n",
    "# Rename columns to match the format of merged_df\n",
    "not_in_vNew.rename(columns={\n",
    "    'Position Number': 'old Position Number',\n",
    "    'Level4': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add new columns with default values for records not in vNew\n",
    "not_in_vNew['new Position Number'] = None\n",
    "not_in_vNew['new Level4'] = None\n",
    "not_in_vNew['Position Changed'] = 'Left'\n",
    "\n",
    "# Reorder columns to match the merged_df structure\n",
    "not_in_vNew = not_in_vNew[['Employee ID', 'new Position Number', 'new Level4', 'old Position Number', 'old Level4', 'Position Changed']]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "final_df = pd.concat([merged_df, not_in_vNew], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={'Position Number_new': 'new Position Number',\n",
    "                          'Level4_new': 'new Level4',\n",
    "                          'Position Number_old': 'old Position Number',\n",
    "                          'Level4_old': 'old Level4'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a346553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Create the 'In Scope' column\n",
    "vOld['In Scope'] = ((vOld['Pos Check'] != vOld['Position Number']) | \n",
    "                    (vOld['BF4 Check'] != vOld['BF Level 4 Name']) | \n",
    "                    (vOld['Country Check'] != vOld['Work Location Country/Territory Name'])).apply(lambda x: 'Movement' if x else 'No Movement')\n",
    "\n",
    "# Create 'Position Changed', 'BF Changed', and 'Country Changed' columns\n",
    "vOld['Position Changed'] = vOld.apply(lambda row: 'Left' if row['Pos Check'] == 'left' else ('Yes' if row['Pos Check'] != row['Position Number'] else 'No'), axis=1)\n",
    "vOld['BF Changed'] = vOld.apply(lambda row: 'Left' if row['BF4 Check'] == 'left' else ('Yes' if row['BF4 Check'] != row['BF Level 4 Name'] else 'No'), axis=1)\n",
    "vOld['Country Changed'] = vOld.apply(lambda row: 'Left' if row['Country Check'] == 'left' else ('Yes' if row['Country Check'] != row['Work Location Country/Territory Name'] else 'No'), axis=1)\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the new and old data from the Excel workbooks\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select the required columns\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge the dataframes on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', how='outer', suffixes=('_new', '_old'))\n",
    "\n",
    "# Create the Status column\n",
    "merged_df['Status'] = merged_df.apply(\n",
    "    lambda row: 'new' if pd.isna(row['Position Number_old']) and not pd.isna(row['Position Number_new']) else \n",
    "                ('left' if pd.isna(row['Position Number_new']) and not pd.isna(row['Position Number_old']) else \n",
    "                'existing'), axis=1)\n",
    "\n",
    "# Save the resulting dataframe to a new Excel file\n",
    "merged_df.to_excel('path_to_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'High', 'Low', 'Total']]\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=0, index=False)\n",
    "        \n",
    "        # Load the workbook and access the worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = workbook[sheet_name]\n",
    "\n",
    "        # Initialize start row for writing subtables\n",
    "        start_row = len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create a list to store all subtables\n",
    "        subtables_list = []\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            subtables_list.append((mapped_l3_heading, None))  # Append heading to list\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                subtables_list.append((role_temp_heading, None))  # Append heading to list\n",
    "\n",
    "                # Append the subtable data to the list\n",
    "                subtables_list.append((None, subtable_data))\n",
    "\n",
    "        # Write subtables to Excel with proper gaps\n",
    "        for item in subtables_list:\n",
    "            if item[0]:  # If it's a heading\n",
    "                worksheet.cell(row=start_row, column=1, value=item[0])\n",
    "                start_row += 2  # 2-line gap before next heading or subtable\n",
    "            elif item[1] is not None:  # If it's subtable data\n",
    "                item[1].to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False, header=True)\n",
    "                start_row += len(item[1]) + 1  # 1-line gap after subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901416c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'Total', 'High', 'Low']]\n",
    "\n",
    "        # Initialize start row for writing grouped data\n",
    "        start_row = 0\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "        start_row += len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            worksheet.cell(row=start_row, column=1, value=mapped_l3_heading)\n",
    "\n",
    "            # Move to the next row after writing unique L3 heading\n",
    "            start_row += 2\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                worksheet.cell(row=start_row, column=1, value=role_temp_heading)\n",
    "\n",
    "                # Move to the next row after writing role_temp heading\n",
    "                start_row += 2\n",
    "\n",
    "                # Insert an empty row to separate headings from subtable data\n",
    "                worksheet.cell(row=start_row, column=1, value=\"\")\n",
    "\n",
    "                # Move to the next row after inserting empty row\n",
    "                start_row += 1\n",
    "\n",
    "                # Write the subtable data to the sheet\n",
    "                subtable_data.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "\n",
    "                # Move to the next row after writing subtable data\n",
    "                start_row += len(subtable_data) + 2  # Add extra space after each subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19c99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, None, None, None, None]  # Initially None or some default value\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is still NaN\n",
    "mask = df1['colS'].isna()\n",
    "\n",
    "# Perform the second merge using colA and colPM for the NaN rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the NaN rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5784398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, '', ' ', '   ', '']  # Different types of empty or blank values\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is empty or blank\n",
    "mask = df1['colS'].apply(lambda x: x == '' or x.isspace() if isinstance(x, str) else False)\n",
    "\n",
    "# Perform the second merge using colA and colPM for the empty or blank rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the empty or blank rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44b6bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colS_x</th>\n",
       "      <th>colPM</th>\n",
       "      <th>colS_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  colA  colB colS_x colPM colS_y\n",
       "0    1    10    NaN   NaN    NaN\n",
       "1    2    20    NaN   NaN    NaN\n",
       "2    7    50    NaN     7      a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b765483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS1\n",
      "0    1    10   NaN\n",
      "1    2    20   NaN\n",
      "2    3    30     a\n",
      "3    4    40     c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4],\n",
    "    'colB': [10, 20, 30, 40],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA and colR are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "\n",
    "# Select only the required columns from df2\n",
    "df2_selected = df2[['colR', 'colS']].drop_duplicates(subset='colR')\n",
    "\n",
    "# Merge df1 with df2_selected on colA and colR\n",
    "df_merged = pd.merge(df1, df2_selected, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Assign the values to the new column in df1 and drop any extra columns\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "# Drop the extra merge column if needed (not strictly necessary, but clean)\n",
    "df1.drop(columns=['colR'], inplace=True, errors='ignore')\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d80a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colA colS1\n",
      "0     1   NaN\n",
      "1     2   NaN\n",
      "2     3     a\n",
      "3     4     b\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1\n",
    "data1 = {'colA': [1, 2, 3, 4]}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with non-unique colR values\n",
    "data2 = {'colR': [3, 3, 4, 5, 6, 6], 'colS': ['a', 'b', 'c', 'd', 'e', 'f']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge df1 with df2 on the condition that df1['colA'] matches df2['colR']\n",
    "df_merged = pd.merge(df1, df2, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Select only relevant columns and rename them\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c042a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id colX colY colZ\n",
      "0   6   X6   Y6   Z6\n",
      "1   7   X7   Y7   Z7\n",
      "2   8   X8   Y8   Z8\n",
      "3   1  NaN   B1   C1\n",
      "4   2  NaN   B2   C2\n",
      "5   3  NaN   B3   C3\n",
      "6   4  NaN   B4   C4\n",
      "7   5  NaN   B5   C5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'colA': ['A1', 'A2', 'A3', 'A4', 'A5'],\n",
    "    'colB': ['B1', 'B2', 'B3', 'B4', 'B5'],\n",
    "    'colC': ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'id': [6, 7, 8],\n",
    "    'colX': ['X6', 'X7', 'X8'],\n",
    "    'colY': ['Y6', 'Y7', 'Y8'],\n",
    "    'colZ': ['Z6', 'Z7', 'Z8']\n",
    "})\n",
    "\n",
    "# Columns to copy from df1 and their corresponding columns in df2\n",
    "columns_to_copy = {\n",
    "    'id': 'id',\n",
    "    'colB': 'colY',\n",
    "    'colC': 'colZ'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with the selected columns from df1\n",
    "new_rows = df1[list(columns_to_copy.keys())].copy()\n",
    "\n",
    "# Rename the columns in the new DataFrame to match the column names in df2\n",
    "new_rows.rename(columns=columns_to_copy, inplace=True)\n",
    "\n",
    "# Append the new DataFrame to df2\n",
    "df2 = df2.append(new_rows, ignore_index=True)\n",
    "\n",
    "# Display the updated df2\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
