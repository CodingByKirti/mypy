{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa781d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import win32com.client as win32\n",
    "\n",
    "# File Paths (Update these)\n",
    "input_file = r\"C:\\Path\\To\\Reportnames.xlsx\"\n",
    "template_file = r\"C:\\Path\\To\\TemplateWorkbook.xlsx\"\n",
    "output_folder = r\"C:\\Path\\To\\OutputFolder\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Read input data\n",
    "df = pd.read_excel(input_file, usecols=[\"RTN CODE CHILD\", \"Report Name\"])\n",
    "\n",
    "# Start Excel application (COM for Refreshing)\n",
    "excel = win32.Dispatch(\"Excel.Application\")\n",
    "excel.Visible = False  # Run in background\n",
    "\n",
    "# Loop through each row in input file\n",
    "for index, row in df.iterrows():\n",
    "    RTN = row[\"RTN CODE CHILD\"]\n",
    "    ReportName = row[\"Report Name\"]\n",
    "\n",
    "    if pd.isna(RTN) or pd.isna(ReportName):  # Skip empty rows\n",
    "        continue\n",
    "\n",
    "    # Open template workbook\n",
    "    wb = excel.Workbooks.Open(template_file)\n",
    "    ws = wb.Sheets(\"Setup\")\n",
    "\n",
    "    # Paste values\n",
    "    ws.Range(\"D3\").Value = RTN\n",
    "    ws.Range(\"D4\").Value = ReportName\n",
    "\n",
    "    # Refresh data\n",
    "    wb.RefreshAll()\n",
    "    time.sleep(5)  # Wait for refresh\n",
    "\n",
    "    # Save a temporary copy (Retains formatting)\n",
    "    temp_copy_path = os.path.join(output_folder, \"Temp_Copy.xlsx\")\n",
    "    wb.SaveCopyAs(temp_copy_path)\n",
    "    wb.Close(SaveChanges=False)\n",
    "\n",
    "    # Open the copied workbook in Excel (COM method ensures proper formatting)\n",
    "    new_wb = excel.Workbooks.Open(temp_copy_path)\n",
    "\n",
    "    # Select all sheets and convert formulas to values using \"Paste Special\"\n",
    "    for sheet in new_wb.Sheets:\n",
    "        sheet.Cells.Copy()\n",
    "        sheet.Cells.PasteSpecial(Paste=win32.constants.xlPasteValues)\n",
    "\n",
    "    # Save the final file\n",
    "    final_name = os.path.join(output_folder, f\"{ReportName}.xlsx\")\n",
    "    new_wb.SaveAs(final_name, FileFormat=51)  # 51 = xlsx format\n",
    "    new_wb.Close(SaveChanges=True)\n",
    "\n",
    "# Quit Excel\n",
    "excel.Quit()\n",
    "\n",
    "print(\"All files processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input/output paths\n",
    "input_folder = \"Input\"\n",
    "output_file = \"JML_Report.xlsx\"\n",
    "input_file = os.path.join(input_folder, \"your_excel_file.xlsx\")  # Update your filename\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file, sheet_name=\"Headcount Employee Detail\", dtype=str)\n",
    "\n",
    "# Convert required columns to string for lookups\n",
    "df[\"Employee ID\"] = df[\"Employee ID\"].astype(str)\n",
    "df[\"Functional Manager Employee ID\"] = df[\"Functional Manager Employee ID\"].astype(str)\n",
    "df[\"BF Level 2 Name\"] = df[\"BF Level 2 Name\"].astype(str)\n",
    "\n",
    "# Create a lookup dictionary for quick access\n",
    "lookup_dict = df.set_index(\"Employee ID\")[[\"Functional Manager Employee ID\", \"BF Level 2 Name\"]].to_dict(\"index\")\n",
    "\n",
    "# Step 1: Filter for 'Central Managed Services' employees\n",
    "df_cms = df[df[\"BF Level 2 Name\"] == \"Central Managed Services\"].copy()\n",
    "\n",
    "# Initialize columns for tracking hierarchy\n",
    "for i in range(1, 6):  # Up to 5 levels\n",
    "    df_cms[f\"Manager{i}\"] = \"\"\n",
    "    df_cms[f\"Check{i}\"] = \"\"\n",
    "\n",
    "# Function to recursively fetch Functional Manager and BF Level\n",
    "def track_manager_hierarchy(emp_id, max_level=5):\n",
    "    managers = []\n",
    "    bf_levels = []\n",
    "    \n",
    "    for i in range(1, max_level + 1):\n",
    "        manager_id = lookup_dict.get(emp_id, {}).get(\"Functional Manager Employee ID\")\n",
    "        bf_level = lookup_dict.get(emp_id, {}).get(\"BF Level 2 Name\")\n",
    "\n",
    "        if not manager_id or pd.isna(manager_id):\n",
    "            break  # Stop if no manager exists\n",
    "        \n",
    "        managers.append(manager_id)\n",
    "        bf_levels.append(bf_level)\n",
    "\n",
    "        # Move to the next level (next manager)\n",
    "        emp_id = manager_id  \n",
    "\n",
    "    return managers, bf_levels\n",
    "\n",
    "# Step 2: Iterate over each employee and fetch hierarchy details\n",
    "for index, row in df_cms.iterrows():\n",
    "    emp_id = row[\"Employee ID\"]\n",
    "    managers, bf_levels = track_manager_hierarchy(emp_id, max_level=5)\n",
    "\n",
    "    for i, (manager, bf_level) in enumerate(zip(managers, bf_levels), start=1):\n",
    "        df_cms.at[index, f\"Manager{i}\"] = manager\n",
    "        df_cms.at[index, f\"Check{i}\"] = bf_level\n",
    "\n",
    "# Step 3: Identify rows where any \"Check\" column has 'Finance'\n",
    "check_cols = [f\"Check{i}\" for i in range(1, 6)]\n",
    "df_cms[\"Updated BF Level2\"] = df_cms[check_cols].apply(lambda x: \"CMS Finance\" if \"Finance\" in x.values else \"\", axis=1)\n",
    "\n",
    "# Step 4: Save full df_cms (with all original + Check/Manager columns) in \"CMS_Details\"\n",
    "with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "    df_cms.to_excel(writer, sheet_name=\"CMS_Details\", index=False)  # Save full CMS details for reference\n",
    "\n",
    "    # Step 5: Filter rows where 'Updated BF Level2' is 'CMS Finance'\n",
    "    cms_df = df_cms[df_cms[\"Updated BF Level2\"] == \"CMS Finance\"].copy()\n",
    "\n",
    "    # Step 6: Read original sheet again for Finance rows\n",
    "    df_finance = df[df[\"BF Level 2 Name\"] == \"Finance\"].copy()\n",
    "    df_finance[\"Updated BF Level2\"] = \"Finance\"\n",
    "\n",
    "    # Step 7: Remove extra columns before appending to `df_finance`\n",
    "    cms_df_cleaned = cms_df[df_finance.columns]\n",
    "\n",
    "    # Step 8: Reset index types for proper concatenation\n",
    "    cms_df_cleaned = cms_df_cleaned.reset_index(drop=True)\n",
    "    df_finance = df_finance.reset_index(drop=True)\n",
    "\n",
    "    # Step 9: Concatenate cleaned `cms_df` with `df_finance`\n",
    "    final_df = pd.concat([df_finance, cms_df_cleaned], ignore_index=True)\n",
    "\n",
    "    # Step 10: Save the final `JML Report`\n",
    "    final_df.to_excel(writer, sheet_name=\"JML Report\", index=False)\n",
    "\n",
    "print(\"âœ… Process completed successfully! The final file is saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Files\n",
    "output_file = pd.read_excel('Output.xlsx', sheet_name='Sheet1')\n",
    "input_file = pd.read_excel('Input.xlsx', sheet_name='Sheet1')\n",
    "mapping_file = pd.ExcelFile('Mapping.xlsx')\n",
    "ospd_file = pd.read_excel('OSPD.xlsb', sheet_name='CC-List')\n",
    "\n",
    "# Load Mapping Sheets\n",
    "gr_mapping = mapping_file.parse('GR')\n",
    "anaplan_mapping = mapping_file.parse('Anaplan')\n",
    "country_mapping = mapping_file.parse('Country Mapping')\n",
    "\n",
    "# Step 2: Add 'Classification' to Output File\n",
    "output_file = output_file.merge(gr_mapping[['Role Type', 'Classification']], on='Role Type', how='left')\n",
    "\n",
    "# Step 3: Add 'Classification' to Input File\n",
    "input_file = input_file.merge(anaplan_mapping[['Position class', 'Classification']], left_on='Role Type', right_on='Position class', how='left')\n",
    "\n",
    "# Step 4: Add 'Country' to Input File based on 'Country R3'\n",
    "input_file = input_file.merge(country_mapping[['Country R3', 'Country']], on='Country R3', how='left')\n",
    "\n",
    "# Step 5: Add 'RTN Code' from OSPD file\n",
    "input_file = input_file.merge(ospd_file[['L4_BF_Description', 'L4_BF_ID']], left_on='Business Framework', right_on='L4_BF_Description', how='left')\n",
    "input_file.rename(columns={'L4_BF_ID': 'RTN Code'}, inplace=True)\n",
    "\n",
    "# Step 6: Create Unique Key for Input File\n",
    "input_file['Unique Key'] = input_file['Classification'] + ' ' + input_file['RTN Code'] + ' ' + input_file['Country']\n",
    "\n",
    "# Step 7: Create Unique Key for Output File\n",
    "output_file['Unique Key'] = output_file['Classification'] + ' ' + output_file['RTN Code L4'] + ' ' + output_file['Country']\n",
    "\n",
    "# Step 8: Create Secondary Keys\n",
    "input_file['Secondary Key'] = input_file['RTN Code'] + ' ' + input_file['Country']\n",
    "output_file['Secondary Key'] = output_file['RTN Code L4'] + ' ' + output_file['Country']\n",
    "\n",
    "# Step 9: Lookup Data from Input to Output File\n",
    "lookup_cols = ['Business Framework', 'Country', 'Classification']\n",
    "out_cols = ['A_L4', 'A_Country', 'A_Role Type']\n",
    "output_file[out_cols] = output_file[['Unique Key']].merge(input_file[['Unique Key'] + lookup_cols], on='Unique Key', how='left')[lookup_cols]\n",
    "\n",
    "# Step 10: Aggregate Cost & FTE based on Unique and Secondary Keys\n",
    "agg_data = input_file.groupby('Unique Key').agg({'Cost': 'sum', 'FTE': 'sum'}).reset_index()\n",
    "output_file = output_file.merge(agg_data, on='Unique Key', how='left').rename(columns={'Cost': 'A_DC', 'FTE': 'A_FTE'})\n",
    "\n",
    "# Aggregate for Secondary Key\n",
    "agg_data_sec = input_file.groupby('Secondary Key').agg({'Cost': 'sum', 'FTE': 'sum'}).reset_index()\n",
    "output_file = output_file.merge(agg_data_sec, on='Secondary Key', how='left').rename(columns={'Cost': 'Total_DC', 'FTE': 'Total_FTE'})\n",
    "\n",
    "# Save the final processed Output File\n",
    "output_file.to_excel('Processed_Output.xlsx', index=False)\n",
    "\n",
    "print(\"Processing completed. Output saved as 'Processed_Output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input and output file paths\n",
    "input_folder = \"Input\"\n",
    "input_file = os.path.join(input_folder, \"your_excel_file.xlsx\")  # Replace with actual filename\n",
    "output_file = \"JML_Report.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(input_file, sheet_name=\"Headcount Employee Detail\", dtype=str)  # Read everything as text\n",
    "\n",
    "# Filter only \"Central Managed Services\"\n",
    "df_cms = df[df[\"BF Level 2 Name\"] == \"Central Managed Services\"].copy()\n",
    "\n",
    "# Initialize columns for tracking manager levels\n",
    "manager_cols = [\"Check\", \"Manager\", \"Check2\", \"Manager2\", \"Check3\", \"Manager3\", \"Check4\", \"Manager4\", \"Check5\", \"Manager5\", \"Check6\"]\n",
    "\n",
    "# Add empty columns\n",
    "for col in manager_cols:\n",
    "    df_cms[col] = \"\"\n",
    "\n",
    "# Create a dictionary for fast lookups (Employee ID â†’ [Manager ID, BF Level])\n",
    "lookup_dict = df.set_index(\"Employee ID\")[[\"Functional Manager Employee ID\", \"BF Level 2 Name\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Function to track hierarchy up to 5 levels\n",
    "def track_manager_hierarchy(emp_id):\n",
    "    manager_data = {\"Check\": \"\", \"Manager\": \"\", \"Check2\": \"\", \"Manager2\": \"\", \"Check3\": \"\", \"Manager3\": \"\", \"Check4\": \"\", \"Manager4\": \"\", \"Check5\": \"\", \"Manager5\": \"\", \"Check6\": \"\"}\n",
    "    current_emp = emp_id\n",
    "\n",
    "    for i in range(1, 7):  # Loop for levels 1 to 6\n",
    "        if current_emp in lookup_dict:\n",
    "            manager_id = lookup_dict[current_emp][\"Functional Manager Employee ID\"]\n",
    "            bf_level = lookup_dict[current_emp][\"BF Level 2 Name\"]\n",
    "\n",
    "            # Save in corresponding columns\n",
    "            if i == 1:\n",
    "                manager_data[\"Check\"] = bf_level\n",
    "                manager_data[\"Manager\"] = manager_id\n",
    "            else:\n",
    "                manager_data[f\"Check{i}\"] = bf_level\n",
    "                manager_data[f\"Manager{i}\"] = manager_id\n",
    "            \n",
    "            # Move up the hierarchy\n",
    "            current_emp = manager_id\n",
    "        else:\n",
    "            break  # Stop if no manager found\n",
    "\n",
    "    return pd.Series(manager_data)\n",
    "\n",
    "# Apply the function to populate manager levels\n",
    "df_cms[manager_cols] = df_cms[\"Employee ID\"].apply(track_manager_hierarchy)\n",
    "\n",
    "# Identify rows where any \"Check\" column has 'Finance'\n",
    "df_cms[\"Updated BF Level2\"] = df_cms[[\"Check\", \"Check2\", \"Check3\", \"Check4\", \"Check5\", \"Check6\"]].apply(lambda x: \"CMS Finance\" if \"Finance\" in x.values else \"\", axis=1)\n",
    "\n",
    "# Keep only relevant rows\n",
    "cms_df = df_cms[df_cms[\"Updated BF Level2\"] == \"CMS Finance\"]\n",
    "\n",
    "# Read Finance rows from original file\n",
    "df_finance = df[df[\"BF Level 2 Name\"] == \"Finance\"].copy()\n",
    "df_finance[\"Updated BF Level2\"] = \"Finance\"\n",
    "\n",
    "# Append cms_df rows to finance rows\n",
    "final_df = pd.concat([df_finance, cms_df], ignore_index=True)\n",
    "\n",
    "# Save to a new Excel file with sheet name \"JML Report\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    final_df.to_excel(writer, sheet_name=\"JML Report\", index=False)\n",
    "\n",
    "print(\"âœ… JML Report generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the yellow fill for highlighting\n",
    "yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
    "\n",
    "# Load your Excel file\n",
    "file_path = \"your_file.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Ensure all expected columns are present\n",
    "required_columns = ['Original Date', 'Date', 'Type', 'Description']  # Add more columns as needed\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"  # Add missing columns with blank values\n",
    "\n",
    "# Function to safely convert date values\n",
    "def safe_convert_date(date_value):\n",
    "    if pd.isnull(date_value):  # Handle missing values\n",
    "        return None\n",
    "    if isinstance(date_value, datetime):  # Already a datetime object\n",
    "        return date_value\n",
    "    if isinstance(date_value, str):  # Try parsing strings\n",
    "        for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\"):  # Common date formats\n",
    "            try:\n",
    "                return datetime.strptime(date_value, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None  # Return None if conversion fails\n",
    "\n",
    "# Ensure Date and Original Date are in proper datetime format\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = df['Date'].apply(safe_convert_date)\n",
    "if 'Original Date' in df.columns:\n",
    "    df['Original Date'] = df['Original Date'].apply(safe_convert_date)\n",
    "\n",
    "# Function to process Date changes\n",
    "def process_dates(row, current_date):\n",
    "    original_date = row.get('Original Date')\n",
    "    date = row.get('Date')\n",
    "    \n",
    "    # Check if dates are valid\n",
    "    if pd.isna(date) or (original_date and (original_date.year < current_date.year or original_date.month < current_date.month)):\n",
    "        return current_date.replace(day=1) - pd.Timedelta(days=1)  # Last day of current month\n",
    "    return date\n",
    "\n",
    "# Get today's date\n",
    "today = pd.Timestamp.now()\n",
    "current_month_last_date = today.replace(day=1) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Apply logic to update Date column and highlight changes\n",
    "for index, row in df.iterrows():\n",
    "    updated_date = process_dates(row, current_month_last_date)\n",
    "    if updated_date != row['Date']:  # Track changes\n",
    "        df.at[index, 'Date'] = updated_date\n",
    "        ws.cell(row=index + 2, column=df.columns.get_loc('Date') + 1).value = updated_date  # Update cell value\n",
    "        ws.cell(row=index + 2, column=df.columns.get_loc('Date') + 1).fill = yellow_fill  # Highlight updated cell\n",
    "\n",
    "# Handle 'Type' and 'Description' specific logic only if the columns exist\n",
    "if 'Type' in df.columns:\n",
    "    # Example logic for 'Type'\n",
    "    df['Type'] = df['Type'].apply(lambda x: x.upper() if pd.notna(x) else \"\")\n",
    "if 'Description' in df.columns:\n",
    "    # Example logic for 'Description'\n",
    "    df['Description'] = df['Description'].fillna(\"No description available\")\n",
    "\n",
    "# Save the updated Excel file\n",
    "wb.save(\"updated_file.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e945c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Example source data\n",
    "data = {\n",
    "    \"FTE\": [6.5, -12.3, 3, 0, None, 10],\n",
    "    \"Type\": [\"BC-TT\", \"Investments\", \"Saves\", \"Other\", \"Saves\", \"BC-TT\"],\n",
    "    \"GM\": [0, 0, 0, 0, 0, 1],\n",
    "    \"MD\": [0, 0, 0, 0, 0, 0],\n",
    "    \"3\": [0, 0, 0, 0, 0, 2],\n",
    "    \"4\": [2.5, -4, 0, 0, 0, 0],\n",
    "    \"5\": [0, 0, 0, 0, 0, 0],\n",
    "    \"6\": [3, -6, 0, 0, 0, 3],\n",
    "    \"7\": [1, -1, 1, 0, 0, 4],\n",
    "    \"8\": [0, -1.3, 0, 0, 0, 0],\n",
    "    \"Entity\": [\"Entity1\", \"Entity2\", \"Entity3\", \"Entity4\", \"Entity5\", \"Entity6\"],\n",
    "    \"Description\": [\"Example 1\", \"Example 2\", \"Example 3\", \"Example 4\", \"Example 5\", \"Example 6\"],\n",
    "    \"Country\": [\"Country1\", \"Country2\", \"Country3\", \"Country4\", \"Country5\", \"Country6\"],\n",
    "    \"RTN\": [\"RTN 123 - Example\", \"RTN 456 - Sample\", \"RTN 789 - Test\", \"RTN 101 - Case\", \"RTN 112 - Trial\", \"RTN 113 - Check\"]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "\n",
    "# Columns to check for GCB values\n",
    "gcb_columns = [\"GM\", \"MD\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# Clean and convert FTE column\n",
    "source_df[\"FTE\"] = pd.to_numeric(source_df[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "# Filter rows where FTE is valid and does not equal 0\n",
    "filtered_df = source_df[\n",
    "    source_df[\"FTE\"].notnull() & (source_df[\"FTE\"] != 0) & (source_df.get(\"Type\", \"\").isin([\"BC-TT\", \"Investments\", \"Saves\"]))\n",
    "]\n",
    "\n",
    "# Create the template dataframe with required columns\n",
    "template_columns = [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\"\n",
    "]\n",
    "template_df = pd.DataFrame(columns=template_columns)\n",
    "\n",
    "# Process each row in the filtered dataframe\n",
    "for _, row in filtered_df.iterrows():\n",
    "    fte_value = row[\"FTE\"]\n",
    "    duplicate_fte = -1 if fte_value < 0 else 1  # Determine positive or negative FTE\n",
    "    abs_fte = abs(fte_value)\n",
    "    int_part = math.floor(abs_fte)  # Integer part of FTE\n",
    "    fractional_part = abs_fte - int_part  # Fractional part of FTE\n",
    "\n",
    "    # Calculate GCB distribution\n",
    "    gcb_distribution = []\n",
    "    for col in gcb_columns:\n",
    "        value = pd.to_numeric(row.get(col), errors=\"coerce\")  # Convert to numeric, handle non-numeric gracefully\n",
    "        if not pd.isna(value) and value != 0:\n",
    "            gcb_distribution.extend([col] * abs(math.ceil(value)))\n",
    "\n",
    "    # Ensure GCB distribution matches the FTE rows\n",
    "    total_required_rows = int_part + (1 if fractional_part > 0 else 0)\n",
    "    if len(gcb_distribution) < total_required_rows:\n",
    "        gcb_distribution.extend([\"na\"] * (total_required_rows - len(gcb_distribution)))\n",
    "\n",
    "    # Dynamically calculate Start Month\n",
    "    current_year = 2025\n",
    "    if fte_value > 0:\n",
    "        start_month = datetime.date(current_year, 1, 31)  # Example logic for positive FTE\n",
    "    elif fte_value < 0:\n",
    "        start_month = datetime.date(current_year, 2, 28)  # Example logic for negative FTE\n",
    "    else:\n",
    "        start_month = datetime.date(current_year, 12, 31)  # Default fallback\n",
    "\n",
    "    # Create rows in the template\n",
    "    for i in range(total_required_rows):\n",
    "        current_fte = (\n",
    "            fractional_part if i == total_required_rows - 1 and fractional_part > 0 else 1\n",
    "        ) * duplicate_fte\n",
    "\n",
    "        big_grid_stack = (\n",
    "            \"New Perm Position (within FRP) - Staff Drawdown\" if current_fte > 0\n",
    "            else \"Forecast program/Other Saves-Saves Forecast Tracker\"\n",
    "        )\n",
    "        if row.get(\"Type\") == \"BC-TT\":\n",
    "            big_grid_stack = \"Inter Boundary Changes\"\n",
    "\n",
    "        new_row = {\n",
    "            \"Big Grid Stack\": big_grid_stack,\n",
    "            \"AOP YEAR\": \"FY25\",\n",
    "            \"Business Framework RTN Code\": row[\"RTN\"].split()[0],\n",
    "            \"Business Framework\": \"\",\n",
    "            \"Business Framework Group\": \"\",\n",
    "            \"Country\": row.get(\"Country\", \"\"),\n",
    "            \"GCB\": gcb_distribution[i],\n",
    "            \"Emp. Type\": \"FTE\",\n",
    "            \"Hiring Source\": \"External\",\n",
    "            \"FTE\": current_fte,\n",
    "            \"Start Month\": start_month.strftime(\"%m/%d/%Y\"),\n",
    "            \"WPB\": \"0%\",\n",
    "            \"GBM\": \"100.00%\",\n",
    "            \"CMB\": \"0%\",\n",
    "            \"Diver\": \"\",\n",
    "            \"Entity\": row.get(\"Entity\", \"\"),\n",
    "            \"Description\": row.get(\"Description\", \"No description available\"),\n",
    "        }\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the result\n",
    "print(template_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import calendar\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "def adjust_and_highlight_dates(input_file, output_file):\n",
    "    # Read the Excel file into a dataframe\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Get today's date, current year, and current month\n",
    "    today = datetime.datetime.today()\n",
    "    current_year = today.year\n",
    "    current_month = today.month\n",
    "\n",
    "    # Convert the Date column to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # Preserve the original dates for comparison\n",
    "    df['Original Date'] = df['Date']\n",
    "\n",
    "    # Iterate through the dataframe and update dates if necessary\n",
    "    for index, row in df.iterrows():\n",
    "        original_date = row['Original Date']\n",
    "        if pd.notna(original_date):  # Ensure the date is valid\n",
    "            # Check if the year is not current or the month is before the current month\n",
    "            if original_date.year != current_year or original_date.month < current_month:\n",
    "                # Get the last day of the current month\n",
    "                last_day_of_month = datetime.date(current_year, current_month, calendar.monthrange(current_year, current_month)[1])\n",
    "                df.at[index, 'Date'] = last_day_of_month  # Update the date\n",
    "\n",
    "    # Save the updated dataframe to the Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    # Open the workbook to apply formatting\n",
    "    wb = load_workbook(output_file)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Apply yellow highlight to cells in the \"Date\" column where changes were made\n",
    "    yellow_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Original Date'] != row['Date']:  # Compare original and updated date\n",
    "            excel_row = index + 2  # Offset for Excel's 1-based index and header row\n",
    "            ws.cell(row=excel_row, column=2).fill = yellow_fill  # Column 2 corresponds to the 'Date' column\n",
    "\n",
    "    # Save the workbook with highlighted changes\n",
    "    wb.save(output_file)\n",
    "    print(\"Dates adjusted and changes highlighted where necessary.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"input_dates.xlsx\"  # Replace with your input file\n",
    "output_file = \"output_dates.xlsx\"  # Replace with your output file\n",
    "adjust_and_highlight_dates(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47208d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl.styles import Alignment, Font, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def format_excel_with_headers(file_path):\n",
    "    # Load the workbook\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    \n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        \n",
    "        # Insert the sheet name as a heading in A1\n",
    "        sheet.insert_rows(1)\n",
    "        sheet['A1'] = sheet_name\n",
    "        sheet.merge_cells(start_row=1, start_column=1, end_row=1, end_column=sheet.max_column)\n",
    "        sheet['A1'].font = Font(size=14, bold=True)\n",
    "        sheet['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        sheet['A1'].fill = PatternFill(start_color=\"B0C4DE\", end_color=\"B0C4DE\", fill_type=\"solid\")  # Light Blue\n",
    "        \n",
    "        # Apply formatting to the header row (now at row 2)\n",
    "        header_row = 2\n",
    "        for col in range(1, sheet.max_column + 1):\n",
    "            cell = sheet.cell(row=header_row, column=col)\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # Light Blue\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        \n",
    "        # Set column widths\n",
    "        for col in range(1, sheet.max_column + 1):\n",
    "            max_length = 0\n",
    "            column = get_column_letter(col)\n",
    "            for row in range(1, sheet.max_row + 1):\n",
    "                cell = sheet[f\"{column}{row}\"]\n",
    "                try:\n",
    "                    if cell.value:\n",
    "                        max_length = max(max_length, len(str(cell.value)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            adjusted_width = max_length + 2\n",
    "            sheet.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    # Save the updated workbook\n",
    "    workbook.save(file_path)\n",
    "    print(f\"Workbook formatted and saved at: {file_path}\")\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"organization_structure.xlsx\"\n",
    "format_excel_with_headers(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91098aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import openpyxl\n",
    "\n",
    "def add_table_slide(presentation, sheet_name, sheet):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    title = slide.shapes.title\n",
    "    title.text = f\"Data for {sheet_name}\"\n",
    "\n",
    "    # Get the data\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(values_only=True)]\n",
    "    rows, cols = len(data), len(data[0])\n",
    "\n",
    "    # Add table\n",
    "    left = Inches(1.0)\n",
    "    top = Inches(1.5)\n",
    "    width = Inches(8.0)\n",
    "    height = Inches(5.0)\n",
    "    table = slide.shapes.add_table(rows, cols, left, top, width, height).table\n",
    "\n",
    "    # Fill the table with data\n",
    "    for i, row in enumerate(data):\n",
    "        for j, value in enumerate(row):\n",
    "            cell = table.cell(i, j)\n",
    "            cell.text = str(value) if value else \"\"\n",
    "            cell.text_frame.paragraphs[0].font.size = Pt(10)\n",
    "\n",
    "    # Format header row\n",
    "    for cell in table.rows[0].cells:\n",
    "        cell.text_frame.paragraphs[0].font.bold = True\n",
    "        cell.fill.solid()\n",
    "        cell.fill.fore_color.rgb = RGBColor(173, 216, 230)  # Light Blue\n",
    "\n",
    "def generate_table_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        add_table_slide(presentation, sheet_name, sheet)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Example Usage\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_table.pptx\"\n",
    "generate_table_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.util import Inches, Pt\n",
    "import openpyxl\n",
    "\n",
    "def add_org_chart_slide(presentation, sheet_name, hierarchy):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    title = slide.shapes.title\n",
    "    title.text = f\"Organizational Chart: {sheet_name}\"\n",
    "\n",
    "    # Add shapes for hierarchy\n",
    "    top_margin = 1.5  # Inches\n",
    "    left_margin = 1.0  # Inches\n",
    "    shape_width = 2.0  # Inches\n",
    "    shape_height = 1.0  # Inches\n",
    "    level_gap = 1.5  # Vertical gap between levels\n",
    "    horizontal_gap = 2.5  # Horizontal gap between shapes\n",
    "\n",
    "    # Recursive function to draw hierarchy\n",
    "    def draw_hierarchy(manager, x, y, slide):\n",
    "        shape = slide.shapes.add_shape(\n",
    "            MSO_SHAPE.RECTANGLE,\n",
    "            Inches(x), Inches(y),\n",
    "            Inches(shape_width), Inches(shape_height)\n",
    "        )\n",
    "        shape.text = manager\n",
    "        shape.text_frame.paragraphs[0].font.size = Pt(12)\n",
    "        return shape\n",
    "\n",
    "    y = top_margin\n",
    "    for manager1, sub_hierarchy in hierarchy.items():\n",
    "        shape1 = draw_hierarchy(manager1, left_margin, y, slide)\n",
    "        x = left_margin\n",
    "        for manager2, employees in sub_hierarchy.items():\n",
    "            y += level_gap\n",
    "            shape2 = draw_hierarchy(manager2, x + horizontal_gap, y, slide)\n",
    "            for i, employee in enumerate(employees):\n",
    "                draw_hierarchy(employee, x + horizontal_gap * (i + 2), y + level_gap, slide)\n",
    "\n",
    "def generate_org_chart_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        hierarchy = extract_hierarchy(sheet)\n",
    "        add_org_chart_slide(presentation, sheet_name, hierarchy)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Example Usage\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_chart.pptx\"\n",
    "generate_org_chart_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from pptx import Presentation\n",
    "from pptx.util import Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "def extract_hierarchy(sheet):\n",
    "    hierarchy = {}\n",
    "    for row in sheet.iter_rows(min_row=3, values_only=True):  # Start reading data from row 3\n",
    "        level1, level2, level3 = row[0:3], row[3:6], row[6:9]\n",
    "        manager1 = \" | \".join([str(x) for x in level1 if x])\n",
    "        manager2 = \" | \".join([str(x) for x in level2 if x])\n",
    "        employee = \" | \".join([str(x) for x in level3 if x])\n",
    "\n",
    "        if manager1 not in hierarchy:\n",
    "            hierarchy[manager1] = {}\n",
    "        if manager2 not in hierarchy[manager1]:\n",
    "            hierarchy[manager1][manager2] = []\n",
    "        hierarchy[manager1][manager2].append(employee)\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def add_slide_with_chart(presentation, sheet_name, hierarchy):\n",
    "    slide = presentation.slides.add_slide(presentation.slide_layouts[5])  # Blank slide\n",
    "    # Add title\n",
    "    title = slide.shapes.title\n",
    "    title.text = sheet_name\n",
    "    title.text_frame.paragraphs[0].font.bold = True\n",
    "    title.text_frame.paragraphs[0].font.size = Pt(24)\n",
    "\n",
    "    # Add text box for the org chart\n",
    "    left = Pt(50)\n",
    "    top = Pt(100)\n",
    "    width = Pt(800)\n",
    "    height = Pt(500)\n",
    "    text_box = slide.shapes.add_textbox(left, top, width, height)\n",
    "    text_frame = text_box.text_frame\n",
    "    text_frame.word_wrap = True\n",
    "    text_frame.margin_left = Pt(10)\n",
    "    text_frame.margin_top = Pt(10)\n",
    "\n",
    "    # Build the org chart text\n",
    "    text_frame.text = f\"Organizational Chart for {sheet_name}\\n\"\n",
    "    for manager1, sub_hierarchy in hierarchy.items():\n",
    "        p = text_frame.add_paragraph()\n",
    "        p.text = manager1\n",
    "        p.font.bold = True\n",
    "        p.font.size = Pt(16)\n",
    "        p.font.color.rgb = RGBColor(0, 51, 102)  # Dark blue\n",
    "\n",
    "        for manager2, employees in sub_hierarchy.items():\n",
    "            p2 = text_frame.add_paragraph()\n",
    "            p2.text = f\"    {manager2}\"\n",
    "            p2.font.bold = False\n",
    "            p2.font.size = Pt(14)\n",
    "            p2.font.color.rgb = RGBColor(0, 102, 0)  # Green\n",
    "\n",
    "            for employee in employees:\n",
    "                p3 = text_frame.add_paragraph()\n",
    "                p3.text = f\"        {employee}\"\n",
    "                p3.font.size = Pt(12)\n",
    "                p3.font.color.rgb = RGBColor(0, 0, 0)  # Black\n",
    "\n",
    "def generate_organizational_ppt(excel_file, ppt_file):\n",
    "    # Load the Excel workbook\n",
    "    workbook = openpyxl.load_workbook(excel_file)\n",
    "    presentation = Presentation()\n",
    "\n",
    "    # Iterate through sheets and process data\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        hierarchy = extract_hierarchy(sheet)\n",
    "        add_slide_with_chart(presentation, sheet_name, hierarchy)\n",
    "\n",
    "    # Save the PowerPoint\n",
    "    presentation.save(ppt_file)\n",
    "    print(f\"PPT saved as {ppt_file}\")\n",
    "\n",
    "# Input Excel and output PPT file paths\n",
    "input_excel = \"formatted_organization_structure.xlsx\"\n",
    "output_ppt = \"organization_structure_presentation.pptx\"\n",
    "\n",
    "# Generate the presentation\n",
    "generate_organizational_ppt(input_excel, output_ppt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4364122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def format_sheet(sheet):\n",
    "    # Set the sheet name as the title in cell A1\n",
    "    sheet_title = sheet.title\n",
    "    sheet.merge_cells('A1:I1')  # Adjust range based on your data width\n",
    "    sheet['A1'] = sheet_title\n",
    "    sheet['A1'].font = Font(bold=True, size=14)\n",
    "    sheet['A1'].alignment = Alignment(horizontal='center')\n",
    "\n",
    "    # Style the header row\n",
    "    header_fill = PatternFill(start_color=\"ADD8E6\", end_color=\"ADD8E6\", fill_type=\"solid\")  # Light blue background\n",
    "    header_font = Font(bold=True)\n",
    "    header_alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "    # Assuming row 2 is the header row\n",
    "    for cell in sheet[2]:  # Header row is the second row\n",
    "        cell.fill = header_fill\n",
    "        cell.font = header_font\n",
    "        cell.alignment = header_alignment\n",
    "\n",
    "    # Set fixed column width\n",
    "    fixed_width = 20  # Set desired width\n",
    "    for col in sheet.columns:\n",
    "        col_letter = get_column_letter(col[0].column)  # Get column letter\n",
    "        sheet.column_dimensions[col_letter].width = fixed_width\n",
    "\n",
    "def format_workbook(file_path, output_path):\n",
    "    # Load the workbook\n",
    "    wb = load_workbook(file_path)\n",
    "\n",
    "    # Format each sheet\n",
    "    for sheet in wb.worksheets:\n",
    "        format_sheet(sheet)\n",
    "\n",
    "    # Save the formatted workbook\n",
    "    wb.save(output_path)\n",
    "    print(f\"Formatted file saved as: {output_path}\")\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"organization_structure.xlsx\"\n",
    "output_file = \"formatted_organization_structure.xlsx\"\n",
    "\n",
    "# Call the function to format the workbook\n",
    "format_workbook(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Load the input Excel file\n",
    "input_file = \"your_input_file.xlsx\"  # Replace with your input file path\n",
    "output_file = \"organization_structure.xlsx\"  # Output file path\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Ensure consistent column names (case-insensitive matching)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Unique countries\n",
    "countries = df['Work Location Country/Territory Name'].dropna().unique()\n",
    "\n",
    "# Create a new workbook\n",
    "wb = Workbook()\n",
    "wb.remove(wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find subordinates\n",
    "def find_subordinates(df, manager_name):\n",
    "    return df[df['Entity Manager Employee Name'] == manager_name]\n",
    "\n",
    "# Process each country\n",
    "for country in countries:\n",
    "    # Filter data for the country\n",
    "    country_df = df[df['Work Location Country/Territory Name'] == country]\n",
    "    # Get unique BF Level 4 Names for the country\n",
    "    bf_level_4_names = country_df['BF Level 4 Name'].dropna().unique()\n",
    "\n",
    "    # Create a sheet for this country\n",
    "    for bf_level_4_name in bf_level_4_names:\n",
    "        # Filter data for the BF Level 4 Name\n",
    "        bf_df = country_df[country_df['BF Level 4 Name'] == bf_level_4_name]\n",
    "\n",
    "        # Sort data by Global Career Band (MD > 3 > 4 > ... > 8)\n",
    "        bf_df['Global Career Band'] = bf_df['Global Career Band'].astype(str)  # Ensure values are strings\n",
    "        sorted_bf_df = bf_df.sort_values('Global Career Band', key=lambda col: col.map(lambda x: 'MD' if x == 'MD' else int(x) if x.isdigit() else 999))\n",
    "\n",
    "        # Prepare data for three levels\n",
    "        rows = []\n",
    "        for _, level_1 in sorted_bf_df.iterrows():\n",
    "            level_1_details = [level_1['Employee Name'], level_1['Global Career Band'], level_1['Position Title']]\n",
    "            level_2_df = find_subordinates(df, level_1['Employee Name'])\n",
    "\n",
    "            if not level_2_df.empty:\n",
    "                first_level_1 = True  # To track if Level 1 details have been added\n",
    "                for _, level_2 in level_2_df.iterrows():\n",
    "                    level_2_details = [level_2['Employee Name'], level_2['Global Career Band'], level_2['Position Title']]\n",
    "                    level_3_df = find_subordinates(df, level_2['Employee Name'])\n",
    "\n",
    "                    if not level_3_df.empty:\n",
    "                        first_level_2 = True  # To track if Level 2 details have been added\n",
    "                        for _, level_3 in level_3_df.iterrows():\n",
    "                            level_3_details = [level_3['Employee Name'], level_3['Global Career Band'], level_3['Position Title']]\n",
    "                            rows.append((level_1_details if first_level_1 else [\"\", \"\", \"\"]) +\n",
    "                                        (level_2_details if first_level_2 else [\"\", \"\", \"\"]) +\n",
    "                                        level_3_details)\n",
    "                            first_level_1 = False\n",
    "                            first_level_2 = False\n",
    "                    else:\n",
    "                        rows.append((level_1_details if first_level_1 else [\"\", \"\", \"\"]) + level_2_details + [\"\", \"\", \"\"])\n",
    "                        first_level_1 = False\n",
    "            else:\n",
    "                rows.append(level_1_details + [\"\", \"\", \"\"] + [\"\", \"\", \"\"])\n",
    "\n",
    "        # Write data to the sheet\n",
    "        sheet_name = f\"{bf_level_4_name} - {country}\"\n",
    "        sheet_name = sheet_name[:31]  # Excel sheet names have a max length of 31\n",
    "        sheet = wb.create_sheet(title=sheet_name)\n",
    "        # Write headers\n",
    "        sheet.append([\"Level 1 Employee Name\", \"Global Career Band\", \"Position Title\",\n",
    "                      \"Level 2 Employee Name\", \"Global Career Band\", \"Position Title\",\n",
    "                      \"Level 3 Employee Name\", \"Global Career Band\", \"Position Title\"])\n",
    "        # Write rows\n",
    "        for row in rows:\n",
    "            sheet.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file)\n",
    "\n",
    "print(f\"Organization structure saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Big Grid Stack AOP YEAR  \\\n",
      "0     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "1     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "2     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "3     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "4     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "5     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "6     New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "7   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "8   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "9   Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "10  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "11  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "12  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "13  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "14  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "15  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "16  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "17  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "18  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "19  Forecast program/Other Saves-Saves Forecast Tr...     FY25   \n",
      "20    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "21    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "22    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "23    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "24    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "25    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "26    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "27    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "28    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "29    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "30    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "31    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "32    New Perm Position (within FRP) - Staff Drawdown     FY25   \n",
      "\n",
      "   Business Framework RTN Code Business Framework Business Framework Group  \\\n",
      "0                          RTN                                               \n",
      "1                          RTN                                               \n",
      "2                          RTN                                               \n",
      "3                          RTN                                               \n",
      "4                          RTN                                               \n",
      "5                          RTN                                               \n",
      "6                          RTN                                               \n",
      "7                          RTN                                               \n",
      "8                          RTN                                               \n",
      "9                          RTN                                               \n",
      "10                         RTN                                               \n",
      "11                         RTN                                               \n",
      "12                         RTN                                               \n",
      "13                         RTN                                               \n",
      "14                         RTN                                               \n",
      "15                         RTN                                               \n",
      "16                         RTN                                               \n",
      "17                         RTN                                               \n",
      "18                         RTN                                               \n",
      "19                         RTN                                               \n",
      "20                         RTN                                               \n",
      "21                         RTN                                               \n",
      "22                         RTN                                               \n",
      "23                         RTN                                               \n",
      "24                         RTN                                               \n",
      "25                         RTN                                               \n",
      "26                         RTN                                               \n",
      "27                         RTN                                               \n",
      "28                         RTN                                               \n",
      "29                         RTN                                               \n",
      "30                         RTN                                               \n",
      "31                         RTN                                               \n",
      "32                         RTN                                               \n",
      "\n",
      "     Country GCB Emp. Type Hiring Source  FTE Start Month WPB      GBM CMB  \\\n",
      "0   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "1   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "2   Country1   4       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "3   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "4   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "5   Country1   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "6   Country1   7       FTE      External  0.5  01/31/2025  0%  100.00%  0%   \n",
      "7   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "8   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "9   Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "10  Country2   4       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "11  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "12  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "13  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "14  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "15  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "16  Country2   6       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "17  Country2   7       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "18  Country2   8       FTE      External   -1  01/31/2025  0%  100.00%  0%   \n",
      "19  Country2  na       FTE      External -0.3  01/31/2025  0%  100.00%  0%   \n",
      "20  Country3   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "21  Country3  na       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "22  Country3  na       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "23  Country6  GM       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "24  Country6   3       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "25  Country6   3       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "26  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "27  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "28  Country6   6       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "29  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "30  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "31  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "32  Country6   7       FTE      External    1  01/31/2025  0%  100.00%  0%   \n",
      "\n",
      "   Diver   Entity Description  \n",
      "0         Entity1   Example 1  \n",
      "1         Entity1   Example 1  \n",
      "2         Entity1   Example 1  \n",
      "3         Entity1   Example 1  \n",
      "4         Entity1   Example 1  \n",
      "5         Entity1   Example 1  \n",
      "6         Entity1   Example 1  \n",
      "7         Entity2   Example 2  \n",
      "8         Entity2   Example 2  \n",
      "9         Entity2   Example 2  \n",
      "10        Entity2   Example 2  \n",
      "11        Entity2   Example 2  \n",
      "12        Entity2   Example 2  \n",
      "13        Entity2   Example 2  \n",
      "14        Entity2   Example 2  \n",
      "15        Entity2   Example 2  \n",
      "16        Entity2   Example 2  \n",
      "17        Entity2   Example 2  \n",
      "18        Entity2   Example 2  \n",
      "19        Entity2   Example 2  \n",
      "20        Entity3   Example 3  \n",
      "21        Entity3   Example 3  \n",
      "22        Entity3   Example 3  \n",
      "23        Entity6   Example 6  \n",
      "24        Entity6   Example 6  \n",
      "25        Entity6   Example 6  \n",
      "26        Entity6   Example 6  \n",
      "27        Entity6   Example 6  \n",
      "28        Entity6   Example 6  \n",
      "29        Entity6   Example 6  \n",
      "30        Entity6   Example 6  \n",
      "31        Entity6   Example 6  \n",
      "32        Entity6   Example 6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Example source data\n",
    "data = {\n",
    "    \"FTE\": [6.5, -12.3, 3, 0, None, 10],\n",
    "    \"Type\": [\"BC-TT\", \"Investments\", \"Saves\", \"Other\", \"Saves\", \"BC-TT\"],\n",
    "    \"GM\": [0, 0, 0, 0, 0, 1],\n",
    "    \"MD\": [0, 0, 0, 0, 0, 0],\n",
    "    \"3\": [0, 0, 0, 0, 0, 2],\n",
    "    \"4\": [2.5, -4, 0, 0, 0, 0],\n",
    "    \"5\": [0, 0, 0, 0, 0, 0],\n",
    "    \"6\": [3, -6, 0, 0, 0, 3],\n",
    "    \"7\": [1, -1, 1, 0, 0, 4],\n",
    "    \"8\": [0, -1.3, 0, 0, 0, 0],\n",
    "    \"Entity\": [\"Entity1\", \"Entity2\", \"Entity3\", \"Entity4\", \"Entity5\", \"Entity6\"],\n",
    "    \"Description\": [\"Example 1\", \"Example 2\", \"Example 3\", \"Example 4\", \"Example 5\", \"Example 6\"],\n",
    "    \"Country\": [\"Country1\", \"Country2\", \"Country3\", \"Country4\", \"Country5\", \"Country6\"],\n",
    "    \"RTN\": [\"RTN 123 - Example\", \"RTN 456 - Sample\", \"RTN 789 - Test\", \"RTN 101 - Case\", \"RTN 112 - Trial\", \"RTN 113 - Check\"]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "\n",
    "# Columns to check for GCB values\n",
    "gcb_columns = [\"GM\", \"MD\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# Clean and convert FTE column\n",
    "source_df[\"FTE\"] = pd.to_numeric(source_df[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "# Filter rows where FTE is valid and does not equal 0\n",
    "filtered_df = source_df[\n",
    "    source_df[\"FTE\"].notnull() & (source_df[\"FTE\"] != 0) & (source_df[\"Type\"].isin([\"BC-TT\", \"Investments\", \"Saves\"]))\n",
    "]\n",
    "\n",
    "# Create the template dataframe with required columns\n",
    "template_columns = [\n",
    "    \"Big Grid Stack\", \"AOP YEAR\", \"Business Framework RTN Code\",\n",
    "    \"Business Framework\", \"Business Framework Group\", \"Country\",\n",
    "    \"GCB\", \"Emp. Type\", \"Hiring Source\", \"FTE\", \"Start Month\",\n",
    "    \"WPB\", \"GBM\", \"CMB\", \"Diver\", \"Entity\", \"Description\"\n",
    "]\n",
    "template_df = pd.DataFrame(columns=template_columns)\n",
    "\n",
    "# Process each row in the filtered dataframe\n",
    "for _, row in filtered_df.iterrows():\n",
    "    fte_value = row[\"FTE\"]\n",
    "    duplicate_fte = -1 if fte_value < 0 else 1  # Determine positive or negative FTE\n",
    "    abs_fte = abs(fte_value)\n",
    "    int_part = math.floor(abs_fte)  # Integer part of FTE\n",
    "    fractional_part = abs_fte - int_part  # Fractional part of FTE\n",
    "\n",
    "    # Calculate GCB distribution\n",
    "    gcb_distribution = []\n",
    "    for col in gcb_columns:\n",
    "        value = pd.to_numeric(row[col], errors=\"coerce\")  # Convert to numeric, handle non-numeric gracefully\n",
    "        if not pd.isna(value) and value != 0:\n",
    "            gcb_distribution.extend([col] * abs(math.ceil(value)))\n",
    "\n",
    "    # Ensure GCB distribution matches the FTE rows\n",
    "    total_required_rows = int_part + (1 if fractional_part > 0 else 0)\n",
    "    if len(gcb_distribution) < total_required_rows:\n",
    "        gcb_distribution.extend([\"na\"] * (total_required_rows - len(gcb_distribution)))\n",
    "\n",
    "    # Dynamically calculate Start Month\n",
    "    current_year = 2025\n",
    "    if fte_value > 0:\n",
    "        start_month = datetime.date(current_year, 1, 31).strftime(\"%m/%d/%Y\")  # Example logic for positive FTE\n",
    "    elif fte_value < 0:\n",
    "        start_month = datetime.date(current_year, 2, 28).strftime(\"%m/%d/%Y\")  # Example logic for negative FTE\n",
    "    else:\n",
    "        start_month = datetime.date(current_year, 12, 31).strftime(\"%m/%d/%Y\")  # Default fallback\n",
    "\n",
    "    # Create rows in the template\n",
    "    for i in range(total_required_rows):\n",
    "        current_fte = (\n",
    "            fractional_part if i == total_required_rows - 1 and fractional_part > 0 else 1\n",
    "        ) * duplicate_fte\n",
    "\n",
    "        big_grid_stack = (\n",
    "            \"New Perm Position (within FRP) - Staff Drawdown\" if current_fte > 0\n",
    "            else \"Forecast program/Other Saves-Saves Forecast Tracker\"\n",
    "        )\n",
    "        if row[\"Type\"] == \"BC-TT\":\n",
    "            big_grid_stack = \"Inter Boundary Changes\"\n",
    "\n",
    "        new_row = {\n",
    "            \"Big Grid Stack\": big_grid_stack,\n",
    "            \"AOP YEAR\": \"FY25\",\n",
    "            \"Business Framework RTN Code\": row[\"RTN\"].split()[0],\n",
    "            \"Business Framework\": \"\",\n",
    "            \"Business Framework Group\": \"\",\n",
    "            \"Country\": row[\"Country\"],\n",
    "            \"GCB\": gcb_distribution[i],\n",
    "            \"Emp. Type\": \"FTE\",\n",
    "            \"Hiring Source\": \"External\",\n",
    "            \"FTE\": current_fte,\n",
    "            \"Start Month\": start_month,\n",
    "            \"WPB\": \"0%\",\n",
    "            \"GBM\": \"100.00%\",\n",
    "            \"CMB\": \"0%\",\n",
    "            \"Diver\": \"\",\n",
    "            \"Entity\": row[\"Entity\"],\n",
    "            \"Description\": row[\"Description\"],\n",
    "        }\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Output the result\n",
    "print(template_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_hdpi_file(input_file, month_name):\n",
    "    \"\"\"\n",
    "    Process the HDPI Excel file.\n",
    "    Modify this function to include your specific processing logic.\n",
    "    \"\"\"\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Perform your processing logic here\n",
    "    # Example: Add a new column for demonstration\n",
    "    df[\"Processed\"] = f\"Processed for {month_name}\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_files_in_folder(ifolder, ofolder):\n",
    "    \"\"\"\n",
    "    Processes all Excel files in the given folder.\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(ofolder, exist_ok=True)\n",
    "\n",
    "    # Get the list of files in the input folder\n",
    "    files = [f for f in os.listdir(ifolder) if f.endswith(\".xlsx\")]\n",
    "\n",
    "    for file_name in files:\n",
    "        try:\n",
    "            # Extract the file path\n",
    "            input_file_path = os.path.join(ifolder, file_name)\n",
    "\n",
    "            # Extract the month name (last three letters before \".xlsx\")\n",
    "            month_name = file_name.split()[-1][:3]\n",
    "\n",
    "            # Load and process the file\n",
    "            processed_df = process_hdpi_file(input_file_path, month_name)\n",
    "\n",
    "            # Save the processed file with the desired name in the output folder\n",
    "            output_file_name = f\"HDPI {month_name}.xlsx\"\n",
    "            output_file_path = os.path.join(ofolder, output_file_name)\n",
    "            processed_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Define input and output folder paths\n",
    "ifolder = r\"C:\\path\\to\\input\\folder\"  # Replace with your input folder path\n",
    "ofolder = r\"C:\\path\\to\\output\\folder\"  # Replace with your output folder path\n",
    "\n",
    "# Process all files in the folder\n",
    "process_files_in_folder(ifolder, ofolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6ebf19",
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, 'Microsoft Word', 'Command failed', 'wdmain11.chm', 36966, -2146824090), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-97dbbe50c67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdocx_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\\\Users\\\\KS\\\\Thesis.docx\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\\\Users\\\\KS\\\\Thesis_test.pdf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mconvert_docx_to_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocx_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-97dbbe50c67d>\u001b[0m in \u001b[0;36mconvert_docx_to_pdf\u001b[1;34m(docx_path, pdf_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Save as PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveAs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFileFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 17 corresponds to the PDF format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Close the document and quit Word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mSaveAs\u001b[1;34m(self, FileName, FileFormat, LockComments, Password, AddToRecentFiles, WritePassword, ReadOnlyRecommended, EmbedTrueTypeFonts, SaveNativePictureFormat, SaveFormsData, SaveAsAOCELetter, Encoding, InsertLineBreaks, AllowSubstitutions, LineEnding, AddBiDiMarks)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, 'Microsoft Word', 'Command failed', 'wdmain11.chm', 36966, -2146824090), None)"
     ]
    }
   ],
   "source": [
    "import win32com.client\n",
    "\n",
    "def convert_docx_to_pdf(docx_path, pdf_path):\n",
    "    # Initialize Word application\n",
    "    word = win32com.client.Dispatch(\"Word.Application\")\n",
    "    word.Visible = False  # Run in the background\n",
    "\n",
    "    # Open the .docx file\n",
    "    doc = word.Documents.Open(docx_path)\n",
    "\n",
    "    # Save as PDF\n",
    "    doc.SaveAs(pdf_path, FileFormat=17)  # 17 corresponds to the PDF format\n",
    "\n",
    "    # Close the document and quit Word\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "\n",
    "# Example usage\n",
    "docx_file = r\"C:\\\\Users\\\\KS\\\\Thesis.docx\"\n",
    "pdf_file = r\"C:\\\\Users\\\\KS\\\\Thesis_test.pdf\"\n",
    "convert_docx_to_pdf(docx_file, pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aa5cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Billing Contact Name  Billing Contact PS ID  Correct Billing Contact PS ID\n",
      "1                Alice                    102                            101\n",
      "3                Alice                    103                            101\n",
      "Filtered Duplicate PSID sheet with mismatched PS IDs has been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Sample data for spocdf and olddf (replace these with actual data loading as needed)\n",
    "spocdf = pd.DataFrame({\n",
    "    'Billing Contact Name': ['Alice', 'Alice', 'Bob', 'Alice', 'Charlie', 'Charlie'],\n",
    "    'Billing Contact PS ID': [101, 102, 201, 103, 301, 301]\n",
    "})\n",
    "\n",
    "olddf = pd.DataFrame({\n",
    "    'Billing Contact Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Billing Contact PS ID': [101, 201, 301]  # Assuming these are the correct PS IDs\n",
    "})\n",
    "\n",
    "# Step 1: Merge spocdf with olddf to add the correct PS ID as a reference\n",
    "merged_df = spocdf.merge(\n",
    "    olddf,\n",
    "    on='Billing Contact Name',\n",
    "    how='left',\n",
    "    suffixes=('', '_correct')\n",
    ")\n",
    "\n",
    "# Step 2: Filter for rows where the 'Billing Contact PS ID' does not match the 'Correct Billing Contact PS ID'\n",
    "# Retain only records where there is a mismatch\n",
    "mismatched_psid_df = merged_df[merged_df['Billing Contact PS ID'] != merged_df['Billing Contact PS ID_correct']].copy()\n",
    "\n",
    "# Rename the correct column for clarity\n",
    "mismatched_psid_df.rename(columns={'Billing Contact PS ID_correct': 'Correct Billing Contact PS ID'}, inplace=True)\n",
    "\n",
    "# # Step 3: Write the filtered DataFrame with mismatched PS IDs to a new sheet in the workbook\n",
    "# file_path = 'your_workbook.xlsx'  # Replace with your actual file path\n",
    "# workbook = load_workbook(file_path)\n",
    "\n",
    "# # Check if 'Duplicate PSID' sheet exists, and delete if it does\n",
    "# if 'Duplicate PSID' in workbook.sheetnames:\n",
    "#     del workbook['Duplicate PSID']\n",
    "# workbook.save(file_path)  # Save after deleting to ensure it's applied\n",
    "\n",
    "# # Append the new data to the workbook in 'Duplicate PSID'\n",
    "# with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "#     mismatched_psid_df.to_excel(writer, sheet_name='Duplicate PSID', index=False)\n",
    "print(mismatched_psid_df)\n",
    "print(\"Filtered Duplicate PSID sheet with mismatched PS IDs has been created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "685ec183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A2    C             SC\n",
      "0  101  new  lookup_value1\n",
      "1  102  old         value2\n",
      "2  103  new  lookup_value3\n",
      "3  104  old         value4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A2': [101, 102, 103, 104],\n",
    "    'C': ['new', 'old', 'new', 'old'],\n",
    "    'SC': [None, 'value2', None, 'value4']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'AA2': [101, 102, 103, 104],\n",
    "    'SC': ['lookup_value1', 'lookup_value2', 'lookup_value3', 'lookup_value4']\n",
    "})\n",
    "\n",
    "# Step 1: Create a mask for rows where 'C' is 'new' and 'SC' is blank (None or NaN)\n",
    "mask = (df1['C'] == 'new') & (df1['SC'].isna())\n",
    "\n",
    "# Step 2: Create a lookup dictionary from df2 for easy access\n",
    "lookup_dict = df2.set_index('AA2')['SC'].to_dict()\n",
    "\n",
    "# Step 3: Use np.where() to update 'SC' column conditionally\n",
    "df1['SC'] = np.where(\n",
    "    mask,  # Only update where the condition is True\n",
    "    df1['A2'].map(lookup_dict).fillna(df1['SC']),  # Map A2 to the lookup dict and fill NaN with existing SC\n",
    "    df1['SC']  # Retain the existing SC values\n",
    ")\n",
    "\n",
    "# Output the updated df1\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60536d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A   C    E     D\n",
      "0  101  X1  Y11  None\n",
      "1  102  X2   Y2  None\n",
      "2  103  X3   Y1  None\n",
      "3  104  X4   Y4  None\n",
      "     A   C    E LookupValue\n",
      "0  101  X1  Y11        Val1\n",
      "1  103  X3   Y2        Val3\n",
      "2  105  X5   Y4        Val5\n",
      "3  106  X6   Y1        Val6\n",
      "     A   C    E     D\n",
      "0  101  X1  Y11  Val1\n",
      "1  102  X2   Y2  Val3\n",
      "2  103  X3   Y1  Val3\n",
      "3  104  X4   Y4  Val5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame 1 (df1)\n",
    "data1 = {'A': [101, 102, 103, 104],\n",
    "         'C': ['X1', 'X2', 'X3', 'X4'],\n",
    "         'E': ['Y11', 'Y2', 'Y1', 'Y4'],\n",
    "         'D': [None, None, None, None]}  # Initially empty Column D\n",
    "df1 = pd.DataFrame(data1)\n",
    "print(df1)\n",
    "# Sample DataFrame 2 (df2)\n",
    "data2 = {'A': [101, 103, 105, 106],\n",
    "         'C': ['X1', 'X3', 'X5', 'X6'],\n",
    "         'E': ['Y11', 'Y2', 'Y4', 'Y1'],\n",
    "         'LookupValue': ['Val1', 'Val3', 'Val5', 'Val6']}  # Column with values to fetch\n",
    "df2 = pd.DataFrame(data2)\n",
    "print(df2)\n",
    "\n",
    "# Step 1: First Lookup - Based on Column 'C'\n",
    "df1 = pd.merge(df1, df2[['C', 'LookupValue']], how='left', on='C')  # Merge on 'C'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Use `mask` to fill NaN in 'D' only\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Step 2: Second Lookup - Based on Column 'E'\n",
    "df1 = pd.merge(df1, df2[['E', 'LookupValue']], how='left', on='E')  # Merge on 'E'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Update only NaNs in 'D'\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Step 3: Third Lookup - Based on Column 'A'\n",
    "df1 = pd.merge(df1, df2[['A', 'LookupValue']], how='left', on='A')  # Merge on 'A'\n",
    "df1['D'] = df1['D'].mask(df1['D'].isna(), df1['LookupValue'])  # Update only remaining NaNs in 'D'\n",
    "df1.drop(columns=['LookupValue'], inplace=True)  # Drop temporary column\n",
    "\n",
    "# Final Output\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c87da5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jan_2024_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-14917c0930e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Call the function to update the DataFrame based on the presence in each monthly file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mcheck_employee_presence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# Output the final DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-14917c0930e9>\u001b[0m in \u001b[0;36mcheck_employee_presence\u001b[1;34m(df, month_files)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmonth_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Read the month file (assuming it's a CSV file or similar, replace with correct method if different format)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmonth_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming each file contains 'Employee ID' column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Get list of employee IDs present in this month's file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirti\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jan_2024_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to generate columns for all months of the year\n",
    "def generate_monthly_columns(year_suffix, num_years=1):\n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    columns = []\n",
    "    \n",
    "    for i in range(num_years):\n",
    "        for month in months:\n",
    "            columns.append(f\"{month} {year_suffix + i}\")\n",
    "    \n",
    "    return columns\n",
    "\n",
    "# Function to update dataframe with monthly data from files\n",
    "def update_monthly_data(df, folder_path, year_suffix):\n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    current_month = datetime.now().month  # Get the current month (1-12)\n",
    "    \n",
    "    for month_idx in range(current_month):\n",
    "        month_name = months[month_idx]  # Get month name\n",
    "        col_name = f\"{month_name} {year_suffix}\"  # Column name to update\n",
    "        \n",
    "        # Read corresponding file for the current month if exists\n",
    "        file_path = os.path.join(folder_path, f\"{month_name}_{year_suffix}.csv\")  # Assuming CSV format\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            monthly_data = pd.read_csv(file_path)  # Adjust if files are in Excel\n",
    "            \n",
    "            # Assuming you have a common column like 'Position ID' to merge data\n",
    "            if 'Position ID' in monthly_data.columns:\n",
    "                # Merge or update existing data based on 'Position ID'\n",
    "                df = df.merge(monthly_data[['Position ID', 'Value']], on='Position ID', how='left')\n",
    "                \n",
    "                # Assign merged 'Value' to the corresponding column (e.g., \"Jan 24\")\n",
    "                df[col_name] = df['Value']\n",
    "                df = df.drop(columns=['Value'])  # Remove the temporary 'Value' column\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 1: Generate columns for one year\n",
    "columns_2024 = generate_monthly_columns(24)\n",
    "\n",
    "# Step 2: Create an empty dataframe with 'Position ID' and the generated columns\n",
    "df = pd.DataFrame(columns=['Position ID'] + columns_2024)\n",
    "\n",
    "# Step 3: Simulate loading data from previous step (e.g., from GHA)\n",
    "# Assuming 'Position ID' column is already there\n",
    "# df = pd.read_csv('previous_step_data.csv')  # Or however you load it\n",
    "\n",
    "# Step 4: Update DataFrame with monthly data up to the current month\n",
    "folder_path = 'path_to_your_files'  # Specify the folder where monthly files are stored\n",
    "df = update_monthly_data(df, folder_path, 24)\n",
    "\n",
    "# Step 5: Save the final updated DataFrame to Excel or CSV\n",
    "df.to_csv('final_output.csv', index=False)  # Adjust for Excel if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8013ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved with drop-down in column 'Gender'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [24, 30, 22],\n",
    "        'Gender': ['Female', 'Male', 'Male']}  # We will apply drop-down for Gender\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to an Excel file\n",
    "output_path = 'output_with_dropdown.xlsx'\n",
    "\n",
    "# Create a workbook and add the dataframe to the sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Write the DataFrame to the worksheet\n",
    "for row in dataframe_to_rows(df, index=False, header=True):\n",
    "    ws.append(row)\n",
    "\n",
    "# Define the list of allowed values for the drop-down\n",
    "drop_down_list = ['Male', 'Female', 'Other']\n",
    "\n",
    "# Create a DataValidation object for the drop-down list\n",
    "dv = DataValidation(type=\"list\", formula1=f'\"{\",\".join(drop_down_list)}\"', showDropDown=False)\n",
    "\n",
    "# Add the data validation to the 'Gender' column (assuming it is in column C, starting from C2)\n",
    "ws.add_data_validation(dv)\n",
    "dv.add(f\"C2:C{len(df) + 1}\")  # Apply validation to the Gender column for all rows\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"Excel file saved with drop-down in column 'Gender'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541cec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved with a drop-down in the 'Gender' column for all rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [24, 30, 22],\n",
    "        'Gender': ['Female', 'Male', '']}  # Leave empty so users can select from the dropdown\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a workbook and add the dataframe to the sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Write the DataFrame to the worksheet\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), 1):\n",
    "    ws.append(row)\n",
    "\n",
    "# Define the list of allowed values for the drop-down\n",
    "drop_down_list = ['Male', 'Female', 'Other']\n",
    "\n",
    "# Create a DataValidation object for the drop-down list\n",
    "dv = DataValidation(type=\"list\", formula1=f'\"{\",\".join(drop_down_list)}\"', showDropDown=False)\n",
    "\n",
    "# Optional: Add an input prompt when the user selects the cell\n",
    "dv.prompt = \"Select Gender from the list\"\n",
    "dv.promptTitle = \"Gender Selection\"\n",
    "\n",
    "# Apply the data validation to the \"Gender\" column (assuming it's column C)\n",
    "ws.add_data_validation(dv)\n",
    "\n",
    "# Extend the drop-down for all rows in the \"Gender\" column (starting from C2)\n",
    "max_row = len(df) + 100  # Adjust the number of rows if needed\n",
    "dv.add(f\"C2:C{max_row}\")  # Apply the drop-down from C2 to C{max_row}\n",
    "\n",
    "# Save the workbook\n",
    "output_path = 'output_with_dropdown_visible.xlsx'\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"Excel file saved with a drop-down in the 'Gender' column for all rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb279e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0448e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Business Framework Country R3  GCB\n",
      "0          Global CMB         UK    0\n",
      "1          Global CMB         UK    1\n",
      "2          Global CMB         UK    2\n",
      "3          Global CMB         UK    3\n",
      "4          Global CMB         UK    4\n",
      "5          Global CMB         UK    5\n",
      "6          Global CMB         UK    6\n",
      "7          Global CMB         UK    7\n",
      "8     Other Framework         US    0\n",
      "9     Other Framework         US    1\n",
      "10    Other Framework         US    2\n",
      "11    Other Framework         US    3\n",
      "12    Other Framework         US    4\n",
      "13    Other Framework         US    5\n",
      "14    Other Framework         US    6\n",
      "15    Other Framework         US    7\n",
      "16                wpb         IN    0\n",
      "17                wpb         IN    1\n",
      "18                wpb         IN    2\n",
      "19                wpb         IN    3\n",
      "20                wpb         IN    4\n",
      "21                wpb         IN    5\n",
      "22                wpb         IN    6\n",
      "23                wpb         IN    7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is in a CSV file or an Excel file\n",
    "# Replace 'your_file.xlsx' with the path to your Excel file if you're reading from one\n",
    "# df = pd.read_excel('your_file.xlsx')\n",
    "\n",
    "# Sample DataFrame to simulate the input\n",
    "data = {\n",
    "    'Business Framework': ['Global CMB', 'Other Framework','wpb'],  # Sample values\n",
    "    'Country R3': ['UK', 'US','IN'],  # Sample values\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare an empty list to collect the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Repeat each row 8 times with GCB values from 0 to 7\n",
    "    for gcb_value in range(8):\n",
    "        # Append the row values with the new GCB value to the list\n",
    "        expanded_rows.append({\n",
    "            'Business Framework': row['Business Framework'],\n",
    "            'Country R3': row['Country R3'],\n",
    "            'GCB': gcb_value\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file or CSV\n",
    "# expanded_df.to_excel('expanded_output.xlsx', index=False)\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Edge WebDriver using basic options\n",
    "options = webdriver.EdgeOptions()\n",
    "\n",
    "# Attempt to launch Edge without specifying the Service\n",
    "driver = webdriver.Edge(options=options)\n",
    "\n",
    "# Step 2: Open a blank tab to avoid the slow homepage\n",
    "driver.execute_script(\"window.open('');\")\n",
    "time.sleep(1)  # Wait for the new tab to open\n",
    "\n",
    "# Step 3: Switch to the new tab\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "# Step 4: Navigate to the target URL in the new tab\n",
    "target_url = 'https://your-target-website.com'  # Replace with your actual URL\n",
    "driver.get(target_url)\n",
    "\n",
    "# Step 5: Close the default homepage tab (which is the first tab)\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.close()\n",
    "\n",
    "# Step 6: Switch back to the target website tab\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# Step 7: Let the browser stay open for some time\n",
    "time.sleep(10)\n",
    "\n",
    "# Finally, close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da873fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Path to your Microsoft Edge WebDriver (Make sure it matches your browser version)\n",
    "edge_driver_path = \"path/to/msedgedriver.exe\"\n",
    "\n",
    "# Set up Edge WebDriver\n",
    "service = Service(edge_driver_path)\n",
    "driver = webdriver.Edge(service=service)\n",
    "\n",
    "# Open the URL\n",
    "driver.get('https://example.com')  # Replace with your actual URL\n",
    "\n",
    "# Wait for the page to load and for the SSO button to be clickable\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'My organization's single sign-on')]\"))\n",
    ")\n",
    "\n",
    "# Locate and click the \"SSO\" button\n",
    "sso_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'My organization's single sign-on')]\")\n",
    "sso_button.click()\n",
    "\n",
    "# Let the SSO process complete\n",
    "# You can wait for the page to load after the login\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"some_element_after_login\"))  # Replace with an element that appears after login\n",
    ")\n",
    "\n",
    "# You can now proceed to interact with the logged-in page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35379e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def options_to_dict(options):\n",
    "    options_dict = {}\n",
    "    for key in dir(options):\n",
    "        if not key.startswith('__'):\n",
    "            options_dict[key] = getattr(options, key)\n",
    "    return options_dict\n",
    "\n",
    "edge_options = Options()\n",
    "edge_options.use_chromium = True\n",
    "\n",
    "options_dict = options_to_dict(edge_options)\n",
    "options_json = json.dumps(options_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaade20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the browser and SSO settings\n",
    "driver = webdriver.Chrome()  # Replace with your default browser\n",
    "\n",
    "# Open the website and log in using default SSO\n",
    "driver.get(\"https://example.com\")  # Replace with the website URL\n",
    "\n",
    "# Wait for the login process to complete\n",
    "WebDriverWait(driver, 10).until(EC.title_contains(\"Logged in\"))\n",
    "\n",
    "# Navigate to the page with filters\n",
    "driver.get(\"https://example.com/filters\")  # Replace with the filter page URL\n",
    "\n",
    "# Identify and interact with the filter elements\n",
    "filter_elements = driver.find_elements_by_css_selector(\".filter-option\")\n",
    "for element in filter_elements:\n",
    "    # Perform the necessary actions to change the filter options\n",
    "    element.click()  # Replace with the actual action required\n",
    "\n",
    "# Click the ellipsis button to download the file\n",
    "download_button = driver.find_element_by_css_selector(\".download-button\")\n",
    "download_button.click()\n",
    "\n",
    "# Specify the file path and name for the downloaded file\n",
    "file_path = \"/path/to/downloaded/file.csv\"  # Replace with the desired file path and name\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8788a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome(service=Service('/path/to/chromedriver'))\n",
    "\n",
    "# Open the website\n",
    "driver.get('https://example.com')  # Replace with the actual URL\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the filter element by its label text (XPath)\n",
    "filter_element = driver.find_element(By.XPATH, \"//em[text()='BF Level 1 Name']\")\n",
    "\n",
    "# Click on the filter element\n",
    "filter_element.click()\n",
    "\n",
    "# Additional interaction (if needed)\n",
    "\n",
    "# Close the browser after operations\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to map 'Mxx' to actual periods\n",
    "def map_month_to_period(month_str, year):\n",
    "    months_map = {\n",
    "        \"M01\": \"Jan\", \"M02\": \"Feb\", \"M03\": \"Mar\", \"M04\": \"Apr\", \"M05\": \"May\", \n",
    "        \"M06\": \"Jun\", \"M07\": \"Jul\", \"M08\": \"Aug\", \"M09\": \"Sep\", \"M10\": \"Oct\", \n",
    "        \"M11\": \"Nov\", \"M12\": \"Dec\"\n",
    "    }\n",
    "    month_abbr = months_map.get(month_str)\n",
    "    return f\"{month_abbr}-{year}\"\n",
    "\n",
    "# Process HC sheet and populate the 'Paste' sheet\n",
    "def process_hc_sheet(df, paste_df, sheet_name, year):\n",
    "    # Find the last 3 columns with 'Mxx' values for the months\n",
    "    month_cols = [col for col in df.columns if 'M' in col]\n",
    "    if len(month_cols) < 3:\n",
    "        print(f\"Not enough 'Mxx' columns found in {sheet_name}\")\n",
    "        return\n",
    "\n",
    "    # Sort the columns to get the last three\n",
    "    last_3_months = month_cols[-3:]\n",
    "    \n",
    "    for month_col in last_3_months:\n",
    "        month_num = month_col[:3]  # Extract Mxx (e.g., M07)\n",
    "        period = map_month_to_period(month_num, year)\n",
    "        \n",
    "        # Prepare the 'Paste' DataFrame row by row\n",
    "        for _, row in df.iterrows():\n",
    "            paste_row = {\n",
    "                'File Name': sheet_name,\n",
    "                'Entity': row['Entity'],\n",
    "                'Function': row['RTN Level 4'],\n",
    "                'FTE/Contractor': row[month_col],  # Value from the month column\n",
    "                'Period': period,\n",
    "                'Attribute Type': 'MoM',\n",
    "            }\n",
    "            # Append to 'Paste' DataFrame\n",
    "            paste_df = paste_df.append(paste_row, ignore_index=True)\n",
    "    \n",
    "    return paste_df\n",
    "\n",
    "# Main script to process the sheets\n",
    "def consolidate_hc_sheets(sheets_dict, year):\n",
    "    # Initialize the 'Paste' DataFrame\n",
    "    paste_columns = [\n",
    "        'File Name','Level 3','Level 4','Cost Grouping','Cost Type','Finance Region', \n",
    "        'Attribute Type','Period','Cost','FTE/Contractor','Country','Level 3.5','Level 4.5',\n",
    "        'Entity','Function','Mapped Country','MICA'\n",
    "    ]\n",
    "    paste_df = pd.DataFrame(columns=paste_columns)\n",
    "\n",
    "    # Iterate through the sheets\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        if 'HC' in sheet_name:\n",
    "            paste_df = process_hc_sheet(df, paste_df, sheet_name, year)\n",
    "\n",
    "    return paste_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'sheets_dict' contains all loaded sheets as DataFrames\n",
    "sheets_dict = {\n",
    "    'HC_Sheet1': pd.DataFrame(...),  # Replace with actual loaded sheet data\n",
    "    'HC_Sheet2': pd.DataFrame(...),\n",
    "    # Add more sheets as needed\n",
    "}\n",
    "\n",
    "# Call the function to process and consolidate HC sheets\n",
    "final_paste_df = consolidate_hc_sheets(sheets_dict, 2024)\n",
    "print(final_paste_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67995bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyxlsb import open_workbook\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Path to the xlsb file (wb2)\n",
    "wb2_path = 'path_to_wb2.xlsb'\n",
    "\n",
    "# New Excel workbook to store the sheets that are not excluded\n",
    "new_wb = Workbook()\n",
    "new_wb.remove(new_wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find the header row based on 'Entity', 'RTN Level 4', 'Account'\n",
    "def find_header_row(sheet):\n",
    "    header_row = None\n",
    "    # Iterate through rows to find the correct header row\n",
    "    for row_num, row in enumerate(sheet.rows()):\n",
    "        values = [item.v for item in row]\n",
    "        if len(values) >= 3 and values[0] == 'Entity' and values[1] == 'RTN Level 4' and values[2] == 'Account':\n",
    "            header_row = row_num  # Store the correct header row index (0-based)\n",
    "            break\n",
    "    return header_row\n",
    "\n",
    "# Function to process a sheet and clean it up\n",
    "def process_sheet(sheet, sheet_name):\n",
    "    data = []\n",
    "    header_row = find_header_row(sheet)\n",
    "\n",
    "    if header_row is None:\n",
    "        print(f\"Header row not found in sheet: {sheet_name}\")\n",
    "        return None\n",
    "\n",
    "    # Read the sheet into a list of lists (rows of data)\n",
    "    for row in sheet.rows():\n",
    "        data.append([item.v for item in row])\n",
    "\n",
    "    # Convert data to a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check the values of E15 and F15 (columns with index 4 and 5)\n",
    "    try:\n",
    "        E15_val = str(df.iloc[header_row + 1, 4])  # Value in column E (0-indexed, so 4)\n",
    "        F15_val = str(df.iloc[header_row + 1, 5])  # Value in column F (0-indexed, so 5)\n",
    "        print(f\"Debug: E15 value: {E15_val}, F15 value: {F15_val}\")\n",
    "    except IndexError:\n",
    "        E15_val = ''\n",
    "        F15_val = ''\n",
    "        print(f\"Debug: E15 or F15 index out of range\")\n",
    "\n",
    "    # Define possible month headers\n",
    "    month_headers = ['M{:02d}'.format(i) for i in range(1, 13)]  # Generates 'M01' to 'M12'\n",
    "    print(f\"Debug: Month headers: {month_headers}\")\n",
    "\n",
    "    # Prepare headers for columns A to C from row 15\n",
    "    headers_A_to_C = df.iloc[header_row + 1, :3].values  # Columns A to C headers from row 15\n",
    "    print(f\"Debug: Headers A to C: {headers_A_to_C}\")\n",
    "\n",
    "    if E15_val in month_headers and F15_val in month_headers:\n",
    "        # Use row 15 for headers for all columns\n",
    "        headers_D_onwards = df.iloc[header_row + 1, 3:].values  # Columns D onwards headers from row 15\n",
    "        headers = list(headers_A_to_C) + list(headers_D_onwards)\n",
    "        start_data_row = header_row + 2  # Data starts after the header row\n",
    "        print(\"Debug: Using headers from row 15 for columns D onwards\")\n",
    "    else:\n",
    "        # Use row 14 for columns D onwards\n",
    "        headers_D_onwards = df.iloc[header_row, 3:].values  # Columns D onwards headers from row 14\n",
    "        headers = list(headers_A_to_C) + list(headers_D_onwards)\n",
    "        start_data_row = header_row + 1  # Data starts after row 14\n",
    "        print(\"Debug: Using headers from row 14 for columns D onwards\")\n",
    "\n",
    "    # Set the correct headers for the dataframe\n",
    "    df = df.iloc[start_data_row:].reset_index(drop=True)  # Keep only rows after the header\n",
    "    df.columns = headers\n",
    "\n",
    "    # Drop rows where all values are NaN or None\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Open the xlsb workbook and process valid sheets\n",
    "with open_workbook(wb2_path) as wb2:\n",
    "    for sheetname in wb2.sheets:\n",
    "        # Exclude sheets with 'excl', 'Pivot', or 'TM1'\n",
    "        if any(exclude_word.lower() in sheetname.lower() for exclude_word in ['excl', 'pivot', 'tm1']):\n",
    "            print(f\"Skipping sheet: {sheetname}\")\n",
    "            continue\n",
    "\n",
    "        with wb2.get_sheet(sheetname) as sheet:\n",
    "            print(f\"Processing sheet: {sheetname}\")\n",
    "            df = process_sheet(sheet, sheetname)\n",
    "\n",
    "            if df is not None:\n",
    "                # Save DataFrame to a new sheet in the new workbook\n",
    "                new_sheet = new_wb.create_sheet(title=sheetname[:31])  # Sheet names max length is 31 characters\n",
    "\n",
    "                # Write the headers to the sheet\n",
    "                new_sheet.append(df.columns.tolist())\n",
    "\n",
    "                # Write the data rows to the sheet\n",
    "                for row in df.itertuples(index=False, name=None):\n",
    "                    new_sheet.append(row)\n",
    "\n",
    "# Save the new workbook with the non-excluded sheets\n",
    "output_path = 'non_excluded_sheets_with_headers.xlsx'\n",
    "new_wb.save(output_path)\n",
    "print(f\"Workbook with non-excluded sheets and correct headers has been saved to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f78f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyxlsb import open_workbook\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Path to the xlsb file (wb2)\n",
    "wb2_path = 'path_to_wb2.xlsb'\n",
    "\n",
    "# New Excel workbook to store the results\n",
    "new_wb = Workbook()\n",
    "new_wb.remove(new_wb.active)  # Remove the default sheet\n",
    "\n",
    "# Function to find the header row based on 'Entity', 'RTN Level 4', 'Account'\n",
    "def find_header_row(sheet):\n",
    "    header_row = None\n",
    "    # Iterate through rows to find the correct header row\n",
    "    for row_num, row in enumerate(sheet.rows()):\n",
    "        values = [item.v for item in row]\n",
    "        if len(values) >= 3 and values[0] == 'Entity' and values[1] == 'RTN Level 4' and values[2] == 'Account':\n",
    "            header_row = row_num  # Store the correct header row index (0-based)\n",
    "            break\n",
    "    return header_row\n",
    "\n",
    "# Function to process a sheet and clean it up\n",
    "def process_sheet(sheet, sheet_name):\n",
    "    data = []\n",
    "    header_row = find_header_row(sheet)\n",
    "\n",
    "    if header_row is None:\n",
    "        print(f\"Header row not found in sheet: {sheet_name}\")\n",
    "        return None\n",
    "\n",
    "    # Read the sheet into a list of lists (rows of data)\n",
    "    for row in sheet.rows():\n",
    "        data.append([item.v for item in row])\n",
    "\n",
    "    # Convert data to a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check the values of E15 and F15 (row 15 is 0-based index 14 in pandas)\n",
    "    E15_val = str(df.iloc[header_row + 1, 4])  # Value in column E (0-indexed, so 4)\n",
    "    F15_val = str(df.iloc[header_row + 1, 5])  # Value in column F (0-indexed, so 5)\n",
    "\n",
    "    # Define possible month headers\n",
    "    month_headers = ['M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10', 'M11', 'M12']\n",
    "\n",
    "    # Adjust headers for columns D onwards based on the content of columns E and F\n",
    "    if E15_val in month_headers and F15_val in month_headers:\n",
    "        # Use row 15 for headers (index 14 in pandas)\n",
    "        headers = df.iloc[header_row + 1].values  # Take headers from row 15\n",
    "        start_data_row = header_row + 2  # Data starts after row 15\n",
    "    else:\n",
    "        # Use row 14 for headers (index 13 in pandas)\n",
    "        headers = df.iloc[header_row].values  # Take headers from row 14\n",
    "        start_data_row = header_row + 1  # Data starts after row 14\n",
    "\n",
    "    # Set the correct headers for the dataframe\n",
    "    df.columns = headers\n",
    "    df = df[start_data_row:].reset_index(drop=True)  # Keep only rows after the header\n",
    "\n",
    "    # Drop rows where all values are NaN or None\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Open the xlsb workbook and process valid sheets\n",
    "with open_workbook(wb2_path) as wb2:\n",
    "    for sheetname in wb2.sheets:\n",
    "        # Exclude sheets with 'excl', 'Pivot', or 'TM1'\n",
    "        if any(exclude_word.lower() in sheetname.lower() for exclude_word in ['excl', 'Pivot', 'TM1']):\n",
    "            print(f\"Skipping sheet: {sheetname}\")\n",
    "            continue\n",
    "\n",
    "        with wb2.get_sheet(sheetname) as sheet:\n",
    "            print(f\"Processing sheet: {sheetname}\")\n",
    "            df = process_sheet(sheet, sheetname)\n",
    "            \n",
    "            if df is not None:\n",
    "                # Save DataFrame to a new sheet in the new workbook\n",
    "                new_sheet = new_wb.create_sheet(title=sheetname)\n",
    "                for r in df.itertuples(index=False, name=None):\n",
    "                    new_sheet.append(r)\n",
    "\n",
    "# Save the new workbook with the updated data\n",
    "output_path = 'updated_wb.xlsx'\n",
    "new_wb.save(output_path)\n",
    "print(f\"Data has been written to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc2a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/KS/data.xlsx None None None None None C:/Users/KS/final_output.xlsx None\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "class ExcelSheetSelector:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Excel Sheet Selector\")\n",
    "\n",
    "        # Custom labels for each file selection\n",
    "        self.custom_labels = [\n",
    "            \"Select OFile:\", \"Select CFile:\", \"Select DFile:\", \"Select Fourth File:\",\n",
    "            \"Select Fifth File:\", \"Select Sixth File:\", \"Select Seventh File:\", \"Select Eighth File:\"\n",
    "        ]\n",
    "\n",
    "        # List to hold selected file paths, initially None\n",
    "        self.selected_files = [None] * 8\n",
    "        self.labels = []\n",
    "\n",
    "        # Create custom labels and buttons for selecting files\n",
    "        for i in range(8):\n",
    "            # Create a custom label for each file selection\n",
    "            custom_label = tk.Label(root, text=self.custom_labels[i], width=20, anchor='w')\n",
    "            custom_label.grid(row=i, column=0, padx=10, pady=5)\n",
    "\n",
    "            # Create a label to display the selected file path\n",
    "            label = tk.Label(root, text=\"Not selected\", width=60, anchor='w')\n",
    "            label.grid(row=i, column=1, padx=10, pady=5)\n",
    "            self.labels.append(label)\n",
    "\n",
    "            # Create a button for each file selection\n",
    "            button = tk.Button(root, text=\"Browse\", command=lambda index=i: self.select_file(index))\n",
    "            button.grid(row=i, column=2, padx=10, pady=5)\n",
    "\n",
    "        # Button to confirm file selections\n",
    "        confirm_button = tk.Button(root, text=\"Confirm Selection\", command=self.confirm_selection)\n",
    "        confirm_button.grid(row=8, column=0, columnspan=3, pady=20)\n",
    "\n",
    "        # Predefined variable names for file paths\n",
    "        self.OFile = None\n",
    "        self.CFile = None\n",
    "        self.DFile = None\n",
    "        self.file4 = None\n",
    "        self.file5 = None\n",
    "        self.file6 = None\n",
    "        self.file7 = None\n",
    "        self.file8 = None\n",
    "\n",
    "    def select_file(self, index):\n",
    "        # Open file dialog to select a file\n",
    "        file_path = filedialog.askopenfilename(title=\"Select a File\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "        \n",
    "        if file_path:  # If a file is selected\n",
    "            self.selected_files[index] = file_path  # Save the selected file\n",
    "            self.labels[index].config(text=file_path)  # Update label with the file path\n",
    "\n",
    "    def confirm_selection(self):\n",
    "        # Check how many files have been selected\n",
    "        num_selected = sum(1 for file in self.selected_files if file)\n",
    "\n",
    "        if num_selected == 0:\n",
    "            messagebox.showwarning(\"Warning\", \"No files selected.\")\n",
    "        else:\n",
    "            # Show confirmation dialog with options to Confirm or Select More Files\n",
    "            result = messagebox.askquestion(\n",
    "                \"Confirmation\", \n",
    "                f\"You have selected {num_selected} files. Do you want to proceed?\", \n",
    "                icon='question'\n",
    "            )\n",
    "            \n",
    "            if result == 'yes':\n",
    "                # Directly assign file paths to predefined variables\n",
    "                self.assign_file_paths()\n",
    "                messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "                # Print assigned files for verification\n",
    "                print(self.OFile, self.CFile, self.DFile, self.file4, self.file5, self.file6, self.file7, self.file8)\n",
    "                # Close the GUI\n",
    "                self.root.destroy()\n",
    "            else:\n",
    "                # Allow the user to select more files\n",
    "                messagebox.showinfo(\"Select More\", \"Please select more files.\")\n",
    "\n",
    "    def assign_file_paths(self):\n",
    "        \"\"\"Assigns selected file paths to predefined variables.\"\"\"\n",
    "        self.OFile = self.selected_files[0] if len(self.selected_files) > 0 else None\n",
    "        self.CFile = self.selected_files[1] if len(self.selected_files) > 1 else None\n",
    "        self.DFile = self.selected_files[2] if len(self.selected_files) > 2 else None\n",
    "        self.file4 = self.selected_files[3] if len(self.selected_files) > 3 else None\n",
    "        self.file5 = self.selected_files[4] if len(self.selected_files) > 4 else None\n",
    "        self.file6 = self.selected_files[5] if len(self.selected_files) > 5 else None\n",
    "        self.file7 = self.selected_files[6] if len(self.selected_files) > 6 else None\n",
    "        self.file8 = self.selected_files[7] if len(self.selected_files) > 7 else None\n",
    "\n",
    "# Initialize the Tkinter window and ExcelSheetSelector class\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ExcelSheetSelector(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36d374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def select_file(index):\n",
    "    # Open file dialog to select a file\n",
    "    file_path = filedialog.askopenfilename(title=\"Select a File\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "    \n",
    "    if file_path:  # If a file is selected\n",
    "        selected_files[index] = file_path  # Save the selected file\n",
    "        labels[index].config(text=file_path)  # Update label with the file path\n",
    "\n",
    "def confirm_selection():\n",
    "    # Check how many files have been selected\n",
    "    num_selected = sum(1 for file in selected_files if file)\n",
    "    \n",
    "    if num_selected == 0:\n",
    "        messagebox.showwarning(\"Warning\", \"No files selected.\")\n",
    "    else:\n",
    "        # Show confirmation dialog with options to Confirm or Select More Files\n",
    "        result = messagebox.askquestion(\"Confirmation\", f\"You have selected {num_selected} files. Do you want to proceed?\", icon='question')\n",
    "        \n",
    "        if result == 'yes':\n",
    "            # Proceed and close the application\n",
    "            messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "            root.destroy()\n",
    "        else:\n",
    "            # Allow the user to select more files\n",
    "            messagebox.showinfo(\"Select More\", \"Please select more files.\")\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"File Selector\")\n",
    "\n",
    "# List to hold selected file paths\n",
    "selected_files = [None] * 8\n",
    "labels = []\n",
    "\n",
    "# Create labels and buttons for selecting files\n",
    "for i in range(8):\n",
    "    # Create a label for each file selection\n",
    "    label = tk.Label(root, text=f\"File {i+1}: Not selected\", width=100, anchor='w')\n",
    "    label.grid(row=i, column=0, padx=10, pady=5)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Create a button for each file selection\n",
    "    button = tk.Button(root, text=\"Select File\", command=lambda index=i: select_file(index))\n",
    "    button.grid(row=i, column=1, padx=10, pady=5)\n",
    "\n",
    "# Button to confirm file selections\n",
    "confirm_button = tk.Button(root, text=\"Confirm Selection\", command=confirm_selection)\n",
    "confirm_button.grid(row=8, column=0, columnspan=2, pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def select_files():\n",
    "    # Open the file dialog to select files\n",
    "    file_paths = filedialog.askopenfilenames(title=\"Select Files\", filetypes=[(\"All Files\", \"*.*\")])\n",
    "    \n",
    "    # Update the listbox with selected files\n",
    "    listbox.delete(0, tk.END)\n",
    "    for file in file_paths:\n",
    "        listbox.insert(tk.END, file)\n",
    "    \n",
    "    # Save the selected files to the global variable\n",
    "    selected_files.clear()\n",
    "    selected_files.extend(file_paths)\n",
    "    \n",
    "    # Confirm the number of selected files\n",
    "    if len(selected_files) == 0:\n",
    "        messagebox.showinfo(\"Information\", \"No files selected.\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Information\", f\"You have selected {len(selected_files)} files.\")\n",
    "\n",
    "def confirm_selection():\n",
    "    # Proceed with the application logic\n",
    "    if len(selected_files) > 0:\n",
    "        messagebox.showinfo(\"Proceed\", \"Files are selected. Proceeding with the application.\")\n",
    "    else:\n",
    "        messagebox.showwarning(\"Warning\", \"No files selected. Proceeding with the application.\")\n",
    "\n",
    "# Initialize the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"File Selector\")\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "# Create a Listbox to display selected files\n",
    "listbox = tk.Listbox(root, width=100, height=10)\n",
    "listbox.pack(pady=10)\n",
    "\n",
    "# Add buttons to select files and confirm selection\n",
    "select_button = tk.Button(root, text=\"Select Files\", command=select_files)\n",
    "select_button.pack(pady=5)\n",
    "\n",
    "confirm_button = tk.Button(root, text=\"Confirm Selection\", command=confirm_selection)\n",
    "confirm_button.pack(pady=5)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Staff ID Position Changed                            Control Items  \\\n",
      "0       101             Left                              EUC 1 Name1   \n",
      "1       102        No Change  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "2       102              Yes  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "3       102             Left  EUC 2 Name2\\nEUC 3 Name3\\nOther 4 Name4   \n",
      "4       103        No Change                              EUC 5 Name5   \n",
      "\n",
      "  Control Domain  \n",
      "0       EUC only  \n",
      "1            mix  \n",
      "2            mix  \n",
      "3            mix  \n",
      "4       EUC only  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Record Type': ['EUC', 'EUC', 'EUC', 'Other', 'EUC'],\n",
    "    'Record ID': [1, 2, 3, 4, 5],\n",
    "    'Record Name': ['Name1', 'Name2', 'Name3', 'Name4', 'Name5'],\n",
    "    'Staff ID': [101, 102, 102, 102, 103],\n",
    "    'Position Changed': ['Left', 'No Change', 'Yes', 'Left', 'No Change']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to concatenate control items and determine control domain\n",
    "def concatenate_control_items(group):\n",
    "    control_items = '\\n'.join(group['Record Type'] + ' ' + group['Record ID'].astype(str) + ' ' + group['Record Name'])\n",
    "    record_types = group['Record Type'].unique()\n",
    "    if len(record_types) > 1:\n",
    "        control_domain = 'mix'\n",
    "    elif 'EUC' in record_types:\n",
    "        control_domain = 'EUC only'\n",
    "    else:\n",
    "        control_domain = 'Other'\n",
    "    return pd.Series({\n",
    "        'Control Items': control_items,\n",
    "        'Control Domain': control_domain\n",
    "    })\n",
    "\n",
    "# Group by 'Staff ID' and apply the function\n",
    "result_df = df.groupby('Staff ID').apply(concatenate_control_items).reset_index()\n",
    "\n",
    "# Merge the result back to the original DataFrame to maintain original rows\n",
    "final_df = df.drop(columns=['Record Type', 'Record ID', 'Record Name']).drop_duplicates().merge(result_df, on='Staff ID', how='left')\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# Save the result to an Excel file if needed\n",
    "final_df.to_excel('final_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818cefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans file: C:/Users/KS/Start-Data-Analysis.xlsx\n",
      "Animals file: C:/Users/KS/Source.xlsx\n",
      "Plants file: C:/Users/KS/sample.xlsx\n",
      "Minerals file: C:/Users/KS/sam.xlsx\n",
      "Microbes file: C:/Users/KS/sf2_output.xlsx\n",
      "Fungi file: C:/Users/KS/modified_workbook1.xlsx\n",
      "Algae file: C:/Users/KS/modified_workbook.xlsx\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "class ExcelSheetSelector:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Excel Sheet Selector\")\n",
    "\n",
    "        self.file_paths = [None] * 7\n",
    "        self.labels = []\n",
    "\n",
    "        # Specific labels for each file selection\n",
    "        self.label_texts = [\n",
    "            \"Select Humans file:\",\n",
    "            \"Select Animals file:\",\n",
    "            \"Select Plants file:\",\n",
    "            \"Select Minerals file:\",\n",
    "            \"Select Microbes file:\",\n",
    "            \"Select Fungi file:\",\n",
    "            \"Select Algae file:\"\n",
    "        ]\n",
    "\n",
    "        # Create labels and buttons for 7 Excel sheets\n",
    "        for i in range(7):\n",
    "            label = tk.Label(root, text=self.label_texts[i])\n",
    "            label.grid(row=i, column=0, padx=10, pady=5)\n",
    "            \n",
    "            button = tk.Button(root, text=\"Browse\", command=lambda i=i: self.select_file(i))\n",
    "            button.grid(row=i, column=1, padx=10, pady=5)\n",
    "            \n",
    "            file_label = tk.Label(root, text=\"No file selected\")\n",
    "            file_label.grid(row=i, column=2, padx=10, pady=5)\n",
    "            self.labels.append(file_label)\n",
    "        \n",
    "        # Button to check and print selected file paths\n",
    "        self.print_button = tk.Button(root, text=\"Confirm Selections\", command=self.check_and_confirm)\n",
    "        self.print_button.grid(row=7, column=0, columnspan=3, pady=10)\n",
    "\n",
    "    def select_file(self, index):\n",
    "        self.root.withdraw()  # Hide the main window\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
    "        self.root.deiconify()  # Show the main window again\n",
    "        if file_path:\n",
    "            self.file_paths[index] = file_path\n",
    "            self.labels[index].config(text=file_path)\n",
    "\n",
    "    def check_and_confirm(self):\n",
    "        if all(self.file_paths):\n",
    "            self.root.destroy()  # Close the window gracefully\n",
    "        else:\n",
    "            messagebox.showwarning(\"Warning\", \"Please select all files before proceeding.\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "app = ExcelSheetSelector(root)\n",
    "root.mainloop()\n",
    "\n",
    "# After closing the window, capture the file paths into separate variables\n",
    "humans_file, animals_file, plants_file, minerals_file, microbes_file, fungi_file, algae_file = app.file_paths\n",
    "\n",
    "# Print the captured file paths (for verification)\n",
    "print(f\"Humans file: {humans_file}\")\n",
    "print(f\"Animals file: {animals_file}\")\n",
    "print(f\"Plants file: {plants_file}\")\n",
    "print(f\"Minerals file: {minerals_file}\")\n",
    "print(f\"Microbes file: {microbes_file}\")\n",
    "print(f\"Fungi file: {fungi_file}\")\n",
    "print(f\"Algae file: {algae_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32917012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Staff ID                          Control Items Position Changed  \\\n",
      "0         1  Issue 101 A\\nIssue 102 B\\nIssue 104 D              Yes   \n",
      "1         2             Action 103 C\\nAction 105 E              Yes   \n",
      "\n",
      "              Record Details  \n",
      "0  Detail1\\nDetail2\\nDetail4  \n",
      "1           Detail3\\nDetail5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data creation for demonstration\n",
    "data = {\n",
    "    'Record Type': ['Issue', 'Issue', 'Action', 'Issue', 'Action'],\n",
    "    'Record ID': [101, 102, 103, 104, 105],\n",
    "    'Record Name': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'Record Details': ['Detail1', 'Detail2', 'Detail3', 'Detail4', 'Detail5'],\n",
    "    'Staff ID': [1, 1, 2, 1, 2],\n",
    "    'Position Changed': ['Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to concatenate the desired fields\n",
    "def concatenate_records(group):\n",
    "    return '\\n'.join(group['Record Type'] + ' ' + group['Record ID'].astype(str) + ' ' + group['Record Name'])\n",
    "\n",
    "# Group by 'Staff ID' and apply the concatenation function\n",
    "grouped_df = df.groupby('Staff ID').apply(lambda x: pd.Series({\n",
    "    'Control Items': concatenate_records(x),\n",
    "    'Position Changed': x['Position Changed'].iloc[0]  # Assuming you want to keep the first 'Position Changed' value\n",
    "})).reset_index()\n",
    "\n",
    "# Optional: Include any other columns you want to keep from the original DataFrame\n",
    "# For demonstration, I'm adding 'Record Details' concatenated in the same way\n",
    "grouped_df['Record Details'] = df.groupby('Staff ID')['Record Details'].apply(lambda x: '\\n'.join(x)).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(grouped_df)\n",
    "\n",
    "# Save the final DataFrame to an Excel file\n",
    "# grouped_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ab357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lookup_and_merge(file1_path, file2_path, vOld_path):\n",
    "    # Read the first file into a DataFrame\n",
    "    df1 = pd.read_excel(file1_path)\n",
    "    \n",
    "    # Read the second file into a DataFrame\n",
    "    df2 = pd.read_excel(file2_path)\n",
    "    \n",
    "    # Read the vOld file into a DataFrame\n",
    "    vOld = pd.read_excel(vOld_path)\n",
    "    \n",
    "    # Ensure the Staff ID in both DataFrames are treated as numeric for accurate matching\n",
    "    df1['Employee ID'] = pd.to_numeric(df1['Employee ID'], errors='coerce').astype('Int64')\n",
    "    df2['Staff ID'] = pd.to_numeric(df2['Staff ID'], errors='coerce').astype('Int64')\n",
    "    vOld['Staff ID'] = pd.to_numeric(vOld['Staff ID'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Perform the lookup by merging the DataFrames on Staff ID and Employee ID\n",
    "    merged_df = df2.merge(df1[['Employee ID', 'Position Changed']], \n",
    "                          left_on='Staff ID', right_on='Employee ID', \n",
    "                          how='left')\n",
    "    \n",
    "    # Drop the redundant 'Employee ID' column from the merged DataFrame\n",
    "    merged_df.drop(columns=['Employee ID'], inplace=True)\n",
    "    \n",
    "    # Filter the DataFrame to retain only rows where 'Position Changed' is 'Left' or 'Yes'\n",
    "    filtered_df = merged_df[merged_df['Position Changed'].isin(['Left', 'Yes'])]\n",
    "    \n",
    "    # Ensure 'Staff ID' and 'Position Changed' columns are filled properly\n",
    "    filtered_df['Staff ID'] = filtered_df['Staff ID'].fillna('No ID')\n",
    "    filtered_df['Position Changed'] = filtered_df['Position Changed'].fillna('No Change')\n",
    "    \n",
    "    # Perform the second lookup by merging the filtered_df with vOld on Staff ID\n",
    "    final_df = filtered_df.merge(vOld[['Staff ID', 'Employee Name', 'Employee Business Email Address', 'Global Career Band']], \n",
    "                                 on='Staff ID', \n",
    "                                 how='left')\n",
    "    \n",
    "    # Rename the fetched columns\n",
    "    final_df.rename(columns={\n",
    "        'Employee Name': 'Functional Manager Employee Name',\n",
    "        'Employee Business Email Address': 'Functional Manager Email',\n",
    "        'Global Career Band': 'Functional Manager GCB'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Return the final DataFrame\n",
    "    return final_df\n",
    "\n",
    "# Paths to the Excel files\n",
    "file1_path = 'path_to_file1.xlsx'  # File with Employee ID, Position Changed, etc.\n",
    "file2_path = 'path_to_file2.xlsx'  # File with Record ID, Record Name, Record Details, Staff ID\n",
    "vOld_path = 'path_to_vOld.xlsx'    # File with additional details based on Staff ID\n",
    "\n",
    "# Perform the lookup, merge, and filter\n",
    "final_df = lookup_and_merge(file1_path, file2_path, vOld_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file\n",
    "final_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def lookup_and_merge(file1_path, file2_path):\n",
    "    # Read the first file into a DataFrame\n",
    "    df1 = pd.read_excel(file1_path)\n",
    "    \n",
    "    # Read the second file into a DataFrame\n",
    "    df2 = pd.read_excel(file2_path)\n",
    "    \n",
    "    # Ensure the Staff ID in both DataFrames are strings for accurate matching\n",
    "    df1['Employee ID'] = df1['Employee ID'].astype(str)\n",
    "    df2['Staff ID'] = df2['Staff ID'].astype(str)\n",
    "    \n",
    "    # Perform the lookup by merging the DataFrames on Staff ID and Employee ID\n",
    "    merged_df = df2.merge(df1[['Employee ID', 'Position Changed']], \n",
    "                          left_on='Staff ID', right_on='Employee ID', \n",
    "                          how='left')\n",
    "    \n",
    "    # Drop the redundant 'Employee ID' column from the merged DataFrame\n",
    "    merged_df.drop(columns=['Employee ID'], inplace=True)\n",
    "    \n",
    "    # Return the merged DataFrame\n",
    "    return merged_df\n",
    "\n",
    "# Paths to the Excel files\n",
    "file1_path = 'path_to_file1.xlsx'  # File with Employee ID, Position Changed, etc.\n",
    "file2_path = 'path_to_file2.xlsx'  # File with Record ID, Record Name, Record Details, Staff ID\n",
    "\n",
    "# Perform the lookup and merge\n",
    "final_df = lookup_and_merge(file1_path, file2_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# If needed, save the final DataFrame to a new Excel file\n",
    "# final_df.to_excel('path_to_output_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e82670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Check and print initial dtypes of columns\n",
    "    print(\"Initial dtypes of vNew:\")\n",
    "    print(vNew.dtypes)\n",
    "    print(\"\\nInitial dtypes of vOld:\")\n",
    "    print(vOld.dtypes)\n",
    "    \n",
    "    # Function to convert columns to int, skipping invalid rows\n",
    "    def convert_to_int(df, column):\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        df.dropna(subset=[column], inplace=True)\n",
    "        df[column] = df[column].astype(int)\n",
    "        return df\n",
    "\n",
    "    # Convert 'Employee ID' and 'Position Number' to integers\n",
    "    vNew = convert_to_int(vNew, 'Employee ID')\n",
    "    vNew = convert_to_int(vNew, 'Position Number')\n",
    "    vOld = convert_to_int(vOld, 'Employee ID')\n",
    "    vOld = convert_to_int(vOld, 'Position Number')\n",
    "    \n",
    "    # Check and print dtypes of columns after coercion attempt\n",
    "    print(\"\\nDtypes of vNew after coercion attempt:\")\n",
    "    print(vNew.dtypes)\n",
    "    print(\"\\nDtypes of vOld after coercion attempt:\")\n",
    "    print(vOld.dtypes)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID using left join\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='left')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID using left join\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='left')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_files(vnew_path, vold_path):\n",
    "    # Read the Excel files into DataFrames\n",
    "    vNew = pd.read_excel(vnew_path)\n",
    "    vOld = pd.read_excel(vold_path)\n",
    "    \n",
    "    # Select necessary columns from vNew\n",
    "    vNew_selected = vNew[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vNew_selected.rename(columns={\n",
    "        'Position Number': 'new Position Number',\n",
    "        'BF Level 4': 'new BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Select necessary columns from vOld\n",
    "    vOld_selected = vOld[['Employee ID', 'Position Number', 'BF Level 4']].copy()\n",
    "    vOld_selected.rename(columns={\n",
    "        'Position Number': 'old Position Number',\n",
    "        'BF Level 4': 'old BF Level 4'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrames on Employee ID\n",
    "    merged_df = pd.merge(vOld_selected, vNew_selected, on='Employee ID', how='outer')\n",
    "    \n",
    "    # Create the Position Changed column based on the specified conditions\n",
    "    def determine_position_changed(row):\n",
    "        if pd.isna(row['new Position Number']) and pd.isna(row['new BF Level 4']):\n",
    "            return 'Left'\n",
    "        elif (row['old Position Number'] != row['new Position Number']) or (row['old BF Level 4'] != row['new BF Level 4']):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No Change'\n",
    "    \n",
    "    merged_df['Position Changed'] = merged_df.apply(determine_position_changed, axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "vnew_path = 'path_to_vNew.xlsx'\n",
    "vold_path = 'path_to_vOld.xlsx'\n",
    "\n",
    "final_df = process_files(vnew_path, vold_path)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_single_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                        record_id_col, record_name_col, record_details_col, record_type_value):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "        record_type_value (str): Value to be placed in 'Record Type' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the specified value\n",
    "    filtered_df['Record Type'] = record_type_value\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add 'Staff ID' and 'Record Type' columns\n",
    "    result_df['Staff ID'] = filtered_df['Staff ID']\n",
    "    result_df['Record Type'] = filtered_df['Record Type']\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def concatenate_files(file_params_list):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_params_list (list of dict): List of dictionaries with parameters for each file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for params in file_params_list:\n",
    "        df = process_single_file(\n",
    "            file_path=params['file_path'],\n",
    "            staff_id_col=params['staff_id_col'],\n",
    "            issue_status_col=params['issue_status_col'],\n",
    "            filter_condition=params['filter_condition'],\n",
    "            record_id_col=params['record_id_col'],\n",
    "            record_name_col=params['record_name_col'],\n",
    "            record_details_col=params['record_details_col'],\n",
    "            record_type_value=params['record_type_value']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_params_list = [\n",
    "    {\n",
    "        'file_path': 'path_to_file1.xlsx',\n",
    "        'staff_id_col': 'Action Owner',\n",
    "        'issue_status_col': 'Issue Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'Issue ID',\n",
    "        'record_name_col': 'Issue Title',\n",
    "        'record_details_col': 'Issue Description',\n",
    "        'record_type_value': 'Issue'\n",
    "    },\n",
    "    {\n",
    "        'file_path': 'path_to_file2.xlsx',\n",
    "        'staff_id_col': 'Owner',\n",
    "        'issue_status_col': 'Status',\n",
    "        'filter_condition': 'open',\n",
    "        'record_id_col': 'ID',\n",
    "        'record_name_col': 'Title',\n",
    "        'record_details_col': 'Description',\n",
    "        'record_type_value': 'Action Owner'\n",
    "    },\n",
    "    # Add parameters for other files here\n",
    "]\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = concatenate_files(file_params_list)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_staff_id(action_owner_col):\n",
    "    \"\"\"Extract Staff ID from the given column using regex.\"\"\"\n",
    "    if isinstance(action_owner_col, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner_col)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "def process_file(file_path, staff_id_col, issue_status_col, filter_condition,\n",
    "                 record_id_col, record_name_col, record_details_col):\n",
    "    \"\"\"\n",
    "    Process a single file to extract relevant data and transform it into a standardized format.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        staff_id_col (str): Column name for the Staff ID extraction.\n",
    "        issue_status_col (str): Column name for filtering records.\n",
    "        filter_condition (str): Value to filter the records (e.g., 'open').\n",
    "        record_id_col (str): Column name for 'Record ID'.\n",
    "        record_name_col (str): Column name for 'Record Name'.\n",
    "        record_details_col (str): Column name for 'Record Details'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Ensure the staff ID column is treated as string\n",
    "    df[staff_id_col] = df[staff_id_col].astype(str)\n",
    "    \n",
    "    # Filter the DataFrame based on the provided condition\n",
    "    filtered_df = df[df[issue_status_col] == filter_condition]\n",
    "    \n",
    "    # Extract Staff ID from the specified column\n",
    "    filtered_df['Staff ID'] = filtered_df[staff_id_col].apply(extract_staff_id)\n",
    "    \n",
    "    # Add the 'Record Type' column with the value 'Issue'\n",
    "    filtered_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Create the final DataFrame with the required columns\n",
    "    result_df = filtered_df[[staff_id_col, record_id_col, record_name_col, record_details_col]].copy()\n",
    "    result_df.rename(columns={\n",
    "        staff_id_col: 'Staff ID',\n",
    "        record_id_col: 'Record ID',\n",
    "        record_name_col: 'Record Name',\n",
    "        record_details_col: 'Record Details'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add the 'Record Type' column\n",
    "    result_df['Record Type'] = 'Issue'\n",
    "    \n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def process_multiple_files(file_paths, column_mapping, filter_condition):\n",
    "    \"\"\"\n",
    "    Process multiple files and concatenate the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_paths (list of str): List of paths to input Excel files.\n",
    "        column_mapping (list of dict): List of dictionaries with column mappings for each file.\n",
    "        filter_condition (str): Value to filter records (e.g., 'open').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with results from all files.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for file_path, columns in zip(file_paths, column_mapping):\n",
    "        df = process_file(\n",
    "            file_path,\n",
    "            staff_id_col=columns['staff_id'],\n",
    "            issue_status_col=columns['issue_status'],\n",
    "            filter_condition=filter_condition,\n",
    "            record_id_col=columns['record_id'],\n",
    "            record_name_col=columns['record_name'],\n",
    "            record_details_col=columns['record_details']\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example usage\n",
    "file_paths = [\n",
    "    'path_to_file1.xlsx',\n",
    "    'path_to_file2.xlsx',\n",
    "    'path_to_file3.xlsx',\n",
    "    'path_to_file4.xlsx',\n",
    "    'path_to_file5.xlsx'\n",
    "]\n",
    "\n",
    "column_mapping = [\n",
    "    {\n",
    "        'staff_id': 'Action Owner',  # Column name to extract Staff ID\n",
    "        'issue_status': 'Issue Status',  # Column name for issue status\n",
    "        'record_id': 'Issue ID',  # Column name for Record ID\n",
    "        'record_name': 'Issue Title',  # Column name for Record Name\n",
    "        'record_details': 'Issue Description'  # Column name for Record Details\n",
    "    },\n",
    "    # Add mappings for other files here\n",
    "]\n",
    "\n",
    "filter_condition = 'open'  # Condition to filter records\n",
    "\n",
    "# Process multiple files and get the final DataFrame\n",
    "final_df = process_multiple_files(file_paths, column_mapping, filter_condition)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to extract Staff ID using regular expression\n",
    "def extract_staff_id(action_owner):\n",
    "    if isinstance(action_owner, str):\n",
    "        match = re.search(r'\\((\\d{8})\\)', action_owner)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel('path_to_your_file.xlsx')\n",
    "\n",
    "# Ensure \"Action Owner\" column is treated as string\n",
    "df['Action Owner'] = df['Action Owner'].astype(str)\n",
    "\n",
    "# Filter the DataFrame for rows where 'Issue Status' is 'open'\n",
    "open_issues_df = df[df['Issue Status'] == 'open']\n",
    "\n",
    "# Extract Staff ID from \"Action Owner\" column for filtered rows\n",
    "open_issues_df['Staff ID'] = open_issues_df['Action Owner'].apply(extract_staff_id)\n",
    "\n",
    "# Add the 'Record Type' column with value 'Issue'\n",
    "open_issues_df['Record Type'] = 'Issue'\n",
    "\n",
    "# Create the final DataFrame with the required columns\n",
    "result_df = open_issues_df[['Staff ID', 'Record Type', 'Issue ID', 'Issue Title', 'Issue Description']]\n",
    "result_df.rename(columns={\n",
    "    'Issue ID': 'Record ID',\n",
    "    'Issue Title': 'Record Name',\n",
    "    'Issue Description': 'Record Details'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={\n",
    "    'Position Number_new': 'new Position Number',\n",
    "    'Level4_new': 'new Level4',\n",
    "    'Position Number_old': 'old Position Number',\n",
    "    'Level4_old': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Create the \"Position Changed\" column\n",
    "merged_df['Position Changed'] = merged_df.apply(\n",
    "    lambda row: 'New' if pd.isna(row['old Position Number']) or pd.isna(row['old Level4']) else 'No Change', axis=1)\n",
    "\n",
    "# Fetch records from vOld where Employee ID is not in vNew\n",
    "not_in_vNew = vOld[~vOld['Employee ID'].isin(vNew['Employee ID'])]\n",
    "\n",
    "# Rename columns to match the format of merged_df\n",
    "not_in_vNew.rename(columns={\n",
    "    'Position Number': 'old Position Number',\n",
    "    'Level4': 'old Level4'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add new columns with default values for records not in vNew\n",
    "not_in_vNew['new Position Number'] = None\n",
    "not_in_vNew['new Level4'] = None\n",
    "not_in_vNew['Position Changed'] = 'Left'\n",
    "\n",
    "# Reorder columns to match the merged_df structure\n",
    "not_in_vNew = not_in_vNew[['Employee ID', 'new Position Number', 'new Level4', 'old Position Number', 'old Level4', 'Position Changed']]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "final_df = pd.concat([merged_df, not_in_vNew], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select relevant columns from vNew and vOld\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge vNew and vOld on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "# Rename columns to match the required format\n",
    "merged_df.rename(columns={'Position Number_new': 'new Position Number',\n",
    "                          'Level4_new': 'new Level4',\n",
    "                          'Position Number_old': 'old Position Number',\n",
    "                          'Level4_old': 'old Level4'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a346553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Create the 'In Scope' column\n",
    "vOld['In Scope'] = ((vOld['Pos Check'] != vOld['Position Number']) | \n",
    "                    (vOld['BF4 Check'] != vOld['BF Level 4 Name']) | \n",
    "                    (vOld['Country Check'] != vOld['Work Location Country/Territory Name'])).apply(lambda x: 'Movement' if x else 'No Movement')\n",
    "\n",
    "# Create 'Position Changed', 'BF Changed', and 'Country Changed' columns\n",
    "vOld['Position Changed'] = vOld.apply(lambda row: 'Left' if row['Pos Check'] == 'left' else ('Yes' if row['Pos Check'] != row['Position Number'] else 'No'), axis=1)\n",
    "vOld['BF Changed'] = vOld.apply(lambda row: 'Left' if row['BF4 Check'] == 'left' else ('Yes' if row['BF4 Check'] != row['BF Level 4 Name'] else 'No'), axis=1)\n",
    "vOld['Country Changed'] = vOld.apply(lambda row: 'Left' if row['Country Check'] == 'left' else ('Yes' if row['Country Check'] != row['Work Location Country/Territory Name'] else 'No'), axis=1)\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel files into DataFrames\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Ensure column names are properly stripped of leading/trailing spaces\n",
    "vNew.columns = vNew.columns.str.strip()\n",
    "vOld.columns = vOld.columns.str.strip()\n",
    "\n",
    "# Initialize new columns in vOld with default value 'left'\n",
    "vOld['Pos Check'] = 'left'\n",
    "vOld['BF4 Check'] = 'left'\n",
    "vOld['Country Check'] = 'left'\n",
    "\n",
    "# Perform the lookup and update the values for matching Employee Ids\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Pos Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Position Number'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'BF4 Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['BF Level 4 Name'])\n",
    "vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Country Check'] = vOld.loc[vOld['Employee ID'].isin(vNew['Employee ID']), 'Employee ID'].map(vNew.set_index('Employee ID')['Work Location Country/Territory Name'])\n",
    "\n",
    "# Display the updated vOld DataFrame\n",
    "print(vOld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the new and old data from the Excel workbooks\n",
    "vNew = pd.read_excel('path_to_vNew.xlsx')\n",
    "vOld = pd.read_excel('path_to_vOld.xlsx')\n",
    "\n",
    "# Select the required columns\n",
    "vNew = vNew[['Employee ID', 'Position Number', 'Level4']]\n",
    "vOld = vOld[['Employee ID', 'Position Number', 'Level4']]\n",
    "\n",
    "# Merge the dataframes on Employee ID\n",
    "merged_df = pd.merge(vNew, vOld, on='Employee ID', how='outer', suffixes=('_new', '_old'))\n",
    "\n",
    "# Create the Status column\n",
    "merged_df['Status'] = merged_df.apply(\n",
    "    lambda row: 'new' if pd.isna(row['Position Number_old']) and not pd.isna(row['Position Number_new']) else \n",
    "                ('left' if pd.isna(row['Position Number_new']) and not pd.isna(row['Position Number_old']) else \n",
    "                'existing'), axis=1)\n",
    "\n",
    "# Save the resulting dataframe to a new Excel file\n",
    "merged_df.to_excel('path_to_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'High', 'Low', 'Total']]\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=0, index=False)\n",
    "        \n",
    "        # Load the workbook and access the worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = workbook[sheet_name]\n",
    "\n",
    "        # Initialize start row for writing subtables\n",
    "        start_row = len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create a list to store all subtables\n",
    "        subtables_list = []\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            subtables_list.append((mapped_l3_heading, None))  # Append heading to list\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                subtables_list.append((role_temp_heading, None))  # Append heading to list\n",
    "\n",
    "                # Append the subtable data to the list\n",
    "                subtables_list.append((None, subtable_data))\n",
    "\n",
    "        # Write subtables to Excel with proper gaps\n",
    "        for item in subtables_list:\n",
    "            if item[0]:  # If it's a heading\n",
    "                worksheet.cell(row=start_row, column=1, value=item[0])\n",
    "                start_row += 2  # 2-line gap before next heading or subtable\n",
    "            elif item[1] is not None:  # If it's subtable data\n",
    "                item[1].to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False, header=True)\n",
    "                start_row += len(item[1]) + 1  # 1-line gap after subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901416c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = 'your_excel_file.xlsx'  # Replace with your file path\n",
    "output_file_path = 'grouped_data_with_subtables.xlsx'  # Replace with your desired output file path\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a Pandas Excel writer object\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        # Read each sheet into a DataFrame\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "\n",
    "        # Group the data by 'Mapped L3' and 'Role_temp'\n",
    "        grouped = df.groupby(['Mapped L3', 'Role_temp'])\n",
    "\n",
    "        # Count the occurrences of 'High' and 'Low' in the 'High/Low' column\n",
    "        result = grouped['High/Low'].value_counts().unstack(fill_value=0).fillna(0)\n",
    "\n",
    "        # Add totals for each row ('Mapped L3', 'Role_temp')\n",
    "        result['Total'] = result.sum(axis=1)\n",
    "\n",
    "        # Add a totals row\n",
    "        total_row = result.sum(axis=0).to_frame().T\n",
    "        total_row.index = pd.MultiIndex.from_tuples([('Total', '')])\n",
    "        result = pd.concat([result, total_row])\n",
    "\n",
    "        # Reset the index to get 'Mapped L3' and 'Role_temp' as columns\n",
    "        result.reset_index(inplace=True)\n",
    "\n",
    "        # Ensure 'High' and 'Low' are present, if not add them with default 0 values\n",
    "        if 'High' not in result.columns:\n",
    "            result['High'] = 0\n",
    "        if 'Low' not in result.columns:\n",
    "            result['Low'] = 0\n",
    "        result = result[['Mapped L3', 'Role_temp', 'Total', 'High', 'Low']]\n",
    "\n",
    "        # Initialize start row for writing grouped data\n",
    "        start_row = 0\n",
    "\n",
    "        # Write the grouped data to the sheet\n",
    "        result.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "        start_row += len(result) + 2  # Move to the next row after grouped data\n",
    "\n",
    "        # Create subtables for each unique 'Mapped L3'\n",
    "        unique_mapped_l3 = df['Mapped L3'].unique()\n",
    "        for mapped_l3 in unique_mapped_l3:\n",
    "            # Skip 'Total' rows if present\n",
    "            if mapped_l3 == 'Total':\n",
    "                continue\n",
    "\n",
    "            # Filter the original DataFrame based on 'Mapped L3'\n",
    "            mapped_l3_data = df[df['Mapped L3'] == mapped_l3]\n",
    "\n",
    "            # Insert a heading for unique 'Mapped L3'\n",
    "            mapped_l3_heading = f\"Unique L3 Value: {mapped_l3}\"\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            worksheet.cell(row=start_row, column=1, value=mapped_l3_heading)\n",
    "\n",
    "            # Move to the next row after writing unique L3 heading\n",
    "            start_row += 2\n",
    "\n",
    "            # Iterate over each unique 'Role_temp' for the current 'Mapped L3'\n",
    "            unique_role_temp = mapped_l3_data['Role_temp'].unique()\n",
    "            for role_temp in unique_role_temp:\n",
    "                # Filter the data for the current 'Mapped L3' and 'Role_temp'\n",
    "                subtable_data = mapped_l3_data[mapped_l3_data['Role_temp'] == role_temp]\n",
    "\n",
    "                # Insert a heading for 'Role_temp'\n",
    "                role_temp_heading = f\"Role Type: {role_temp}\"\n",
    "                worksheet.cell(row=start_row, column=1, value=role_temp_heading)\n",
    "\n",
    "                # Move to the next row after writing role_temp heading\n",
    "                start_row += 2\n",
    "\n",
    "                # Insert an empty row to separate headings from subtable data\n",
    "                worksheet.cell(row=start_row, column=1, value=\"\")\n",
    "\n",
    "                # Move to the next row after inserting empty row\n",
    "                start_row += 1\n",
    "\n",
    "                # Write the subtable data to the sheet\n",
    "                subtable_data.to_excel(writer, sheet_name=sheet_name, startrow=start_row, index=False)\n",
    "\n",
    "                # Move to the next row after writing subtable data\n",
    "                start_row += len(subtable_data) + 2  # Add extra space after each subtable\n",
    "\n",
    "print(f\"Grouped data with subtables has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19c99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, None, None, None, None]  # Initially None or some default value\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is still NaN\n",
    "mask = df1['colS'].isna()\n",
    "\n",
    "# Perform the second merge using colA and colPM for the NaN rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the NaN rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5784398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS\n",
      "0    1    10  NaN\n",
      "1    2    20  NaN\n",
      "2    3    30    a\n",
      "3    4    40    b\n",
      "4    7    50    c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4, 7],\n",
    "    'colB': [10, 20, 30, 40, 50],\n",
    "    'colS': [None, '', ' ', '   ', '']  # Different types of empty or blank values\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    'colPM': [7, 8, 9, 10, 11, 12]\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA, colR, and colPM are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "df2['colPM'] = df2['colPM'].astype(str)\n",
    "\n",
    "# Initial merge using colA and colR\n",
    "df_merged_initial = pd.merge(df1, df2[['colR', 'colS']], left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Update df1's colS with the values from initial merge\n",
    "df1['colS'] = df_merged_initial['colS_y']\n",
    "\n",
    "# Identify rows where colS is empty or blank\n",
    "mask = df1['colS'].apply(lambda x: x == '' or x.isspace() if isinstance(x, str) else False)\n",
    "\n",
    "# Perform the second merge using colA and colPM for the empty or blank rows\n",
    "df_merged_second = pd.merge(df1[mask], df2[['colPM', 'colS']], left_on='colA', right_on='colPM', how='left')\n",
    "\n",
    "# Update colS in df1 for the empty or blank rows with values from the second merge\n",
    "df1.loc[mask, 'colS'] = df_merged_second['colS_y']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44b6bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colS_x</th>\n",
       "      <th>colPM</th>\n",
       "      <th>colS_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  colA  colB colS_x colPM colS_y\n",
       "0    1    10    NaN   NaN    NaN\n",
       "1    2    20    NaN   NaN    NaN\n",
       "2    7    50    NaN     7      a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b765483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colA  colB colS1\n",
      "0    1    10   NaN\n",
      "1    2    20   NaN\n",
      "2    3    30     a\n",
      "3    4    40     c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1 with 30 columns (only a few shown here for simplicity)\n",
    "data1 = {\n",
    "    'colA': [1, 2, 3, 4],\n",
    "    'colB': [10, 20, 30, 40],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with 45 columns (only a few shown here for simplicity)\n",
    "data2 = {\n",
    "    'colR': [3, 3, 4, 5, 6, 6],\n",
    "    'colS': ['a', 'b', 'c', 'd', 'e', 'f'],\n",
    "    # Add other columns as needed\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Ensure the data types of colA and colR are the same\n",
    "df1['colA'] = df1['colA'].astype(str)\n",
    "df2['colR'] = df2['colR'].astype(str)\n",
    "\n",
    "# Select only the required columns from df2\n",
    "df2_selected = df2[['colR', 'colS']].drop_duplicates(subset='colR')\n",
    "\n",
    "# Merge df1 with df2_selected on colA and colR\n",
    "df_merged = pd.merge(df1, df2_selected, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Assign the values to the new column in df1 and drop any extra columns\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "# Drop the extra merge column if needed (not strictly necessary, but clean)\n",
    "df1.drop(columns=['colR'], inplace=True, errors='ignore')\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d80a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colA colS1\n",
      "0     1   NaN\n",
      "1     2   NaN\n",
      "2     3     a\n",
      "3     4     b\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for df1\n",
    "data1 = {'colA': [1, 2, 3, 4]}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Sample data for df2 with non-unique colR values\n",
    "data2 = {'colR': [3, 3, 4, 5, 6, 6], 'colS': ['a', 'b', 'c', 'd', 'e', 'f']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge df1 with df2 on the condition that df1['colA'] matches df2['colR']\n",
    "df_merged = pd.merge(df1, df2, left_on='colA', right_on='colR', how='left')\n",
    "\n",
    "# Select only relevant columns and rename them\n",
    "df1['colS1'] = df_merged['colS']\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c042a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id colX colY colZ\n",
      "0   6   X6   Y6   Z6\n",
      "1   7   X7   Y7   Z7\n",
      "2   8   X8   Y8   Z8\n",
      "3   1  NaN   B1   C1\n",
      "4   2  NaN   B2   C2\n",
      "5   3  NaN   B3   C3\n",
      "6   4  NaN   B4   C4\n",
      "7   5  NaN   B5   C5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'colA': ['A1', 'A2', 'A3', 'A4', 'A5'],\n",
    "    'colB': ['B1', 'B2', 'B3', 'B4', 'B5'],\n",
    "    'colC': ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'id': [6, 7, 8],\n",
    "    'colX': ['X6', 'X7', 'X8'],\n",
    "    'colY': ['Y6', 'Y7', 'Y8'],\n",
    "    'colZ': ['Z6', 'Z7', 'Z8']\n",
    "})\n",
    "\n",
    "# Columns to copy from df1 and their corresponding columns in df2\n",
    "columns_to_copy = {\n",
    "    'id': 'id',\n",
    "    'colB': 'colY',\n",
    "    'colC': 'colZ'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with the selected columns from df1\n",
    "new_rows = df1[list(columns_to_copy.keys())].copy()\n",
    "\n",
    "# Rename the columns in the new DataFrame to match the column names in df2\n",
    "new_rows.rename(columns=columns_to_copy, inplace=True)\n",
    "\n",
    "# Append the new DataFrame to df2\n",
    "df2 = df2.append(new_rows, ignore_index=True)\n",
    "\n",
    "# Display the updated df2\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
