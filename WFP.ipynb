{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADWP processing script (memory-optimized)\n",
    "- Reads sheet 'Data' from large Excel workbook without fully loading into memory\n",
    "- Produces 'Output' sheet with dynamic 24-month columns (selected_year -> selected_year+1)\n",
    "Requirements:\n",
    "    pip install pandas openpyxl python-dateutil tqdm\n",
    "Run:\n",
    "    python adwp_process.py --input \"GHA.xlsx\" --sheet \"Data\" --out \"ADWP_Output.xlsx\" --year 2025 --current_month 8\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dateparser\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Helpers / Config\n",
    "# -------------------------\n",
    "def month_label(dt):\n",
    "    # dt is datetime-like -> returns \"Jan-25\"\n",
    "    return dt.strftime(\"%b-%y\")\n",
    "\n",
    "def first_of_month_from_str(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    try:\n",
    "        d = dateparser.parse(str(s), dayfirst=False)\n",
    "        return d.replace(day=1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Step 0: CLI\n",
    "# -------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--input\", required=True, help=\"Input Excel file path\")\n",
    "    p.add_argument(\"--sheet\", default=\"Data\", help=\"Sheet name containing vertical data\")\n",
    "    p.add_argument(\"--out\", default=\"ADWP_Output.xlsx\", help=\"Output Excel file\")\n",
    "    p.add_argument(\"--year\", type=int, required=True, help=\"Selected start year (e.g., 2025)\")\n",
    "    p.add_argument(\"--current_month\", type=int, required=True, help=\"Current month number (1-12)\")\n",
    "    p.add_argument(\"--chunksize\", type=int, default=100000, help=\"CSV chunk size for pandas\")\n",
    "    return p.parse_args()\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Stream Data sheet -> temp CSV\n",
    "# -------------------------\n",
    "def excel_sheet_to_csv(excel_path, sheet_name, csv_path):\n",
    "    # Use openpyxl in read_only mode to stream rows\n",
    "    wb = load_workbook(filename=excel_path, read_only=True, data_only=True)\n",
    "    if sheet_name not in wb.sheetnames:\n",
    "        raise ValueError(f\"Sheet {sheet_name} not found in {excel_path}\")\n",
    "    ws = wb[sheet_name]\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Iterate rows and write them. First row assumed header.\n",
    "        for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "            # convert openpyxl None -> '' for CSV cleanliness\n",
    "            writer.writerow([(\"\" if v is None else v) for v in row])\n",
    "    wb.close()\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Chunked normalization -> intermediate compact CSV\n",
    "# -------------------------\n",
    "def process_chunks_to_intermediate(csv_path, tmp_intermediate, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Reads big CSV in chunks, normalizes columns, computes PositionKey,\n",
    "    keeps last row per (PositionKey, PeriodMonth) within chunk, and appends to intermediate CSV.\n",
    "    The intermediate CSV will contain:\n",
    "      PosKey, PID, MPP_ID, GCB, BF, ServiceL2, BFG, CountryR1, CountryR2, CountryR3,\n",
    "      Stack, PeriodMonth_iso, MonthLabel, FTE, row_order\n",
    "    We'll rely on later aggregation to reduce duplicates across chunks.\n",
    "    \"\"\"\n",
    "    # You MUST update these header names if your Data sheet uses different names.\n",
    "    # Based on your spec:\n",
    "    expected_cols = {\n",
    "        \"PID\": \"PID\",\n",
    "        \"MPP ID\": \"MPP ID\",\n",
    "        \"GCB\": \"GCB\",\n",
    "        \"Business Framework\": \"Business Framework\",\n",
    "        \"Business Framework Group\": \"Business Framework Group\",\n",
    "        \"Business Service L2\": \"Business Service L2\",\n",
    "        \"Country R1\": \"Country R1\",\n",
    "        \"Country R2\": \"Country R2\",\n",
    "        \"Country R3\": \"Country R3\",\n",
    "        \"FTE\": \"FTE\",\n",
    "        \"Stack\": \"Stack\",\n",
    "        \"Period Month\": \"Period Month\"\n",
    "    }\n",
    "\n",
    "    # read header first to map columns (do one small read)\n",
    "    header_df = pd.read_csv(csv_path, nrows=0)\n",
    "    header_cols = header_df.columns.tolist()\n",
    "\n",
    "    # if column names differ slightly, you can extend mapping logic.\n",
    "    # For now assume exact matches as per user spec.\n",
    "\n",
    "    usecols = [c for c in header_cols]  # read all columns; modify if you want to restrict\n",
    "\n",
    "    chunk_iter = pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize, iterator=True, dtype=str)\n",
    "\n",
    "    # Prepare intermediate CSV\n",
    "    intermediate_cols = [\n",
    "        \"PosKey\", \"PID\", \"MPP ID\", \"GCB\", \"Business Framework\", \"Business Framework Group\",\n",
    "        \"Business Service L2\", \"Country R1\", \"Country R2\", \"Country R3\", \"Stack\",\n",
    "        \"PeriodMonth_iso\", \"MonthLabel\", \"FTE\", \"row_idx\"\n",
    "    ]\n",
    "    with open(tmp_intermediate, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(intermediate_cols)\n",
    "\n",
    "    row_global_idx = 0\n",
    "    for chunk in tqdm(chunk_iter, desc=\"Processing chunks\"):\n",
    "        # Ensure known columns exist and coerce types\n",
    "        # Normalize column names (strip)\n",
    "        chunk.columns = [c.strip() for c in chunk.columns]\n",
    "\n",
    "        # Convert Period Month -> first of month\n",
    "        chunk[\"PeriodMonth_dt\"] = chunk[\"Period Month\"].apply(first_of_month_from_str)\n",
    "\n",
    "        # Build PosKey: prefer PID if present else MPP ID, combined with GCB and BF\n",
    "        def build_poskey(r):\n",
    "            pid = r.get(\"PID\", \"\")\n",
    "            mpp = r.get(\"MPP ID\", \"\")\n",
    "            gcb = r.get(\"GCB\", \"\")\n",
    "            bf = r.get(\"Business Framework\", \"\")\n",
    "            if pid not in (None, \"\", \"nan\"):\n",
    "                return f\"PID|{pid}|{gcb}|{bf}\"\n",
    "            else:\n",
    "                return f\"MPP|{mpp}|{gcb}|{bf}\"\n",
    "\n",
    "        chunk[\"PosKey\"] = chunk.apply(build_poskey, axis=1)\n",
    "\n",
    "        # Keep last row per PosKey + PeriodMonth within this chunk (by appearance)\n",
    "        chunk[\"row_idx_local\"] = range(row_global_idx, row_global_idx + len(chunk))\n",
    "        row_global_idx += len(chunk)\n",
    "\n",
    "        # Normalize FTE numeric\n",
    "        chunk[\"FTE\"] = pd.to_numeric(chunk[\"FTE\"], errors=\"coerce\")\n",
    "\n",
    "        # Keep needed cols for intermediate\n",
    "        mini = chunk[[\n",
    "            \"PosKey\", \"PID\", \"MPP ID\", \"GCB\", \"Business Framework\", \"Business Framework Group\",\n",
    "            \"Business Service L2\", \"Country R1\", \"Country R2\", \"Country R3\", \"Stack\",\n",
    "            \"PeriodMonth_dt\", \"FTE\", \"row_idx_local\"\n",
    "        ]].copy()\n",
    "        # rename for writing\n",
    "        mini = mini.rename(columns={\"PeriodMonth_dt\": \"PeriodMonth_dt\", \"row_idx_local\": \"row_idx\"})\n",
    "        # drop rows with null PeriodMonth_dt? We keep (they'll have empty month)\n",
    "        mini[\"MonthLabel\"] = mini[\"PeriodMonth_dt\"].apply(lambda x: month_label(x) if pd.notna(x) else \"\")\n",
    "\n",
    "        # For duplicate PosKey+PeriodMonth inside chunk, keep the last (max row_idx)\n",
    "        mini[\"PosMonth\"] = mini[\"PosKey\"].astype(str) + \"|\" + mini[\"MonthLabel\"].astype(str)\n",
    "        mini.sort_values(by=\"row_idx\", inplace=True)\n",
    "        # drop duplicates, keep last\n",
    "        mini = mini.drop_duplicates(subset=[\"PosMonth\"], keep=\"last\")\n",
    "\n",
    "        # write to intermediate CSV\n",
    "        with open(tmp_intermediate, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for _, r in mini.iterrows():\n",
    "                writer.writerow([\n",
    "                    r[\"PosKey\"], r.get(\"PID\", \"\"), r.get(\"MPP ID\", \"\"), r.get(\"GCB\", \"\"), r.get(\"Business Framework\", \"\"),\n",
    "                    r.get(\"Business Framework Group\", \"\"), r.get(\"Business Service L2\", \"\"), r.get(\"Country R1\", \"\"),\n",
    "                    r.get(\"Country R2\", \"\"), r.get(\"Country R3\", \"\"), r.get(\"Stack\", \"\"),\n",
    "                    (r[\"PeriodMonth_dt\"].isoformat() if pd.notna(r[\"PeriodMonth_dt\"]) else \"\"), r[\"MonthLabel\"],\n",
    "                    (\"\" if pd.isna(r[\"FTE\"]) else r[\"FTE\"]), r[\"row_idx\"]\n",
    "                ])\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Aggregate intermediate -> final pivot\n",
    "# -------------------------\n",
    "def build_final_output_from_intermediate(tmp_intermediate, output_excel, selected_year, current_month):\n",
    "    # read intermediate (should be much smaller)\n",
    "    df = pd.read_csv(tmp_intermediate, dtype=str)\n",
    "    # fix dtypes\n",
    "    df[\"FTE\"] = pd.to_numeric(df[\"FTE\"], errors=\"coerce\")\n",
    "    df[\"PeriodMonth_iso\"] = df[\"PeriodMonth_iso\"].replace(\"\", pd.NA)\n",
    "    # If duplicate PosKey+Month across chunks, keep the last by row_idx\n",
    "    df[\"row_idx\"] = pd.to_numeric(df[\"row_idx\"], errors=\"coerce\")\n",
    "    df.sort_values(by=\"row_idx\", inplace=True)\n",
    "    df[\"PosMonth\"] = df[\"PosKey\"].astype(str) + \"|\" + df[\"MonthLabel\"].astype(str)\n",
    "    df = df.drop_duplicates(subset=[\"PosMonth\"], keep=\"last\")\n",
    "    # pivot to wide: rows = PosKey (but we need dimensions too)\n",
    "    # Build list of month headers for the 24-month horizon\n",
    "    start = datetime(selected_year, 1, 1)\n",
    "    # We need labels from Jan-selected_year to Dec-(selected_year+1)\n",
    "    labels = []\n",
    "    months = []\n",
    "    for y in (selected_year, selected_year + 1):\n",
    "        for m in range(1, 13):\n",
    "            d = datetime(y, m, 1)\n",
    "            labels.append(month_label(d))\n",
    "            months.append(d)\n",
    "    # We'll create pivot table with these labels\n",
    "    # create a column 'MonthLabel' already exists; ensure consistent format (e.g., 'Jan-25')\n",
    "    # Pivot:\n",
    "    pivot = df.pivot_table(index=\"PosKey\", columns=\"MonthLabel\", values=\"FTE\", aggfunc=\"last\")\n",
    "    # Ensure all labels exist\n",
    "    for lbl in labels:\n",
    "        if lbl not in pivot.columns:\n",
    "            pivot[lbl] = pd.NA\n",
    "    pivot = pivot[labels]  # reorder\n",
    "\n",
    "    # We now need to assemble dimension columns per PosKey:\n",
    "    # Take the most recent non-null record for each PosKey (by row_idx)\n",
    "    dims = df.sort_values(by=\"row_idx\").groupby(\"PosKey\", as_index=False).last()\n",
    "    # dims contains PID, MPP ID, GCB, Business Framework, etc.\n",
    "    # Compose final dataframe\n",
    "    final = dims[[\n",
    "        \"Business Service L2\", \"Business Framework Group\", \"Business Framework\",\n",
    "        \"Country R1\", \"Country R2\", \"Country R3\", \"PID\", \"GCB\", \"MPP ID\", \"Stack\"\n",
    "    ]].copy()\n",
    "    final.index = dims[\"PosKey\"]\n",
    "    # Add FTE (current month FTE): we must determine the current month label\n",
    "    curr_dt = datetime(selected_year, current_month, 1)\n",
    "    current_label = month_label(curr_dt)\n",
    "    # If current_label not in pivot columns (because current_month might not be in the two-year window), handle\n",
    "    if current_label not in pivot.columns:\n",
    "        # If current month is outside our 24-month window, choose nearest (last available)\n",
    "        # But usually it will be within window\n",
    "        print(f\"Warning: current month label {current_label} not in pivot columns.\")\n",
    "        final[\"FTE\"] = pd.NA\n",
    "    else:\n",
    "        final[\"FTE\"] = pivot[current_label]\n",
    "\n",
    "    # Attach monthly columns after the 'Stack' column; user asked order:\n",
    "    # Required order: Business Service L2, Business Framework Group, Business Framework, Country R1, Country R2, Country R3, PID, GCB, FTE, MPP ID, Stack, then month columns\n",
    "    ordered = final[[\n",
    "        \"Business Service L2\", \"Business Framework Group\", \"Business Framework\", \"Country R1\",\n",
    "        \"Country R2\", \"Country R3\", \"PID\", \"GCB\", \"FTE\", \"MPP ID\", \"Stack\"\n",
    "    ]].copy()\n",
    "\n",
    "    # Add the 24 month columns (forward-fill logic per rules)\n",
    "    for lbl in labels:\n",
    "        ordered[lbl] = pivot[lbl] if lbl in pivot.columns else pd.NA\n",
    "\n",
    "    # Forward-fill months after current month per PosKey if required:\n",
    "    # Rules recap:\n",
    "    # - For PID-present groups: After current month, forward-fill with current month FTE for that row (if PID group exists and current month has value)\n",
    "    # - For MPP groups (PID missing): If first non-null month is future hire, then fill from that month forward with that FTE\n",
    "    # Simpler universal approach:\n",
    "    # 1) For rows where PID is present:\n",
    "    #    - If current month has non-null FTE value, fill NaNs for months after current month with that value\n",
    "    # 2) For rows where PID is missing:\n",
    "    #    - Find the earliest non-null month label; starting from that month forward, fill forward with that value (this handles hire/demise logic if FTE is -1)\n",
    "    # Apply:\n",
    "    for idx, row in ordered.iterrows():\n",
    "        pid = row.get(\"PID\", \"\")\n",
    "        # build month-value series\n",
    "        month_vals = row[labels].copy()\n",
    "        if pid not in (None, \"\", \"nan\"):\n",
    "            # PID present => forward-fill after current month with current month value\n",
    "            curr_val = row.get(\"FTE\")\n",
    "            if pd.notna(curr_val):\n",
    "                after_mask = [ (months[i] > curr_dt) for i in range(len(months)) ]\n",
    "                for i, after in enumerate(after_mask):\n",
    "                    if after:\n",
    "                        # if month empty, set to curr_val\n",
    "                        if pd.isna(month_vals.iloc[i]):\n",
    "                            month_vals.iloc[i] = curr_val\n",
    "        else:\n",
    "            # MPP case: find first non-null month index\n",
    "            nonnull = month_vals.first_valid_index()\n",
    "            if nonnull is not None:\n",
    "                # find the value at that month (first non-null)\n",
    "                first_idx = month_vals.first_valid_index()\n",
    "                if first_idx is not None:\n",
    "                    first_val = month_vals.loc[first_idx]\n",
    "                    # fill forward from that month onward if blanks exist\n",
    "                    start_idx = labels.index(first_idx)\n",
    "                    for i in range(start_idx, len(labels)):\n",
    "                        if pd.isna(month_vals.iloc[i]):\n",
    "                            month_vals.iloc[i] = first_val\n",
    "        # write back\n",
    "        for lbl in labels:\n",
    "            ordered.at[idx, lbl] = month_vals[lbl]\n",
    "\n",
    "    # Final tidy: sort as requested\n",
    "    ordered = ordered.sort_values(by=[\"Business Service L2\", \"Business Framework Group\", \"Business Framework\", \"PID\", \"GCB\"])\n",
    "\n",
    "    # Write to Excel as sheet 'Output'\n",
    "    ordered.to_excel(output_excel, index=False, sheet_name=\"Output\")\n",
    "    print(f\"Final output written to {output_excel}\")\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    excel_path = args.input\n",
    "    sheet_name = args.sheet\n",
    "    out_file = args.out\n",
    "    year = args.year\n",
    "    curr_month = args.current_month\n",
    "    chunksize = args.chunksize\n",
    "\n",
    "    # temp files\n",
    "    tmp_dir = tempfile.mkdtemp(prefix=\"adwp_\")\n",
    "    csv_path = os.path.join(tmp_dir, \"data_stream.csv\")\n",
    "    tmp_intermediate = os.path.join(tmp_dir, \"intermediate_posmonth.csv\")\n",
    "    print(\"Temporary dir:\", tmp_dir)\n",
    "\n",
    "    print(\"Streaming Excel sheet to CSV (memory-friendly)...\")\n",
    "    excel_sheet_to_csv(excel_path, sheet_name, csv_path)\n",
    "\n",
    "    print(\"Processing CSV in chunks and building compact intermediate file...\")\n",
    "    process_chunks_to_intermediate(csv_path, tmp_intermediate, chunksize=chunksize)\n",
    "\n",
    "    print(\"Building final pivoted Output sheet (this reads a much smaller intermediate file)...\")\n",
    "    build_final_output_from_intermediate(tmp_intermediate, out_file, year, curr_month)\n",
    "\n",
    "    print(\"Done. Temporary files located at\", tmp_dir)\n",
    "    # Optionally remove tmp_dir if you want to conserve disk\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
